# 2 What is MLOps ?

### This chapter covers

- Introducing the concept of MLOps
- Discussion of the challenges associated with ML
- Distinction and similarities between MLOps and DevOps

Machine learning is often not the end product of an organization, but is the means to an end of selling more books, getting more people to watch new shows or making sure that viewers receive an endless stream of interesting content. ML is therefore a tool that drives business value by improving the customers experience, be it internal or external. At its core then, if a model is unable to deliver business value, it has very little reason to exist or pursue.

This value generation is the primary reason why ML and by extension MLOps is hard. Very few companies truly do research on model development and instead reuse architectures and train/adapt off the shelf models for specific domains and problem sets. Availability of comprehensive open source libraries like Huggingface and OpenMMLab also make modeling trivial. After defining a problem and identifying an architecture to solve the problem statement, the hard questions come into focus. How would the model be trained ? How would data get to the model, how would it interact with the other services, where would it be run and how do we make sure the model is accurate over time?

In the previous chapter we talked about ML as well as the different stages that a practitioner undergoes in the normal course of model development. This chapter dives a bit deeper into the idea that value from ML often comes when it is part of a closed loop that continuously improves over time. We start by creating a mental framework of the loop and then break down some key ideas. We then contrast MLOps and traditional DevOps and identify how they are similar in some aspects but differ in some other key areas. We end with notes on how large organizations approach the value proposition of MLOps and the levels of maturity they encounter on their journey.

## 2.1 ML as a loop

When talking about real world machine learning, it is often easier to think in terms of a closed loop. The models, data and hyperparameters are best thought of as being ephemeral and parts of a loop that on the whole evolves over time. In thinking this way, it's easy to see that artifacts have a natural life cycle that you may have seen explained a few times in literature before.

Another reason it is advantageous to see an ML project as being a loop is that it naturally lends itself to thinking in terms of setting up a process for when the model changes. Models in the real-world change very frequently and having a robust workflow in place makes sure that developer velocity is not compromised and standardization is applied to processes from different teams. Iteration is a core aspect of ML development and could be due to changes in either the underlying assumptions the engineer makes, methods and opportunities to improve a model or changes in the landscape surrounding the model. For example, assume we develop a model to predict the price of a car for a used car dealership and deploy it so that users can find the best price for their vehicle. Over time, the model would evolve with changing customer needs and market conditions. You may even develop more versions of the model for commercial and industrial vehicles. Setting up a robust workflow to build, test and deploy models means that the new features can all be rolled out while focusing on the core business value provided by the models and not on the deployment.

The ML Loop starts with defining the problem and collecting the data to use for predictions. It is very important that the problem statement is well defined and has been aligned with stakeholders before embarking on any ML project. For example, a statement like “Predict the churn rate of customers” is insufficient to properly model and find evaluation criteria for. In our experience, having the answers to the following questions is a good starting point for any team embarking on an ML solution:

1. What is the problem or task that requires an ML solution ? Why does it need ML ?
1. Can the problem be solved with ML ? What research / literature exists on the domain ?
1. What are the goals of the product ? What end state metrics decide if the model is working ?
1. What is the business value generated by the model? How does it compare to the development efforts estimated ?
1. Where and how is the data stored ? What needs to be done to get the data from production to the model ?
1. How would the model be evaluated ? What are the metrics that can be used ?
1. How would the model be deployed? What are the resources that are needed to run the model and preprocessing (if any) ?
1. Does the deployment strategy provide for future upgrades and maintenance ?
1. What metrics provide a threshold for retraining the model ?
1. What is the timeline for the project?
1. Are there any ethical and privacy concerns to take special note of? How should the dataset be handled ? Does it have sensitive information ?
1. What is the business impact of a model that provides wrong predictions? How wide are the tolerances for the metrics before the model causes damage to the business ?

Answering these questions should set you up on an ML project that is well defined with a clear scope and deliverables. More importantly, it ensures that the data scientists and ML engineers working on the project fully understand the requirements and can work easily with each other.

![Figure 2.1 ML as a loop](https://drek4537l1klr.cloudfront.net/tanweihao2/v-5/Figures/02__image001.png)

### 2.1.1 Data collection

Once the problem has been defined and measurable Key Performance Indicators (KPIs) identified, data has to be collected for modeling. Data collection and curation must be done carefully as the future steps all depend on and carry forward any biases from this step.

This step is also one of the areas where research and enterprise ML differs. Researchers are often optimizing for metrics and assume the dataset they have is of sufficient quality. In the field of computer vision for example, researchers often rely on large open source datasets that have been through multiple reviews and revisions to assure high quality annotations and class balancing. Enterprise practitioners of ML, on the other hand, often do not have open source datasets for their target application or domain and even if they did, the models would still have to be fine-tuned (also called transfer learning) on data from the deployment environment. In our example, this would be analogous to the enterprise building a dataset from scratch.

However, the core requirements behind the spirit of data collection remain largely the same for both the enterprise and research practitioners.

- Data relevance to problem domain
- Size of the dataset with respect to problem complexity
- Quality of the dataset. Prevention of harmful biases and unintentional leakage of samples
- Distribution of data and features is representative of the deployment environment.
- Sufficient diversity in the data collection process that defines the problem domain well.
- Lineage and detailed tracking of raw data, intermediate versions and annotated datasets.

While most of the points above are obvious, lineage is often overlooked when starting on an ML project. This is often similar to technical debt in that it trades off initial velocity for future complexity and maintainability. Lineage of data is critical because:

- Data lineage establishes the origin of data. This means that the metadata on when, where, how and why the data was collected is important in understanding the limitations and / or selecting the appropriate features.
- It makes dataset versioning and compilation a linear process and provides the ability to go back and correct wrong data. In our experience, the first version of the dataset is often not the best and revisions are inherent in the iterative process of data analysis and collection. Having proper lineage can mean the difference between writing a simple query for recreating a dataset due to an error and having to rewrite/ rerun complex extraction scripts.

Let's try to use the guidelines for designing the data collection campaign for the toy example of a car price estimator. Looking at the core guidelines, we first collect relevant data for our use case. Here, this means collecting data for all passenger cars that are on the second-hand market and not other vehicles like trucks or motorbikes. We also need to keep in mind appropriate sampling techniques to ensure that the dataset is representative of the target domain and avoid biases. Finally, to establish lineage, we would use a versioned ETL pipeline to generate a dataset.

Data selection must be done carefully with special care taken to handle cases like class imbalance, fairness, and the lack of any biases. More often than not, when starting, some of these will be violated, but since we already know that the entire process is highly iterative, we can begin with a dataset that tries to satisfy as many of the above conditions.

For the ID card detector project, we are going to use the publicly available MIDV500 dataset. The dataset consists of 500 video clips for 50 different identity document types including ID cards, passports, and driving licenses from different countries.

![Figure 2.2 Examples of the data in MIDV500](https://drek4537l1klr.cloudfront.net/tanweihao2/v-5/Figures/02__image003.png)

As part of the ID card detector project, we will also cover human annotations with CVAT, an open-source, web-based image and video annotation tool which is used for labeling data for computer vision algorithms. We will cover its setup and how image annotations can be made a part of an ML loop. While this section is optional, it gives the reader an opportunity to try annotating some data and understanding the problems human annotation cycles add to an ML loop.

![](https://drek4537l1klr.cloudfront.net/tanweihao2/v-5/Figures/02__image005.png)

Fortunately, we will not annotate the entire dataset, primarily because we're using synthetic data, which has the advantage of not requiring any labeling, as the image and its labels are procedurally generated.

For the recommender, we will use the popular MovieLens-1m dataset for the project. The dataset has 1m anonymous ratings of about 3000 movies by 6000 users who joined MovieLens in 2000. There are 3 main files in the dataset, users.dat, movies.dat and ratings.dat. The code for downloading and processing the data is available in the repository provided with this book.

In the preparation, we will split the dataset into smaller parts to simulate receiving more data as users would generate if our model was in production. We would also build pipelines to ‘extract’ more data and version it. This is also where we get started with dataset versioning.

### 2.1.2 Exploratory Data Analysis (EDA)

EDA is, put simply, the exploration of the available data and understanding the nuances of its statistics. This step is one of the most important there is, since it is often the first line of defense against problems that are not sufficiently well defined, problems with the dataset and is critical to refine the estimate of effort required in developing and maintaining a model. It is therefore important to do EDA with the following goals in mind:

- What does my data look like? What is its schema? Can the schema change? How would I guard against invalid values? Does the data require cleaning?
- How is my data distributed? Are all targets equally represented or should there be additional class balancing?
- Does my data have robust features for the task I have in mind? Are the features expensive to compute?
- Does the input data vary cyclically? Does it exhibit correlations to external factors that are not modeled ?
- Are there any outliers in the dataset? What must be done to outlier values in production ?

When analyzing data, multivariate analysis (evaluation of multiple variables at the same time) can offer clues on the underlying distribution and patterns that can be exploited to develop robust features. Depending on the number of variables involved, dimensionality reduction methods like PCA or t-SNE [1] would be required.

It is very important at this stage to understand the assumptions we have about the data and its distribution. All assumptions must be treated as risks until they are validated, and checks must be put in place early in the data pipeline to ensure these assumptions are not violated. These assumptions could be as simple as the schema of the incoming data, a hidden bias in sampling or even a misunderstanding of a particular data field. This saves a lot of debugging hours later and can help scope problems down very quickly. Confronting our assumptions also means that we can pivot to a different approach early in the process if it does not hold up to scrutiny.

As a data scientist, a lot of the points above may be second nature; however, ML engineers with the same experience and skill can provide important clues and context for issue resolution and deployment issues like drift.

EDA is an iterative process and would involve revisiting assumptions of the dataset as we progress through the loop, have new insights into the dataset or when an experiment does not work. This part of the loop is time consuming, and you must be open to exploring alternative approaches to the model and change perspectives on the problem statement.

### 2.1.3 Modelling and Training

Armed with the knowledge of the data and features we can leverage, the next step is to create the actual model that would do the prediction. The focus therefore is to make sure that the initial objectives we set about to solve are realized and that the model is capable of delivering the metrics we need.

The modeling phase starts with determining the appropriate model or algorithm that would solve the problem statement. Decision factors for this phase include performance (theoretical), input data type, the class of problem to be solved (classification, detection, regression, etc) and associated considerations.

A few important components of the MLOps toolbox appear in this step:

- Model and data versioning
- Experiment tracking
- Model training pipelines
- Hyperparameter search

While we will discuss these components in detail in the upcoming chapters, it is important to understand why they are critical to the success of an ML loop. The common thread that these components all touch upon are lineage and traceability while removing manual steps that can lead to mistakes. Model and data versioning ensures that all models are intrinsically tied to a specific dataset version and therefore can easily be reproduced from scratch for debugging or compliance. Experiment tracking brings this rigor to the prototyping phase and ensures that all results are logged along with the data used and the exact model configuration and checkpoints. This also means that experiments and results are never lost. Collaboration and reporting of model characteristics and performance also improves due to the single unified interface used for experiment and model tracking.

The last component, hyper parameter optimization, refers to the process of finding the best hyperparameters in a given search space or to optimize a given function. Effective hyperparameter search is automated where the model developer provides the search space and the loss function. Therefore, it is best to adopt a modular approach to the codebase early on in the project in which the trained model is considered an artifact of applying configurations and data to a codebase. This approach also has benefits in lineage and reproducibility and is generally a good way to structure the codebase for a project.

![Figure 2.3 A view of a retraining pipeline using the modular codebase concept. This approach of keeping the model, code, configuration files, and data as distinct versioned components with lineage links ensures that the process remains flexible, fast, and adaptable while enabling experimentation, debugging, and iterative development.](https://drek4537l1klr.cloudfront.net/tanweihao2/v-5/Figures/02__image007.png)

This modular approach to structuring the codebase has benefits in increasing experimentation velocity as changing model architectures and changing training regimes is as easy as editing a configuration file. For example, if a model is suspected of overfitting, an easy way would be to reduce the model size and layer count with a configuration parameter and validate the hypothesis as opposed to manually running an experiment. These files should be tracked with git and when combined into the lineage of a model provides powerful introspection and debugging capabilities. Since the training and model hyper parameters are also configurable, it lends itself naturally to hyper parameter search and optimization. Figure 2.3 also highlights how modularizing the codebase also makes it easy to develop tracking and lineage systems for both the data and the source code. We will discuss this more in detail in the upcoming chapters, but it should be easy to see that having this separation in mind while developing initial models also makes extension, pipelining and modularization smoother in the future.

##### NOTE

As important as it is to leverage the factory pattern, it is also important not to over parametrize it. Start with a simple config and iteratively improve to maximize the speed vs flexibility tradeoff.

For the ID card detection, we will use the very popular YOLO (You only look once) model to train a custom object detector. At the time of this writing, YOLO is in its 8th iteration.

Out of the box, YOLO doesn't detect identity cards. It is trained using the COCO (Common Objects in Context) dataset, which is trained to detect 91 classes of objects, though none of them are identity cards and passports. But fret not! With a little fine-tuning, we can train YOLOv8 to detect identity cards, given that we have a well-labeled dataset.

The library that we will be using to train YOLOv8 also contains the evaluation logic. We will set aside enough images for training, testing, and validation.

For the recommender project, we start with a simple baseline recommender model and train it on our training subset. We begin in a notebook and we will then build pipelines for retraining and log models and experiments to a tracking server.

We will revisit this step after monitoring and try to evaluate a more complex model against the model in production. Here, we zoom in on evaluation criteria between architectures and how we can deploy and test new models while minimizing the negative effects. Metrics and evaluation criteria will also be made part of a reporting framework which can be used for communication with business stakeholders.

### 2.1.4 Model Evaluation

Once we have a trained model that performs well in testing during training, it is important to ensure that the model performs well in the target domain. The importance of a good problem definition and proper metrics selection comes to the fore here. When selecting metrics, it is important to keep in mind that some metrics are better suited for some domains than others and can be heavily influenced by data factors like class imbalance and drift. For example, precision, recall and F1 score may be important for classification and detection problems while MAE, MSE and others would be suitable for regression problems. Even in classification, the target domain may have different requirements for precision and recall (for example medical applications have stringent requirements on the false positive rate and precision while financial practitioners may choose to value recall). It is therefore imperative to understand the implications and tradeoffs involved in choosing metrics for evaluation and aligning it with the requirements set and business goals in the beginning.

In formal settings, model evaluation is done automatically on a holdout dataset that has been carefully curated to match the diversity, distribution and characteristics of data in production with special care taken to prevent leakage. Automating the evaluation also means that variability due to human error is eliminated and the results are repeatable and reproducible. It is important to note that the holdout dataset also evolves over time and is treated with as much rigor as the training dataset with respect to lineage, versioning and tracking. If the test/train/holdout data split changes for any reason, data leakage will be an issue and therefore mandates a revaluation and retraining loop.

Evaluation is also where rigorous analysis of model errors takes place. It is important to carefully examine misclassified samples, analyze patterns in them and identify systemic problems and biases of the model. These results then feed back into the data collection components. Sensitivity and robustness of the model are also tested here and approaches such as adversarial testing and edge case simulations all help guard against potential model errors down the line.

As you can see, evaluation is often a multi-stage process that has components, code and data that evolves over time and almost all subcomponents of evaluation inform previous steps. To help manage this complexity, automated pipelines and centralized tracking of results is highly recommended. Proper documentation and visibility into evaluation results prevents duplication of work and enables future comparisons and improvements.

##### NOTE

Some organizations enforce interpretability while evaluating models. Interpreting a model is where feature importances, decision boundaries, activation layers and other model components are inspected and visualized or techniques like LIME and SHAP are employed to better understand model behavior and its inherent limitations. This is a powerful tool, but one that comes with non-trivial complexity to implement. It is therefore recommended to carefully evaluate the benefits of interpretability when starting out against your needs before incorporating or planning it for the future.

### 2.1.5 Deployment

Once you have a trained model, the next step is to get the model into production to start generating value for the business. Common deployments methods include:

- Creating an API endpoint that fits into a microservices architecture. In this setup, the model works under an API layer that then serves requests.
- Deploying to a specific hardware environment or edge device. Although this form of deployment is not used as widely as the API endpoint, deployment to a specific hardware is important in fields like Automotive, security devices and robotics where the model performs important tasks like perception on the device.

Both these methods focus on model performance and latency, so an optimization step is usually performed where the models are converted from a raw trained model to one that has specific optimizations for the deployment environment before the models are transferred. It is important to note that in this context, model performance testing must be done on the final version ( optimized ) so that the results in the real world correlate with the measured performance. For example, while training on a GPU, a model may appear to perform within requirements but if the same model is deployed to an embedded device like a Jetson or even a different accelerator type, the performance could be quite different.

Deployment is also often split into staging and production, where staging models operate in an evaluation mode to test their effectiveness against the models in production. This means that deployment is a two-stage process where the initial model deployment is to the staging environment and once it has completed evaluation is automatically moved into production. Robust model version control and lineage helps this process by making sure that all model transitions are logged and rolling back to a previous version is as simple as changing the model in production.

In the projects we will use BentoML to serve the models over an HTTP endpoint. We will then package it up into a Docker container, which will then be deployed using Yatai, a model deployment platform made by the same folks at BentoML.

Model deployment doesn't end here though. The model in production must be monitored to ensure that it provides business value. Since every prediction the model makes during its lifetime cannot be evaluated, we can leverage on the fact that model services built using BentoML expose a few prometheus metrics out of the box. These include performance metrics such as requests per second (RPS) and also track things like HTTP response codes. Finally, we will explore some concepts like batching, canary deployments and A/B testing.

### 2.1.6 Monitoring

Model deployment enables the model to serve user requests and generate value. However, it is important to make sure that the model continues to perform well and any deviations in downstream metrics are immediately noticed and fixed. As you might guess, a wrong prediction from a model can actively hinder or even cause damage to the business.

Model monitoring aims to solve this by tracking various metrics, anomalies in data as well as model performance and ensures that the model behaves as intended in production. This step is also crucial because model performance can vary over time due to changes in input data, drifts in concepts and hardware or software failures. By monitoring the model, issues are detected immediately and actions can be taken to fix them.

Model monitoring has the following variants:

- **Data monitoring**: This type of monitoring refers to identifying anomalies by evaluating the statistical properties of the input data. Changes to the input data is called data drift and is problematic because the model has been trained on data that is different from the data in production which in turn can lead to unexpected and wrong predictions. Monitoring incoming data and developing robust metrics are key in evaluating model performance in production.
- **Performance monitoring**: Tracking performance metrics like accuracy and prediction of the model in production helps ensure that the model effectiveness remains high. Monitoring these metrics over time allows you to identify performance issues and take corrective actions.
- **Error monitoring**: Identifying wrong predictions from the model and understanding the errors is a powerful tool to improve model performance over time. By analyzing the errors, one can gain a better understanding of the model behavior in edge cases and actively work on reducing the impact of or even eliminating the error.

Monitoring relies heavily on a few components in the ML loop. Model version control helps here by providing important context for a model’s performance and understanding the problems in its training or dataset. Logging also helps in reproducing edge cases, investigating systemic problems and tracking down problems that may be caused by specific code changes, all of which may be difficult to track down over long periods of time. Finally, logging and version control together provides important documentation in regulated industries.

An important piece of the monitoring component is a reliable alerting and notification system. You could have the best metrics and log them all while having detailed lineage for the models, but if the alerts and metrics don't make it to the responsible person, the end effect will be the same as not having any of this at all.

Implementing a reliable monitoring system is therefore a crucial component in ensuring that the model maintains optimal performance under changing conditions and remains reliable in real world deployments.

The projects will demonstrate the concepts of drift and the importance of having a good monitoring setup. This will be further reinforced with new data, alternative models, model retraining and model comparisons. We will rely on many of the tools we discussed before and introduce Evidently to identify and trigger retraining runs for detected drifts.

### 2.1.7 Maintenance, updates and review

The final component effectively completes the loop and therefore "closes the loop". Here, the outputs from the model monitoring is used to

1. Implement bug fixes to fix issues and shortcomings in the model in production.
1. Collect data to train the model on specific edge cases and improve performance.
1. Mitigate for data drift by informing the data collection component and retrain a new model.

It is also important to think of this component as being distributed through the whole loop. All components we discussed so far will have some maintenance and updates as part of its natural lifecycle. For example, the deployment component will have a maintenance component because it is also responsible for replacing the model in production with a newer version. Similarly, data collection has an update component to handle concept and data drift that are noticed by monitoring.

By incorporating maintenance and updates into the ML loop, organizations can ensure that their machine learning models remain effective, reliable, and aligned with evolving data and user requirements. Regular updates and continuous monitoring enable the models to adapt to changing conditions and deliver accurate and valuable insights.

In the projects, we will work with new incoming data and new model architectures. As discussed, we will rely on the tooling we have set up so far but with additional focus on specific processes and tooling that help with model maintenance and updates in a controlled manner.

For the ID card detection project, we will dive into how the detector models perform with a specific, potentially unseen, identity card and how this new data is detected in production with the monitoring and drift detection systems. Can our identity card detector be a drop-in replacement?

For the recommender project, we will include more data from users and again leverage on our drift detection and monitoring systems to alert us of the deviation in performance. To make things more interesting, we will also try out a new model architecture and evaluate them with the model in production. We will use well defined metrics to evaluate model performance and then determine the best model to deploy.

## 2.2 Why is robust MLOps important ?

As we talked about in the previous chapter, MLOps is an extremely multidisciplinary area that combines expertise from arguably the harder parts of software engineering, high performance computing, CI/CD, version control, machine learning and statistics.

A single block in the ML loop, like data collection, is by itself a daunting topic to handle and one that requires specialists to successfully implement at scale. Every block in the loop adds a layer (or more!) of complexity to the solution, without which the model cannot provide consistent business value. This is very hard to do right the first time and often requires constant tweaking of components and infrastructure until a good solution converges in the organization.

Data is a critical component of the MLOps workflow, and the availability of good data and robust data pipelines are the markers of a successful ML project. However, managing data at scale without compromising on visibility and ensuring privacy and implementing robust data security is often expensive and can potentially open up the business to litigation for violation or significant financial losses. Curating this data and maintaining the concepts of fairness, diversity and lack of biases adds yet another layer of complexity to this core component.

A modern robust MLOps system and its tooling is an extraordinarily complex beast. How separate components work with each other, how they exchange data, how the entire system architecture looks, and a million more decisions all require specific choices that can have potentially large impact on the output of the model and the project as a whole. A full featured system that has components for data management, extraction, annotation, version control, EDA, model training, deployment and monitoring has so many moving parts that it is often harder to see the big picture and diagnose systemic issues.

Another reason why MLOps is hard in a conventional organization is because the specialists at each stage have very little in common with each other in terms of the tools they use and the method of communication. For example, a data scientist might consider their work in optimizing precision and recall for the target domain by careful modeling and hyperparameter optimization and would communicate their results in terms of improved metrics like precision, recall and F1 scores. A ML engineer on the other hand may talk about model registries, serving latency and communicating their results with deployment turnaround times, scaling to handle customer requests and latency of model in serving. For a successful ML loop, all these contributors and the external stakeholders must speak the same language and be able to understand each other while being able to collaborate seamlessly.

Optimization and scaling are topics that are not the highest priority for most teams that start off on an ML project but are ones that can potentially break an ML model’s usability. The best model in the world provides no business value if it cannot work in production or breaks under heavy load. Even if a model is optimizable, the performance characteristics of a model after optimization can be very different from the non-optimized version. Similarly, scaling a model to serve thousands or millions of requests has challenges that are very different from running evaluations or toy workloads on. Performance characterization and robust testing methodologies are hard to get right and require careful design and architecture.

Contact with the real world and its messy, chaotic data is another area where MLOps and ML in general can experience challenges. If not prepared for, edge cases and improper data will cause failures and in the worst case skews the output without throwing any visible errors. This can be prevented by having a robust monitoring strategy and strong data validation, but edge cases can still slip through and cause unexplainable outputs.

Governance and compliance mandates explainable and interpretable ML. Ensuring compliance while employing responsible AI practices and tackling potential biases can be challenging.

Finally, the fragmentation of tooling in the space can also be quite confusing to an organization that is new to formal ML. For example, just on the topic of model registries there are at least 10 offerings that all purport to solve the problems faced by AI. Comprehensive solutions rarely exist and, in our experience, focus on core areas like EDA and model deployment but may completely miss out on crucial aspects like data management and lineage. The current philosophy in the tooling space seems to be of specialist tools and a few over-arching solutions that need to be combined to suit the needs of a team.

All these and more cause smaller teams to often skip over robust MLOps practices and instead choose to quickly build a solution. While this works in the short term or for a single model, this accrues immense technical debt in the long term and makes models unmaintainable. Overcoming these challenges requires investment into technical skills, cross functional collaboration and a culture that is conducive to continuous learning. MLOps is hard, but we firmly believe that having and implementing a robust strategy will pay dividends in the long term in improved velocity, maintainability and most importantly, business value.

## 2.3 Role of MLOps in a mature organization

As you can now hopefully see, robust MLOps provides a lot of advantages for an organization that wants to leverage ML for improving their product or to provide an ML based product. In a mature ML organization, robust MLOps is a key enabler for efficient development, deployment, management, monitoring and optimization of models. Key benefits include:

1. **Accelerated innovation and experimentation**: having a robust MLOps infrastructure and tooling in place means that data scientists can focus on the core of their expertise, developing models to solve business problems and iterating quickly. Abstraction of infrastructure, provision of tools for easy model versioning and comparison that is backed by strong lineage for code, configs, model checkpoints and data and the setup necessary for easy A/B testing all mean that data scientists can test hypotheses and tune hyper parameters and respond to changing business needs quicker.
1. **Optimized costs**: In the absence of a robust central ML workflow, teams usually roll their own silos of development and infrastructure which leads to less than optimal resource utilization and fragmentation of best practices. A centrally cohesive ML strategy means that teams can leverage on each other’s strengths to build pipelines and share components and faster improvement of ML reliability within the organization. A central strategy and infrastructure also means that resource allocation can be more effectively managed and enables automated scaling and cost monitoring.
1. **Collaboration between teams**: A robust MLOps strategy means that teams can work across domains to solve common challenges and business problems. It can even improve collaboration within the team by improving visibility into each other's activity and standard tools for managing parts of the workflow. It also establishes clear channels of communication and collaboration and maximizes the value of ML projects to the organization.
1. **Improved efficiency: **MLOps and CI/CD automates away most of the repetitive tasks in the ML lifecycle like data mining, pre-processing, feature engineering, training and deployment. This means that the steps are less prone to human error and have deterministic outputs and behavior. It also means that data scientists can focus on higher value tasks such as modeling and innovation.
1. **Improved maintainability of models**: As we discussed in the beginning, models are continuously evolving and changing. Having a robust MLOps workflow in place means that this evolution is visible to the data scientists and provides methods to monitor, debug and rollback in the event of an issue. Robust monitoring also means that processes are in place to handle drift, model biases and regression. Finally, strong lineage means that we can easily track down the configuration and dataset that was used to train a model and debug issues offline.
1. **Robust scaling**: MLOps enable organizations to standardize the challenges with scaling and large scale data processing, by providing common workflows and optimized resource allocation. This means that the models can perform well in production and handle large data volumes.

On the whole, a good MLOps strategy and implementation lowers the barrier for ML good practices. For example, having model logging readily configured and integrated into the training codebase means that data scientists would use model versioning and logging by default as opposed to manually managing training runs and generating ad-hoc reports. In a mature organization, MLOps brings together people, expertise, processes and best practices so that it is readily accessible and works together in creating business value.

## 2.4 DevOps vs MLOps

A common question that everyone has at this point is why MLOps exists as a separate domain and is not an extension of DevOps. To explain our view, let's first take a look at how they are similar to each other.

- DevOps and MLOps both advocate robust automation. The driving goal in both is to improve efficiency by leveraging automation and streamlining processes. They both aim to automate repetitive and error prone steps for improved velocity and more reliable software as the end product.
- They are also similar in their emphasis of Continuous integration and Continuous deployment (CI/CD) ethos. Automated test suites and seamless automated deployment to production environments are common in both.
- They are both multi-disciplinary in nature and often require generalists who are comfortable with a few separate domains. Both methodologies provide for tight collaboration between the project team and stakeholders by shared tooling and reporting methods.

While there are overlaps between the domains, a few special characteristics of an ML workflow sets MLOps apart:

- MLOps is primarily concerned with the challenges of training, deploying, monitoring and optimizing ML models. This is different from DevOps that focuses on the software lifecycle.
- Data is a crucial differentiator between MLOps and DevOps. MLOps sees data as an integral building block of the model and process and has a large part dedicated to the concerns of managing the data lifecycle. DevOps does not nearly have the same level of rigor applied to data management.
- Models continuously change and evolve, even without any code changes in the codebase. This also sets MLOps apart from DevOps by adding Continuous Training (CT [2]) and having special considerations for model management and versioning.
- MLOps also inherently concerns itself with model interpretability and bias. It includes techniques for monitoring the model and ensuring compliance with agreed standards.
- Monitoring the performance of the model is critical in MLOps because performance degradation can occur due to changing characteristics of the input data. As google mentions, ML models can degrade in performance in more ways than conventional software. [2]
- Finally, MLOps is inherently experimental and iterative. It is therefore important to track the experiments themselves rather than just a build.

As you can see, MLOps is quite different in a few important areas from classic DevOps. While they share a few common principles, MLOps has a specific focus on managing the unique challenges of deploying and managing machine learning models in production environments.

## 2.5 Levels of MLOps maturity

According to Google [2], the level of automation of the steps we talked about earlier in the discussion of the ML loop is a good indicator of the maturity of MLOps in a company. This directly translates into the velocity of new model development or adapting to changes.

### 2.5.1 Level 0 - Basic

Level 0 is where most companies who are just embarking on their MLOps journey start. It is primarily characterized by manual script-based processes for building, training and deployment ML models. There is no monitoring implemented for the models in this stage and the release iterations are few and far between.

### 2.5.2 Level 1 - Intermediate

Level 1 is primarily characterized by the speed of experiments and continuously retraining the model in production. While Level 0 deployed only the model, deployment in Level 1 is an entire retraining pipeline that enables continuous delivery of models to prediction services that keeps up with new data and automatically improves over time. Another key aspect of a Level 1 team are modular pipeline components that are freely shared among teams and enable the use of the same pipeline for deployment in development as well as in production, termed “experimental - operational symmetry” by the authors.

A few important components need to be in place to enable Level 1 activities, they are

- Robust data and model validation mechanisms
- A feature store (if needed)
- Pipeline monitoring, lineage and associated metadata
- Auto-triggered ML pipeline runs depending on custom triggers.

### 2.5.3 Level 2 - Advanced

The final stage is automating the build and deployment of the pipelines themselves and having the pipeline components be a critical part of the ML loop. At this stage almost everything except data analysis and model analysis is automated. Pipelines and components are also owned by the whole team / organization instead of only the ML Engineers and new components can also come from the data scientists based on their observations.

While these levels are a rough guide and do not represent a linear increase in difficulty between stages, understanding where you or your workplace are on the scale will help you plan the steps you need to take to have a steady progression while building up a strong MLOps culture internally. The tools we talk about in the rest of the book will be able to scale all levels of maturity.

Now that we laid the theoretical foundations and the business side of MLOps, the next chapters will dive into building up a scalable platform for running ML workloads starting with containerization and kubernetes.

## 2.6 Summary

- ML exists to solve a business problem and it is important to understand the requirement in depth before starting an ML project.
- MLOps is the iterative process of developing, monitoring and improving an ML model.
- A model is an artifact of the ML loop that aims to improve model performance over time.
- MLOps is hard due to data management, complex tooling, organizational setups, scaling challenges and the unpredictability of the real world.
- Skipping established ML practices can appear to be faster in the short term, but duplication and technical debt will quickly erase any gains.
- DevOps and MLOps have similarities, but differences in data and model management, among others, means that MLOps has some unique challenges.
- Robust MLOps is a highly experimental, iterative process with room for institutional learning and rapid prototyping to identify things that work for you and your organization.
