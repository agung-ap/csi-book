# Chapter 8. Future Trends in Ultra-Scale AI Systems Performance Engineering

# A Note for Early Release Readers

With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.

This will be the 8th chapter of the final book.

If you’d like to be actively involved in reviewing and commenting on this draft, please reach out to the editor at *arufino@oreilly.com*.

The case studies in the previous chapter gave us a snapshot of some real-world, full-stack, and ultra-scale AI optimization studies. Looking ahead, AI Systems Performance Engineers will face an exciting mix of challenges and opportunities. The next era of AI models will demand bigger and faster hardware - as well as smarter and more efficient ways to use it.

Key trends on the horizon include a heavy focus on massive AI data centers, intelligent software toolchains, AI-assisted system optimizers, inference-focused optimizations, faster optical interconnects, offloading compute to network hardware, 3D GPUs and memory, energy efficiency and sustainability, the emergence of hybrid classical-quantum computing, and innovations that help scale toward 100-trillion-parameter models. In this chapter, we’ll explore these trends, keeping our focus on practical insights and best practices for performance engineers.

# Convergence of AI and Scientific Computing

Over time, the distinction between an “AI supercomputer” and an HPC-based “scientific supercomputer” will dissolve. We’re already heading that way since modern GPU-based systems like the GB200 NVL72 are capable of traditional HPC simulations. Conversely, scientific applications are increasingly using AI methods to accelerate discovery. In the near future, GPUs will be truly universal computing engines for both simulation and AI.

In fact, in one NVIDIA [blog post](https://blogs.nvidia.com/blog/blackwell-scientific-computing/), the company highlighted monumental science boosts with Blackwell including weather simulations running 200× cheaper on Blackwell than on Hopper - and consuming 300× less energy for certain digital twin workloads. These numbers were achieved on the same hardware that also delivers AI breakthroughs.

The trend is a fusion of workloads in which an exascale weather simulation might directly incorporate neural network components for sub-grid physics, running in tandem on the same GPUs. We’ll also see AI models trained specifically to enhance and steer simulations running concurrently on HPC infrastructure. This convergence will push system software to support more heterogeneity. Scheduling software will efficiently intermix AI training jobs with HPC jobs on the same GPU pool. Programming models might allow seamless switching between Tensor Core operations and classic double-precision arithmetic within a single application.

It’s expected that many scientific breakthroughs will come from simulations boosted by AI such as a molecular dynamics simulation where critical quantum chemistry calculations are accelerated by an AI model - all executed on the same machine. NVIDIA is clearly preparing for this future by ensuring their GPUs excel at both HPC and AI domains. The fact that Blackwell GPUs will be 18× faster than CPUs in scientific computing tasks and simultaneously perform well on AI tasks is an early sign.

In fact, we likely won’t be talking about “AI systems” and “HPC systems” separately. There will just be accelerated computing systems that handle the full spectrum of computational problems. For performance engineers, this means techniques and optimizations from the AI world - such as mixed precision and sparsity - will benefit HPC. And vice versa, HPC techniques and optimizations - such as numerical stability techniques, scheduling algorithms from HPC will benefit AI training.

This trend towards HPC and AI convergence ultimately leads to more robust, versatile computing platforms. The supercomputer of the future might solve a physics problem in the morning and train a massive AI model in the afternoon, with equal proficiency.

# Massive Data Centers Powering Self-Improving, Perpetually-Learning AI Agents

In early 2025, a [report](https://ai-2027.com/) from the [AI Futures Project](https://ai-futures.org/) describes a series of milestones and AI models/agents that measure technological progress, enhance research speed, and provide transformative benefits for AI research and development over the next few years. The report describes how the frontier AI labs are currently designing and building some of the biggest AI data centers the world has ever seen.

These superclusters will provide exponentially more compute than previous systems and enable a massive leap in model performance. For context, training GPT 3 required 3×10^23 FLOPs and GPT 4 required 2×10²⁵ FLOPs. These new data centers are engineered to train models with 10^27 and 10^28 FLOPS as shown in [Figure 8-1](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch08.html#ch11_figure_1_1744914899744795). It describes a fictitious Agent-1 model trained on 2 orders of magnitude more compute FLOPs than .

![A pixelated map of the state of arizona  AI-generated content may be incorrect.](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9798341627772/files/assets/ch11_figure_1_1744914899744795.png)

###### Figure 8-1. Actual amount of compute needed to train GPT-3 and GPT-4 compared to the expected compute for the fictitious next generation model called Agent-1 by the researchers at the AI Futures Project. (Source: https://ai-2027.com)

These massive clusters will provide 2 and 3 more orders-of-magnitude research capacity and set the stage for consistently faster training runs and quicker feedback loops. The result is a robust platform that unlocks unprecedented throughput and efficiency and drastically cuts research cycle times and accelerates breakthrough discoveries in machine learning.

Agent-1 is expected to be a self‑improving model to generate and optimize code in real time. By automating coding tasks ranging from routine debugging to complex kernel fusion, this frontier AI system reduces time-to-insight and expands the creative horizon for research engineers all across the world. Automated coding acts as a force multiplier that enables rapid iteration and allows researchers to explore more ambitious ideas with less manual overhead.

These massive AI systems are expected to allow continuous model fine-tuning and improvement. The follow-up model, Agent‑2, might be an always‑learning AI that never actually finishes training. So instead of checkpointing and deploying a static model, Agent‑2 is designed to update its weights every day based on freshly generated synthetic data.

This perpetual learning process ensures that the system stays at the cutting edge by continuously refining its performance and adapting to new information. This approach guarantees that improvements in data efficiency and model intelligence persist over time, while also dramatically reducing the gaps between successive breakthroughs. With this continuous learning framework, the organization sets a new standard for staying current in rapidly changing research domains.

Agent‑3 is described as an AI system that leverages algorithmic breakthroughs to drastically enhance coding efficiency. By integrating advanced neural scratchpads and iterated distillation and amplification techniques, Agent‑3 transforms into a fast, cost‑effective superhuman coder.

Running as many as 200,000 copies in parallel, Agent-3 creates a virtual workforce equivalent to tens of thousands of top-tier human programmers operating 30x faster. This breakthrough would not only accelerate research cycles but also democratize advanced algorithmic design. This would allow new ideas to be rapidly conceived, tested, and refined. The resulting acceleration in R&D paves the way for consistent, measurable gains in AI performance.

Self‑improving AI will soon reach a point where it can effectively surpass human teams in research and development tasks. These systems operate continuously and without rest. They diligently process massive streams of data and refine algorithms at speeds that far exceed human capabilities.

Non‑stop cycles of improvement mean that every day brings a new level of enhancement to model accuracy and efficiency. This self‑improving progress streamlines R&D pipelines, reduces operational costs, and enables a level of innovation that was previously unimaginable. At this point, human teams transition into roles of oversight and high‑level strategy, while the AI handles the heavy lifting and delivers breakthroughs at a pace that redefines the future of technology.

Agent‑4 evolves as a superhuman researcher and epitomizes the potential for superhuman research capability. It builds on its predecessors but distinguishes itself through its ability to rewrite its own code and optimize complex research tasks with maximum efficiency.

Agent-4 demonstrates a form of intelligence that accelerates problem solving and clarifies its internal decision processes through refined mechanistic interpretability which helps to understand the internal workings of the AI’s underlying algorithm and reasoning process. In practical terms, Agent‑4’s performance allows it to solve scientific challenges, generate innovative research designs, and push the boundaries of what generative AI models can achieve. It does all of this at speeds unmatched by humans. This breakthrough would mark a turning point in AI research and development by creating a virtuous cycle of discovery and technological progress.

The evolution of Agents showcases advancements in AI system infrastructure, automated coding, continuous learning, and self‑improving models. Each generation dramatically enhances research productivity and innovation. Together, they set the stage for a future in which AI system performance and efficiency become critically important and reach unprecedented levels.

# Smart Compilers and Automated Code Optimizations

We are entering an era of extremely smart compilers and automation in the AI performance toolkit. Gone are the days when a performance engineer hand-tunes every CUDA kernel or fiddles with every low-level knob. Increasingly, high-level tools and even AI-powered systems are doing the heavy lifting to squeeze out the last bits of performance.

AI frameworks like PyTorch, TensorFlow, and JAX are rapidly evolving to harness the latest GPU capabilities using smart compilers and execution-graph optimizers. Techniques like PyTorch TorchDynamo and OpenAI’s Triton compiler can automatically perform optimizations like kernel fusing with NVIDIA’s Tensor Cores and asynchronous data movement with their Tensor Memory Accelerator (TMA).

Additionally, OpenAI’s Triton** **compiler lets developers write GPU kernels using its Python-based language. Triton compiles these Python-based kernels into efficient CUDA kernels under the hood, but this complexity is abstracted away from the Triton user.

This kind of tooling is becoming more and more powerful by the day. In fact, OpenAI and NVIDIA are now [collaborating](https://developer.nvidia.com/blog/openai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability/#:~:text=The%20open,through%20an%20intuitive%20programming%20model) closely to ensure Triton fully supports the newest GPU architectures and automatically takes advantage of their specialized features. As soon as a new GPU generation is released, an updated Triton compiler exposes the GPUs new capabilities without the researcher or engineer needing to know the low-level C++ code or PTX assembly code. Instead, they write high-level Python code and the compiler generates optimized code for that specific GPU environment.

Very soon, many of the optimizations that we’ve been coding by hand will be handled automatically. Automatic kernel fusion, autotuning of kernel-launch parameters, and even decisions about numerical precision could all be delegated to compilers and AI assistants.

Using AI to optimize AI software is part of a broader trend that has been occurring for quite a while. In fact, Google’s DeepMind gave a striking proof-of-concept back in 2022 with its [AlphaTensor](https://www.quantamagazine.org/ai-reveals-new-possibilities-in-matrix-multiplication-20221123). As discussed in the previous chapter, AlphaTensor is an AI that discovered new matrix multiplication algorithms that are more efficient than any human-developed algorithms. If an AI can beat 50 years of human math hand-tuning for matrix multiplication, why not give AI a chance to find better ways to multiply matrices on hardware specially optimized for matrix multiplication?

As mentioned in the previous chapter, other companies are exploring these AI-assisted optimizations, as well. One effort by a startup called Predibase used reinforcement learning (RL) to train a large language model (LLM) to write efficient GPU code given a reference PyTorch function. They built the LLM to take a high-level operation and generate a custom CUDA/Triton kernel. The code is both correct and faster than a human-optimized version.

Predibase reported that their AI-generated kernels outperformed strong baselines like OpenAI’s own optimizer and a system called DeepSeek-R1 which we have discussed. by about 3× on a set of benchmark tasks. This kind of AI-driven autotuning could well become part of standard toolchains for performance engineers. We might soon be able to run a PyTorch model in a special “performance” mode. Under the hood an AI agent would iteratively rewrite and test generated kernels to speed up a specific workload on specific hardware. It’s like having a tireless performance engineer living inside the compiler - constantly trying to make your code run a bit faster with every iteration.

Beyond kernel generation, modern frameworks are getting smarter about execution graphs and scheduling. Graph execution helps to reduce CPU-GPU synchronization overhead and opens the door to global optimizations across the whole graph. Technologies like NVIDIA’s CUDA Graphs allow capturing a sequence of GPU operations - along with their dependencies - as a static graph that is launched more efficiently as shown in [Figure 8-2](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch08.html#ch11_figure_2_1744914899744833).

![A diagram of a project  AI-generated content may be incorrect.](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9798341627772/files/assets/ch11_figure_2_1744914899744833.png)

###### Figure 8-2. Graph execution in CUDA reduces overhead when launching multiple kernels in a sequence (Source: https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)

We’re seeing AI frameworks automatically capturing training loops and other repetitive patterns into graphs to reduce overhead. Even if the execution graph is dynamic instead of static, the framework can trace it once and then run the trace repeatedly.

Moreover, overlapping communication with computation will be increasingly automated. This used to require manual effort to arrange, but the system might analyze your model and realize, for example, that while GPU 1 is computing layer 10, GPU 2 could start computing layer 11 in parallel – effectively doing pipeline parallelism under the hood.

We’ve seen complex narratives on how to implement 3D, 4D, and 5D parallelism to maximize GPU utilization when training and serving large models. Techniques like these are an art and science that currently involves a lot of human intuition and experience. While these techniques are currently described in expert guides like [Hugging Face’s Ultra-Scale Playbook](https://www.threads.net/@sung.kim.mw/post/DGQ85hVyDtm?hl=en#:~:text=,why%20overlap%20compute%20%26%20communication), the hope is that they’ll be baked into compilers, libraries, and frameworks soon.

In essence, the AI framework should understand these patterns and schedule work to keep all parts of a distributed system busy - without the user profiling, debugging, and optimizing every GPU stream, memory transfer, and network call. For example, we might one day have an AI advisor that, when you define a 500 billion-parameter model, immediately suggests “You should use 8-way tensor parallelism on each node and then 4-way pipeline across nodes. And, by the way, use these layer groupings and chunk sizes for optimal efficiency.”

For performance engineers this would become a huge productivity boost. Instead of trying endless strategies and configurations, you could ask an AI system for a near-optimal solution from the start. By combining human insight with compiler/AI automation, you can achieve optimal results with less effort than in the past. It’s a bit like moving from assembly to high-level languages all over again as we’re delegating more responsibility to the tools. For performance engineers, this means our role shifts more towards guiding these tools - and quickly verifying that they’re doing a good job - rather than slowly experimenting and verifying everything manually.

In summary, the software stack for AI is getting increasingly intelligent and autonomous. The best practice here is to embrace these tools rather than fight them. Leverage high-level compilers like OpenAI’s Triton that know about your hardware’s capabilities and performance options. And keep an eye on new AI-driven optimization services as they might seem like black boxes at first, but they encapsulate a lot of hard-won performance knowledge.

# AI-Assisted Real-Time System Optimizations and Cluster Operations

The push for automation isn’t just in code – it’s at the system and cluster operations level, as well. In the future, AI systems will increasingly manage and optimize themselves - especially in large-scale training and inference clusters where there are myriad concurrent jobs and requests in flight at any given point in time - requiring complex resource sharing strategies.

One imminent development is autonomous scheduling and cluster management driven by AI. Today we use mostly-static heuristics and relatively-simple resource allocation mechanisms for our Kubernetes and SLURM/HPC clusters. But imagine a smart agent observing the entire cluster’s state and learning how to schedule inference requests and training jobs for maximum overall throughput.

This scheduling agent might learn that certain requests or jobs can be co-located on the same node without interfering with each other - perhaps because one is compute-heavy while another is memory-bandwidth-heavy. By ingesting large amounts of monitoring data, the AI scheduler could dynamically pack and migrate tasks to keep all GPUs busy, maximize goodput, and minimize idle time.

In a sense, the cluster begins to behave like a self-driving car, constantly adjusting its driving strategy (resource allocation) based on real-time conditions - rather than following a fixed route. The benefit to performance engineers is higher resource utilization and fewer bottlenecks. Our job would shift to setting the high-level policies and goals for the AI scheduler and letting it figure out the specifics.

We could also see AI performance co-pilots system operators. LLMs can become part of the infrastructure in a support role. For example, a performance engineer might have an AI assistant they can ask, “How can I speed up my training job?” and get informed suggestions. This sounds fanciful, but it’s plausible when you consider such an assistant could be trained on the accumulated knowledge of thousands of past runs, logs, and tweaks.

The AI performance co-pilot might also recognize that your GPU memory usage is low and suggest increasing batch size, or notice that your gradient noise scale is high and suggest a learning rate schedule change. This agent would encapsulate some of the hard-won experience of human experts - making this knowledge available anytime.

Similarly, AI assistants could watch over training jobs and inference servers and flag anomalies. For instance, the assistant could be monitoring a training job and say, “Hey, the loss is diverging early in training, maybe check if your data input has an issue or reduce the learning rate” as shown in [Figure 8-3](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch08.html#ch11_figure_3_1744914899744861).

![A cartoon of a robot standing next to a computer screen  AI-generated content may be incorrect.](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9798341627772/files/assets/ch11_figure_3_1744914899744861.png)

###### Figure 8-3. AI assistant monitoring a long-running training job and suggesting actions to fix an anomaly

Already, companies like Splunk (now Cisco) and PagerDuty are using AI models on system log data to predict failures or detect anomalies in data centers. Extending that to AI workload-specific telemetry is actively in progress. In short, we would get a pair of fresh AI eyes on every job and every inference server - monitoring and advising in real time.

Another powerful use of AI is in automated debugging and failure analysis for AI systems. When a training job fails halfway through its 3 month run, a human has to read through error logs, device statistics, and perhaps even memory dumps to figure out what went wrong. Was it a hardware fault? A numerical overflow? A networking hiccup?

In the future, an AI system could digest all that data including logs, metrics, and alerts - and pinpoint likely causes much faster than they do today. It could say, “Node 42 had 5 ECC memory errors right before the job crashed – likely a GPU DIMM failure.” Or, “The loss became NaN at iteration 10,000 – perhaps an unstable gradient; consider gradient clipping.”

By learning from many past incidents, the AI troubleshooter could save engineers many hours of detective work. Some large computing sites are already training models on their incident databases to predict failures and suggest fixes.

Taking things a step further, RL can be applied to real-time control of system behavior in ways that fixed algorithms cannot easily match. For example, a power-management RL agent could be trained to continuously tweak frequencies and core allocations to maximize performance per watt in a live system. This agent would learn the optimal policy by analyzing the system in real-time.

Another example is actively managing memory in AI models. An AI agent could learn which tensors to keep in GPU memory and which to swap to CPU or NVMe - beyond static rules like “swap least recently used.” By observing live access patterns, an AI can manage a cache more efficiently. This is especially effective when patterns are non-obvious or workload-dependent.

Already state-of-the-art practitioners are using RL to optimize cache eviction, network congestion control, and more. The complexity of ultra-scale systems - with hundreds of interacting components and resources - makes them prime candidates for such learning-based control. There are just too many tunable knobs for a human to stumble upon the best settings in a timely manner - and a manner that adapts to different workloads in real-time.

For the performance engineer, the rise of AI-assisted operational agents means the role will become more about orchestrating and supervising AI-driven processes rather than manually tweaking every single parameter. It’s somewhat analogous to how pilots manage autopilot in a modern aircraft. They still need deep knowledge and oversight, but much of the millisecond-by-millisecond control is automated. Same with someone driving a Tesla in Full Self Driving (FSD) mode. The driver still needs knowledge and intuition to avoid difficult situations and prevent accidents, but the vehicle’s control is automated by the FSD software.

To guide the AI assistant to manage our cluster efficiently, we simply set the objectives, provide the safety and fairness guardrails, and handle novel situations that the AI hasn’t seen before. Routine optimizations like load balancing, failure recovery, and memory-buffer tuning is handled by the AI. Embracing this paradigm will be important for the future. Those who insist on doing everything by hand in such complex systems will simply be outpaced by those who let the automation run. These AI-automation-friendly folks will be able to focus their energy on human intuition and creativity - where they will add the most value in this brave new world.

# Sparsity and Conditional Computation as First-Class Citizens

As models grow in size and capabilities, one of the most promising ways to cope with their computational demands is to avoid doing unnecessary work. Soon, it’ll be possible that hardware and software will aggressively embrace sparse and conditional computing. NVIDIA’s Ampere architecture took a first step by [introducing](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/#:~:text=2%3A4%20structured%20sparse%20matrix%20W%2C,and%20its%20compressed%20representation) structured 2:4 sparsity in the Tensor Cores. This effectively doubles throughput when a network zeroes out 50% of its weights. In practice, sparsity delivers about a 30% performance-per-watt gain when inferencing with pruned models vs. dense models on A100 GPUs.

We’ll likely see these capabilities greatly expanded. Future GPUs could even support dynamic sparsity where the pattern of computation can skip zeros or irrelevant activations on the fly - not just the fixed 2:4 pattern. This ties in with model architectures like Mixture-of-Experts (MoE) which activate only a subset of model parameters for each input.

NVIDIA [hints](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/#:~:text=The%20second,of%20replacement%20for%20larger%20precisions) at this direction by noting that Blackwell’s Transformer Engine will accelerate MoE models alongside dense LLMs. For example, the hardware is tuned to rapidly switch between different “experts” and take advantage of the fact that not all parts of the network are used at once.

Over time, as compiler technology and software libraries evolve, we expect them to learn how to automatically identify and take advantage of irregular (or unstructured) sparsity in neural networks. Today, this is a difficult task because most compilers are optimized for regular, predictable data structures.

Consider a dynamic neural network in which the execution graph can change from one input or training iteration to another, often by using if/else statements. With dynamic networks, you can build in conditional execution so that only the portions of the graph relevant to a given input are run. For instance, an if/else branch can allow the network to skip over entire sections when they are not needed as shown in [Figure 8-4](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch08.html#ch11_figure_4_1744914899744886).

![A diagram of data processing  AI-generated content may be incorrect.](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9798341627772/files/assets/ch11_figure_4_1744914899744886.png)

###### Figure 8-4. Dynamic graph execution based on input data (Source: https://developer.nvidia.com/blog/enabling-dynamic-control-flow-in-cuda-graphs-with-device-graph-launch/)

This reduces the number of operations that must be performed. This means that the network can selectively execute only the “active” parts of the graph, leading to more efficient use of computational resources, lower power consumption, and shorter processing times compared to static networks that always run the full computation graph.

Dynamic networks tend to produce irregular sparsity patterns. This means that zeros or inactive units are scattered in a non-uniform, unpredictable manner. Current compilers, which perform best when data is laid out in regular, easily predictable patterns, find it very challenging to detect and optimize these irregularities. To fully harness the potential of dynamic networks, both compiler and hardware improvements are needed including advances in compiler analysis and optimization techniques, as well as new hardware designs that can handle non-uniform sparsity effectively.

On the hardware side, there’s potential for GPUs to gain support for what’s known as block sparsity with various block sizes. Currently, hardware is often designed with the assumption that neural network data is neatly aligned—even the inactive parts. With block sparsity support, a GPU would have the capability to process these if/else branches more intelligently by “flowing through” the branches without incurring the usual computational overhead associated with skipped (inactive) paths. In essence, if a dynamic network chooses not to run a branch, the GPU wouldn’t suffer the typical delays or penalties, leading to significant improvements in performance.

It’s plausible that many state-of-the-art models will be sparse by default - either through training techniques or adaptive inference. And GPUs will accordingly deliver more tera-ops for those sparse operations than for dense ones. Academic work is already making progress. One recent research prototype “[Eureka](https://ieeexplore.ieee.org/document/10411411)” demonstrated over 4× speedups beyond Ampere’s 2:4 sparsity by handling more general sparsity patterns in hardware. NVIDIA GPUs could incorporate similar ideas.

The distinction between dense TFLOPS and sparse TOPS is important for getting an accurate picture of computational efficiency in modern AI applications. For sparse workloads, many operations simply aren’t executed because the hardware detects that portions of the input (e.g. weights below a certain threshold) contribute negligibly to the final result. This operation-skipping allows the GPU to effectively double its work rate on useful computations. As such, one can easily understate the true capability of the GPU when comparing sparse computational throughput to dense computational throughput.

Imagine you have a machine that can do a huge number of math operations. When we say a GPU has, for example, “500 TFLOPS,” we mean that if it had to perform every single floating‑point operation (e.g. multiplying or adding) without ever skipping anything, it could do 500 trillion of these operations every second. That’s the “dense” measurement.

However, in sparse AI tasks, many of these operations turn out to not be needed because many of the network’s weights - the numbers it uses to make decisions - are so tiny that they don’t affect the result. Modern GPUs can detect these near-zero or zero values and skip the unnecessary calculations.

Because the GPU skips a lot of these unimportant operations, it actually performs more useful work, or goodput. So instead of measuring performance by counting every theoretical multiplication like we do with dense TFLOPS, we use a measurement called TOPS (tera operations per second) to count only the operations the GPU actually performs on the sparse workload. So, a GPU might be rated at 500 dense TFLOPS but achieve an effective throughput of 1000 sparse TOPS when it skips the unnecessary work on zeros.

###### Tip

Even though sparse throughput is given in TOPS instead of TFLOPS, it still uses floating‑point math. The difference is purely in how many operations are actually executed. TOPS count only the valuable operations - not the skipped operations.

A simple analogy is to think of a factory assembly line that is capable of assembling 500 items per minute if every worker worked on every item. But if an item is already completed - or doesn’t need further assembly - the worker can skip that item and move to the next item. If they effectively skip every other item, the line appears to handle 1000 useful items per minute even though its full capability of assembling 500 items per minute hasn’t changed.

The key takeaway for performance engineers is to focus on the actual goodput of your system on your workload. This, again, is the useful work done by the system and not just the raw capacity which is the documented number of operations the GPU can theoretically perform. This perspective is vital for modern AI tasks that exploit sparsity in data.

Sparsity and conditional execution will likely move from niche optimization to center stage. This will be enabled by advances in GPU architecture and model design and allows us to handle models with trillions of parameters by ensuring, at any given time, we’re only computing the most-useful portions of the model that contribute to an accurate response.

# GPUs Performing More Tasks Across the Entire AI Pipeline

As GPUs become more generalized and powerful, they will start moving into areas traditionally served by CPUs. One striking example is data preprocessing and analytics. By moving these workloads onto GPUs, one can eliminate I/O and interconnect bottlenecks that traditionally reduce throughput and increase latency.

NVIDIA’s Blackwell architecture gives a [hint](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/) of this future by incorporating a Decompression Engine directly on the GPU for accessing compressed data and decompressing large amounts of data very quickly. With support for popular compression formats such as LZ4, Snappy, and Deflate.

The next generation of systems will use GPUs for training an AI/ML model as well as ingesting compressed data, parsing it, augmenting it, and performing database joins and filtering operations. All of this will happen within the GPU. In fact, NVIDIA is actively working toward this with initiatives like RAPIDS (GPU-accelerated Spark and ETL) and the integration of Apache Arrow data formats into GPU memory. A GPU-based system will be able perform the data ingest, analytics, training, and inference without requiring round-trips to the CPU.

With Grace-Blackwell, we already see the GPU able to directly pull data from the CPU’s memory at 900 GB/s and decompress it in real-time. GPUs may gain more logic or optimized cores for things like SQL query processing, graph traversal, or even running web servers for inference without needing any CPU resources. This may or may not be ideal for certain workloads, but the option would be worth testing.

The motivation is clear. By avoiding the latency of moving data between different processors, a GPU can load a massive dataset from NVMe, decompress and filter it, and then immediately start training a model on the filtered data. This greatly reduces overhead and complexity.

We’re already seeing Blackwell’s ability to accelerate end-to-end data analytics pipelines as the NVIDIA’s [blog](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/#:~:text=Data%20analytics%20and%20database%20workflows,of%20data%20for%20data%20analytics) mentions speeding up Spark queries by doing decompression on the GPU with Grace’s CPU memory acting as a buffer.

AI systems performance is about optimizing the entire full-stack workflow - not just the ML math. That means GPUs are becoming a one-stop compute engine. It wouldn’t be surprising if future NVIDIA GPUs come with integrated NICs and DPU-like capabilities on-die so they can ingest networked data and pre-process it without any CPU intervention. We may soon see a collapse of the once multi-stage pipeline. AI accelerators will handle from data ingestion to insight. This would blur the boundaries between data engineering and model training - at the benefit of increased AI system performance.

# High-Throughput, Low-Latency Inference Orchestration

With the proliferation of generative AI models and other massive AI services, serving models efficiently is a high priority. Inference serving is another example of software evolving hand-in-hand with GPU capabilities. NVIDIA’s open source Dynamo inference framework, launched in 2025, is an example of this trend toward specialized orchestration for generative AI.

Dynamo can boost the throughput of LLM inference by up to 30× on new Blackwell GPU clusters according to their launch [blog post](https://developer.nvidia.com/blog/introducing-nvidia-dynamo-a-low-latency-distributed-inference-framework-for-scaling-reasoning-ai-models/#:~:text=NVIDIA%20announced%20the%20release%20of,AI%20researchers%C2%A0%20to%20accelerate%20AI). It uses advanced inference strategies like disaggregating the decoding stage from the prefill stage - and spreading it across GPUs. Dynamo also supports dynamic load balancing and smart caching of model states.

Dynamo also introduces cache-aware scheduling. For example, routing requests in a way that avoids redundant recomputation of Transformer KV caches for each prompt. With techniques like token-level scheduling, batched pipelines, and memory-optimal placement, Dynamo ensures that no GPU cycle is wasted. The result is snappier, more-response inference service with lower cost per query - even as model sizes scale up to the trillions.

These optimizations dramatically increase the number of requests that each GPU can handle. In practice, an H100 or Blackwell GPU farm running Dynamo will serve many more simultaneous chatbot users at lower latency than the same hardware using a generic scheduler.

# Silicon Photonics and Optical Interconnects

As GPU clusters continue to grow in scale and require more network bandwidth, optical communication is positioned to become a game changer. Soon, we expect the copper wiring that currently connects GPUs and racks will largely be replaced by optical fibers and co-packaged optics (CPO). NVIDIA has already [announced](https://nvidianews.nvidia.com/news/nvidia-spectrum-x-co-packaged-optics-networking-switches-ai-factories) prototype Spectrum-X and Quantum-X photonic switches that integrate silicon photonics right into the data center switching fabric - yielding 1.6 Tb/s per port throughput.

In addition, these co-packaged optical switches offer about 3.5× better power efficiency than traditional electrical Ethernet/InfiniBand switches and can maintain signal integrity over far longer distances. In practical terms, this means that future GPU racks could be directly connected by optical links. This will eliminate the bandwidth vs. distance tradeoff.

We’ll see optical NVLink or NVLink-over-optical for connecting GPUs across cabinets - or even between server rooms - with minimal latency hit. And AI superclusters with tens of thousands of GPUs will likely use fully optical backbones and create a single coherent system across the data center.

NVIDIA’s vision is to scale AI factories to millions of GPUs across multiple physical locations. This simply isn’t feasible today with copper interconnects due to attenuation and energy costs. Photonics is the only path forward here. By moving photons instead of electrons, networks will reach** **bandwidth densities of multiple terabits per second per link, with much lower power per bit.

Soon, we will see co-packaged optical transceivers sitting right next to GPU dies which convert high-speed electrical signals to light right at the source. This would slash the energy needed for GPU-to-GPU communication and largely remove network bottlenecks for distributed training and inference.

The latter half of the 2020s will likely witness optical links becoming commonplace in AI infrastructure, from on-card interconnects up through rack and datacenter scales. This is an essential step in creating ultra-scale GPU clusters.

# Globally Distributed Data Centers (AI Factories)

The scale of AI deployments will transcend single data centers and locations. We’re likely to see planet-spanning AI systems where multiple data center “nodes” work together on training or inference as a unified resource. To connect distant GPU clusters with manageable latency, we’ll need to utilize ultra-high-bandwidth data center connections such as optical networks.

NVIDIA’s CEO often talks about “AI factories” or AI supercomputers. He envisions connecting these factories together with photonic switching and optical fiber. When this happens, it will be possible to treat, for example, three 1,000,000-GPU clusters on three continents as one massive 3,000,000-GPU cluster for a massive, ultra-scale training run.

In fact, the Spectrum-X photonics platform unveiled in 2025 explicitly [mentions](https://nvidianews.nvidia.com/news/nvidia-spectrum-x-co-packaged-optics-networking-switches-ai-factories#:~:text=GTC%E2%80%94NVIDIA%20today%20unveiled%20NVIDIA%20Spectrum,optical%20communications%20at%20massive%20scale) scaling to millions of GPUs across sites. In a decade or so, networking and distributed systems might advance enough to make this practical for select workloads. We might witness a single neural network training session running across Tokyo, London, and Silicon Valley simultaneously - synchronized over transoceanic optical links.

The trend here is that datacenter-scale becomes global-scale. From a performance standpoint, the challenges are immense given speed of light limitations, fault tolerance, power and energy, etc. But the industry might mitigate these challenges by algorithmic means. Some examples include clever asynchronous update schemes that tolerate latency or tensor, pipeline, and expert parallel strategies that reduce inter-node communication.

If successful, the effective compute to train or serve a massive AI model becomes unbounded by location. One concrete offshoot of this could be truly real-time planetary inference services. Imagine an AI model that is distributed globally such that responses are computed partly in each region and aggregated before returning back to the end user.

The seeds of this are visible today in content delivery networks and distributed databases, but soon they will extend to distributed neural networks. NVIDIA’s acquisition of Mellanox in 2019, for example, suggests they are eyeing this future. They want the network to be as fast as the GPU so that location doesn’t matter. Performance engineering might involve optimizing within a single cluster as well as across clusters across the world. The global, unified, wide-area networks become part of the system design. The AI of the future could very well treat the whole globe as its computer.

# Smart Networking and Distributed Compute Offload

In tandem with optical interconnects, the future will heavily rely on network offload engines like BlueField Data Processing Units (DPUs) to support scaling to AI supercomputers with thousands and millions of GPUs. As individual GPUs become faster, feeding them with data and coordinating them in large clusters becomes a serious challenge.

As described in an earlier chapter, BlueField DPUs are essentially smart network interface controllers (NICs) with specialized ARM-based network processors that handle everything from RDMA communication to storage access. DPUs free the CPUs and GPUs from those tasks - allowing them to focus on higher-level, more-useful goodput activities. For example, offloading collective communication algorithms to a DPU can let a GPU focus purely on math while the DPU handles the all-reduce of gradients.

An NVIDIA sustainability [report](https://www.nvidia.com/en-us/sustainability/#:~:text=Less%20Power%20Consumption) shows that current DPUs reduce CPU utilization and overall system power needs by 25%. They do this by handling data movement more efficiently.

As GPU clusters continue to scale, the role of DPUs will continue to expand. We’ll soon see DPUs deeply integrated into AI systems. They may handle real-time load balancing, cache parameters for partitioned models, and perform security tasks like encryption - all with minimal latency. We will likely get to a point where each server node has an embedded DPU that essentially acts as an intelligent dispatcher coordinating dozens of GPUs.

Another innovation enabled by DPUs is in-network computing like NVIDIA’s SHARP protocol. With SHARP and DPUs, the NVIDIA network switches can perform reductions and aggregations as the data traverses the network. This accelerates distributed training and inference by a large factor. Combining in-network computing with network offloading, DPUs help the AI cluster improve its overall throughput and latency.

Soon, a typical AI system will involve hierarchical communication patterns where DPUs and smart switches perform most of the synchronization work for gradients, model weights/layers, optimizer states, and activations. The performance impact will be significant as there will be less diminishing returns when scaling to a very large number of GPUs and compute nodes - effectively linear scaling. This is a future where the network is no longer a passive conduit of data. Instead, the network is an active part of the overall AI compute stack. The network will collaborate closely with GPUs to efficiently run massive models across thousands of nodes and millions of GPUs.

# HBM Memory Stacked on GPUs (3D GPUs)

One of the most transformative GPU hardware trends on the horizon is the reducing the physical distance between the GPU and its memory. For example, merging memory and GPU cores into the same package or die. Today’s HBM memory stacks sit beside the GPU on a silicon interposer substrate. The interposer routes and connects the GPU to the HBM memory, but its relatively-long wiring contributes to latency and reduced bandwidth. However, future iterations, currently in development, aim to stack memory directly on top of GPU compute tiles which bonds them in 3 dimensions. This increases bandwidth and reduces latency between the GPU and HBM.

SK Hynix, a major semiconductor company specializing in memory products like HBM, and NVIDIA are working together on a radical 3D [redesign](https://www.tomshardware.com/news/sk-hynix-plans-to-stack-hbm4-directly-on-logic-processors#:~:text=SK%20Hynix%20has%20started%20recruiting,may%20transform%20the%20foundry%20industry) for next-generation GPUs that would implement the 3D stacking. They would mount HBM4 memory stacks directly on top of the GPU processor which would remove the need for the interposer. This approach would dramatically increase bandwidth and reduce latency between logic and memory. This increase is likely an order of magnitude leap similar to the performance advantage of on-chip SRAM over off-chip DDR RAM.

By eliminating the interposer with the 3D design, each bit would only travel microns from the HBM cell to the GPU processor. This would enable interfaces, or data channels, far wider than 1024 bits per stack. 3D stacking would allow larger amounts of data to be transferred concurrently - boosting overall memory bandwidth.

The 3D design will dramatically change chip fabrication as SK Hynix would essentially become a quasi-foundry by integrating and stacking its chips onto GPU processors produced by NVIDIA. If successful, this could completely transform the semiconductor manufacturing industry and blur traditional boundaries between fabless chip-design companies and chip-fabricating foundries.

For AI system performance, the implications are profound. Memory bandwidth has long been a bottleneck - especially for large models and memory-bound operations such as token generation/decoding. With logic-memory stacking, future GPUs might have petabytes per second of internal bandwidth.

With 3D stacking, we could feed data to tens of thousands of Tensor Cores with virtually no starvation. Imagine each GPU streaming multiprocessor (SM) having a slice of HBM memory right on top of it. This would turn HBM into an extension of the faster L2 cache. Removing the interposer in the 3D design would also drastically improve energy efficiency as, currently, a lot of GPU energy is spent transmitting electrical signals across the interposer from the GPU to the HBM memory.

Very soon, we expect at least one generation of NVIDIA GPUs to adopt a form of compute-memory 3D stacking. It might start as a multi-chip module where memory dies sit on top of compute dies in a chiplet-on-chiplet architecture. Eventually, perhaps we’ll see GPU processors and HBM intermixed in the same 3D stack.

Performance engineers in the near future might not even talk about memory bandwidth as a bottleneck. Instead, they’ll focus on other factors because the GPU’s memory is effectively on-die. Achieving this is hugely challenging, however, since cooling the 3D stack - and maintaining high yields for this combined stack - is difficult. Regardless of the challenges, all signs indicate that this technology is progressing, and it will happen sooner than later.

# Energy Efficiency and Sustainability at Scale

As models and GPU clusters grow exponentially, so do their appetites for power. Energy efficiency is fast becoming a first-class goal in AI system design - alongside raw performance. We’ve already witnessed dramatic gains on this front. For example, NVIDIA’s Grace-Blackwell GB200 NVL72 rack-scale system delivers on the order of 25× more performance at the same power [compared](https://www.nvidia.com/en-in/data-center/gb200-nvl72/) to a previous-generation air-cooled H100-based setup. The GB200 NVL72’s leap in energy efficiency comes from many factors including improved GPU architecture, advanced liquid cooling, and better ability to keep all those chips working at high utilization. Simply put, the NVL72 does more useful work per joule of energy.

For performance engineers, this means power is now a key constraint to consider when optimizing your system. Performance-per-dollar used to be the primary metric to optimize, but now performance-per-watt is equally as critical. Every optimization must consider energy efficiency. Structured sparsity is a great example. By skipping unnecessary calculations, you save both power and time. If half of a matrix can be zeros with minimal accuracy loss, exploiting that sparsity gives you essentially the same result for roughly half the energy. We can expect future hardware and software libraries/frameworks to provide even more support for sparsity and other efficiency tricks, so it’s wise to design models with these power-saving techniques in mind.

Another emerging best practice is dynamic power management tied to workload phases. For example, sometimes the GPUs are in a compute-bound phase, and other times they’re in a memory-bound or communication-bound phase. Modern AI systems take advantage of these phase transitions. For example, when your code enters a memory-bound phase, a smart scheduler might temporarily lower GPU clock speeds or voltages to save power and reduce heat because a full throttle compute isn’t necessary when the bottleneck is memory-bound. Then, when your code enters a compute-heavy phase, the system can ramp clocks and voltages back up to maximize throughput. This kind of dynamic voltage-frequency scaling may soon happen automatically under the hood. An AI-powered controller could learn the optimal power settings dynamically on the fly.

Sustainability concerns are also bringing renewable-energy awareness into AI operations. We might schedule power-hungry training jobs for times of day when green energy is plentiful. There’s active research on aligning AI workload scheduling with renewable energy availability. Imagine a future dashboard that shows GPU utilization and the percentage of power coming from carbon-free sources. Companies might start bragging, “We trained a 10 trillion-parameter model in 2 months - and we did it with 80% renewable energy!” As a performance engineer, you may find yourself collaborating with facility power managers, thinking about when and where to run jobs to best tap into available power with minimal carbon footprint.

In short, ultra-scale AI computing must scale wisely given power constraints. Pushing for every ounce of FLOPS is great, but doing so efficiently is the real win. The good news is that AI systems such as the Grace-Blackwell NVL72 show that the industry is heavily investing in efficiency. Our job will be to leverage these advancements by keeping the GPUs busy but not wasteful, exploiting features like low-precision modes and sparsity, and designing workflows that sip power frugally when they can. This is a relatively-new mindset that treats power as a scarce resource that needs to be optimized and budgeted alongside time and money.

# Advanced Cooling and Energy Reuse Techniques

Data centers are already dealing with the reality of higher power densities per rack. The GB200 NVL72 currently consumes 120 kW, but this will likely exceed 200 kW in the near future. With sustainability in mind, we need wider adoption of cutting-edge cooling solutions like single-phase and two-phase immersion cooling, cold plates with evaporative cooling, and even on-chip micro-fluidic cooling. Some AI data centers are already experimenting with immersion cooling by submerging entire server boards in dielectric fluid. They [report](https://www.cnbc.com/2024/08/27/nvidia-partner-sustainable-metal-cloud-ai-data-center-energy-consumption.html#:~:text=This%20Nvidia%20partner%20can%20help,typically%20used%20in%20data%20centers) up to a 50% reduction in energy consumption for liquid cooling vs. traditional air cooling.

Soon, many AI clusters will be designed from the ground up to use immersion cooling technology. This improves energy efficiency and allows the waste heat to be captured more easily via heat exchangers. We will likely see large AI compute farms integrated with facilities that use this captured heat for other processes - effectively recycling the energy.

On the energy supply side, NVIDIA and others will continue their efforts to power these “AI factories” with renewable energy and clever energy storage mechanisms to offset the energy demands during peak loads. The performance impact here is that thermal constraints are a key limiter of sustained performance. If you can keep a GPU 20°C cooler, it can potentially maintain higher clocks and avoid thermal throttling during multi-day training runs. Superior cooling translates directly into faster and more reliable throughput. Most high-end GPU clusters will be immersion-cooled going forward.

We will also see more standards for liquid-cooled racks become mainstream. Just as the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) created guidelines for air-cooled data centers, the industry will standardize liquid-cooled rack configurations for HPC and AI clusters. In fact, in 2024, NVIDIA [contributed](https://developer.nvidia.com/blog/nvidia-contributes-nvidia-gb200-nvl72-designs-to-open-compute-project/) their GB200 NVL72 rack and cooling designs to the Open Compute Project (OCP) to help set an industry standard for efficient, scalable, and liquid-cooled systems.

This trend will enable data centers to continue increasing compute density without hitting a thermal wall. Imagine a single 48U rack housing 100+ petaflops of FP8 compute. Currently, this would only be feasible with advanced cooling systems.

AI system performance isn’t just about FLOPS. It’s about removing the bottlenecks that prevent those FLOPS from being fully utilized. And one of those bottlenecks is heat. The industry will continue to respond with novel cooling and energy-efficiency techniques that allow GPUs to run hotter, denser, and greener.

# Hybrid Classical-Quantum Computing (CUDA Quantum)

Looking a bit further out on the horizon, we have quantum computing. It’s no secret that if practical quantum computers emerge, they could revolutionize certain types of computation. While general-purpose quantum AI is not here yet, we’re starting to see the early steps toward integrating quantum accelerators into the AI stack. An image of the X quantum computer is shown in [Figure 8-5](https://learning.oreilly.com/library/view/ai-systems-performance/9798341627772/ch08.html#ch11_figure_5_1744914899744909).

![A close-up of a machine  AI-generated content may be incorrect.](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9798341627772/files/assets/ch11_figure_5_1744914899744909.png)

###### Figure 8-5. Google Quantum Computer (Source: https://quantumai.google/quantumcomputer)

NVIDIA has been preparing for this quantum future with its [CUDA Quantum](https://developer.nvidia.com/cuda-q) (CUDA-Q) initiative which provides a unified programming model for hybrid quantum-classical computing. The basic idea is that tomorrow’s HPC clusters might have quantum processing units (QPUs) sitting alongside GPUs and CPUs - each tackling tasks for which they are uniquely optimized.

In the near term, quantum processors will likely remain relatively small and specialized. They won’t be training a transformer with 100 trillion parameters end-to-end, for example, as current quantum devices simply can’t handle that scale of data. However, they could be useful as accelerators for particularly-hard problems that arise within AI workloads. For example, some optimization problems or sampling tasks might be sped up by quantum methods.

Consider a large generative model that needs to sample from an extremely complex probability distribution as part of its operation. A quantum annealer or small gate-based quantum computer might be able to perform that sampling much more efficiently by leveraging quantum superposition or tunneling. If so, a hybrid algorithm would let the GPU handle the bulk of neural network computation, then offload this special sampling step to the QPU. CUDA Quantum is positioned as the plumbing to make this type of integration seamless.

As a performance engineer, this means if and when quantum computing hits the point of usefulness for AI, we won’t have to reinvent our entire software stack. We’ll simply treat the QPU as another device with its own kernels and optimization techniques. This is similar to how we treat the GPU today relative to the CPU. In fact, CUDA-Q already allows you to write a single program where you call both GPU kernels and quantum kernels. You can even simulate the quantum parts on a GPU for development as described in an NVIDIA [blog post](https://developer.nvidia.com/blog/performant-quantum-programming-even-easier-with-nvidia-cuda-q-v0-8). This is great because we can start writing and testing hybrid algorithms today using simulations since actual large QPUs don’t yet exist at scale. This way, we’ll be ready to run them on real quantum hardware when this technology matures.

So what should we, as performance specialists, watch for in this realm? First, keep an eye on quantum hardware’s progress - especially any tasks where a quantum accelerator shows even a hint of speedup versus a GPU. As of this writing, no one has demonstrated a clear quantum advantage for mainstream ML tasks, but research is ongoing in things like quantum kernels for ML and quantum-inspired tensor network methods. If a breakthrough comes, it will be the trigger that hybrid classical-quantum computing is ready for prime time. For example, let’s say a QPU computes a large matrix multiply or solves a complex combinatorial optimization problem faster than a GPU. This would be a huge step towards a quantum future.

If this happens, a performance engineer must understand the data movement and latency between the CPUs, GPUs, and QPUs in a system. We’ll need to ensure that the time gained by the QPU isn’t lost waiting on data transfers or device synchronizations. This is reminiscent of the classic heterogeneous CPU-GPU computing challenges, but with an even-more exotic and powerful device.

In practical terms, nothing major changes right now. Most of us won’t be working with quantum hardware in the very immediate future. But it’s likely that within a decade, parts of some AI workloads will run on fundamentally different computing substrates within a single AI system. Similar to how GPUs introduced a new paradigm alongside CPUs, QPUs bring their own characteristics and optimizations into the mix. The complexity of our job will jump again as we’ll be profiling and optimizing across CPU, GPU, and QPU (and others?) all in one application. Fun times ahead! The good news is that frameworks like CUDA-Q are already thinking about how to present a coherent platform for these new devices.

It’s worth noting that you can currently use GPUs to simulate quantum algorithms during development. NVIDIA’s cuQuantum SDK, for instance, allows fast simulation of moderate-qubit counts on GPUs. If someone designs an AI algorithm involving, say, a quantum program that requires 30 qubits, we could simulate those qubits on the GPU during development and testing.

This means HPC GPUs might be running neural-network code together with quantum-circuit simulations. Optimizing the quantum circuit simulations is actually quite analogous to optimizing neural network operations - optimizing linear algebra and matrix operations corresponding to quantum gates.

So in a funny way, high-level optimization principles transfer across devices. It’s all about efficient linear algebra, memory bandwidth utilization, and parallelism. Performance engineers who are curious about quantum can contribute on the simulation side right now.

Quantum for AI is a speculative play, but the groundwork is being laid now. The best practice here is simply to stay curious and open-minded. Play around with quantum programming frameworks and simulators to understand the basics of how QPUs work. Keep track of developments that show potential for AI acceleration. That way, if and when the quantum era arrives for AI workloads, you’ll be ready to hit the ground running rather than playing catch-up with entirely new concepts. It’s an exciting frontier where completely new performance engineering challenges - and opportunities - await.

# Scaling Toward 100-Trillion-Parameter Models

Finally, let’s revisit the relentless march toward ultra-scale, 100-trillion parameter models. We’ve already broken the trillion-parameter threshold. Now the question is how to scale to tens or even hundreds of trillion parameter models in the coming years. What does that kind of model demand from our systems, and what innovations are needed to make training such a powerful model feasible? This is where everything we’ve discussed comes together including efficient hardware, smart software, clever algorithms. Reaching 100 trillion parameter models will require using every trick in the book - and then some tricks that may not have been discovered yet. Let’s dive in!

On the hardware front, the obvious need is for more memory and more bandwidth - preferably right on the GPU. If you have 100 trillion parameters and you want to train them, you need to store and move an insane amount of data efficiently. The next generations of memory technology will be critical. High Bandwidth Memory (HBM) keeps evolving: HBM3/3e is in the current generation of GPUs with HBM4 likely coming to the next generation of GPUs. HBM4 doubles bandwidth per stack again - on the order of 1.6 TB/s per stack. It will also increase capacity per stack to possibly 48GB or 64GB per module.

HBM4’s higher capacity and throughput means that future GPUs could have, say, 8 stacks of HBM4 at 64GB each which totals 512 GB of super-fast HBM RAM on a single board with over 10 TB/s aggregate memory bandwidth. That kind of local HBM capacity could hold a lot of model parameters directly on each GPU - drastically reducing the need to swap data in and out. It’s not hard to see how this enables larger models, higher-bandwidth training runs, and lower-latency inference servers: what used to require sharding across 8 GPUs might fit in one, what required 100 GPUs might fit in 10, and so on.

In addition to multi-chip architectures like the Grace-Blackwell superchip, multiple racks of NVL72’s each can be linked into one giant cluster to create hundreds of GPUs sharing a unified fast network. Essentially, your cluster behaves like a single mega-GPU from a communication standpoint. This is important for scaling to 100 trillion parameters because it means we can keep adding GPUs to get more total memory and compute - without hitting a communication bottleneck wall. This assumes that NVLink (or similar) continues to scale to those ultra-scale sizes.

However, hardware alone won’t solve the 100-trillion parameter challenge. Software and algorithmic innovations are equally, if not more, important. Training a model of that size with naive data parallelism, for example, would be incredibly slow and expensive. Imagine the optimizers having to update 100 trillion weights every step! We will need to lean heavily on techniques that reduce the effective computation. One big area that we explored is low numerical precision. In addition to FP8 and FP4, future hardware might support even lower (1-bit) precision for some parts of the network. Hybrid schemes will likely be critical to use lower precision for most of the model, but higher precision for sensitive parts.

As performance engineers, we should watch for these new capabilities and be ready to use them. To train 100 trillion parameter models, you very likely need to use low precision for efficiency, otherwise the workload would be prohibitively slow and expensive. The good news is that hardware and libraries will make this transition relatively seamless as we’ve seen including first-class CUDA and PyTorch low-precision support for NVIDIA’s Tensor Cores and Transformer Engine, for example.

Another critical approach is sparsity and conditional computation. We already use sparse activation in models like Sparse Mixture-of-Experts (MoE), where only a fraction of the model’s parameters are active for a given input. This idea can be generalized so that you don’t always use the full 100T parameters every time; you use just the parts you need. Models using the MoE architecture are proving to be very capable and efficient. By the time 100 trillion parameter models arrive, I expect a lot of them will need to be sparsely activated.

As performance engineers, the implication is that throughput will be about matrix multiplication speed as well as the efficiency of MoE conditional routing, caching of expert outputs, and communication patterns for sparse data exchange. This adds complexity, but also opportunity. If you can ensure the right experts are on the right devices at the right time to minimize communication, you can drastically accelerate these massive models.

We should also consider algorithmic efficiency improvements. Optimizers that use less memory could be vital. The traditional Adam optimizer variants typically keep two extra copies of weights for momentum and variance estimates. This effectively triples memory usage. So if you have 100-trillion parameter weights, you need an extra 200 trillion values to hold the optimizer states! Memory-efficient optimizers like Adafactor and Shampoo help to reduce this overhead.

Techniques like gradient checkpointing help to trade compute for memory by recomputing activations instead of storing them. At a 100 trillion parameter scale you’d almost certainly be checkpointing aggressively. An even more radical idea is, perhaps, we don’t update all weights on every step. Consider updating subsets of weights in a rotating fashion - similar to how one might not water every plant every day, but rotate through them. If done wisely, the model still learns effectively but with less frequent updates per parameter. This reduces the total computational needs of the system.

These kinds of ideas blur into the algorithm design realm, but a performance-aware perspective is useful. We should ask, “Do we really need to do X this often or at this precision?” for every aspect of training and inference. Often the answer is that we can find a cheaper approximation that still works. At a 100 trillion parameter scale, these approximations can save months of time or millions of dollars.

An often overlooked aspect of ultra-scale training is infrastructure and networking. When you’re talking about clusters of 10,000+ GPUs working on one model, the network fabric becomes as important as the GPUs themselves. Ethernet and InfiniBand technologies are advancing in terms of increased throughput and smarter adaptive routing techniques, etc.

NVIDIA’s Spectrum-X is a recent example of an Ethernet-like fabric optimized for AI, promising less congestion and high bisection bandwidth. We may also see more usage of advanced, open industry standard interconnects like Compute Express Link (CXL) that enable high-throughput, low-latency shared memory access across compute nodes to form a massive global memory pool accessible by NVIDIA’s GPUDirect RDMA technology, for example.

Performance engineers will need to deeply understand these tiers and ensure that data is in the right place at the right time. The goal will be to simulate a huge memory space that spans GPUs and CPUs, so that even if a model doesn’t fit in one machine, it can be treated somewhat transparently by the programmer. Some of this is happening already today with unified memory and paging systems, but at a 100 trillion parameter scale, this functionality will really be put to the test.

It’s not surprising that frontier research labs like xAI, OpenAI, and Microsoft are [reportedly](https://semianalysis.com/2024/05/07/openai-is-doomed-et-tu-microsoft/#:~:text=Microsoft%20has%20plans%20for%20a,Many%20firms%20use) planning large, single clusters of 100,000 and 1,000,000 GPUs. At a 100-trillion parameter scale, you might have one job spanning an entire datacenter’s worth of hardware. Performance engineers must think at datacenter and multi-datacenter (global) scale.

Lastly, there’s a socio-technical trend as models - and their required compute - scale up. It may become infeasible for any single team - or even single corporation - to train the biggest models alone. We (hopefully) will see more collaboration and sharing in the AI community to handle these enormous projects. This would be analogous to how big science projects - like particle physics experiments - involve many institutions. Initiatives similar to the [now-dissolved](https://blog.opencollective.com/open-collective-official-statement-ocf-dissolution/) Open Collective Foundation could pool compute resources to train a 100-trillion-parameter model, which is then shared with the world.

This will require standardizing things like checkpoint formats, co-developing training code, and thinking about multi-party ownership of models. While this is not a performance issue per se, it will influence how we build large AI systems. We’ll need to make them even more fault-tolerant and easily snapshot-able to share partial results. As an engineer, you might end up optimizing for pure speed as well as reproducibility and interoperability. This allows different teams to work on different parts of the training and inference workflow smoothly and efficiently.

Reaching 100T models will require holistic, full-stack innovation. There’s no single solution to this challenge. Instead, every piece of the puzzle must improve. Hardware needs to be faster and hold more data. Software needs to self-optimize more - and use resources more efficiently through compilers, AI assistants, and real-time adaptation. Algorithms need to be clever about not avoiding unnecessary work through sparsity, lower precision, and better optimizers.

The role of the performance engineer will be to integrate all these advancements into a coherent workflow. It’s like assembling a high-performance racing car. The engine, tires, aerodynamics, and driver skill all have to work in unison. If we do it right, what seems impossible now - e.g. 100 trillion parameters trained without breaking the bank - will become achievable. It wasn’t long ago that 1 and 10 trillion-parameter models sounded crazy, but they’re being done today. So history suggests that with ingenuity, we’ll conquer this next milestone too.

# Key Takeaways

Unified Computing Engines.Modern GPUs are evolving to become universal processors that can handle both traditional high‑performance scientific (HPC) simulations and AI workloads. This convergence means that traditional distinctions between “AI supercomputers” and HPC systems are blurring. Performance engineers will increasingly apply techniques from both domains in an integrated way including mixed precision, sparsity, and scheduling algorithms.

Seamless Workload Integration.Future systems will allow HPC tasks like exascale weather simulations to blend with neural network acceleration seamlessly. Programming models may support on‐the‑fly switching between tensor operations used in deep learning and double‑precision arithmetic critical for scientific simulation. This paves the way for more versatile computing systems.

Exponential Compute Scaling.Next‑generation AI data centers are being designed to provide orders‑of‑magnitude increases in computational capacity. These facilities will train AI models with compute budgets far beyond today’s levels. This enables training runs that use 100 to 1,000 times the FLOPs used in current systems.

Evolving AI Models and Agents.Future models will be self‑improving systems capable of generating and optimizing code, continuously updating their weights with fresh data, and even rewriting their own code. This perpetual cycle of learning and refinement will reduce the time between breakthroughs and create a virtual workforce that outperforms human teams in research and R&D tasks.

Automation at the Software Stack.A central theme is the shift from manual kernel optimizations toward increasingly autonomous software toolchains. Modern AI frameworks such as PyTorch, TensorFlow, and JAX are incorporating smart compilers and execution‑graph optimizers that automatically fuse kernels, tune launch parameters, and optimize memory transfers.

AI‑Assisted Optimization.Emerging AI‑powered assistants will continually offload many performance‑tuning tasks from human engineers. In the near future, the vast majority of low‑level optimizations will occur automatically, freeing performance engineers to focus on high‑level strategies and verification.

Autonomous Scheduling and Cluster Management.Future systems will use AI to monitor and dynamically optimize entire clusters. Smart scheduling agents will analyze real‑time data to co‑locate jobs with complementary workloads, maximize resource utilization, and mitigate bottlenecks by adjusting parameters such as GPU memory usage and clock speeds on‑the‑fly.

Real‑Time Troubleshooting.In addition to scheduling, AI co‑pilots will monitor system logs and training runs to detect anomalies quickly. This includes automated debugging, failure analysis, and even learning optimal configurations through reinforcement learning to maximize performance per watt and per unit time.

Exploiting Inherent Data Sparsity.As models grow, so does the opportunity to avoid unnecessary operations. Hardware and software will increasingly leverage techniques that activate only relevant portions of a network (e.g. mixture‑of‑expert architectures) to double effective throughput and improve energy efficiency. Sparse and conditional computation models will become central to maximizing “goodput” rather than raw TFLOPS.

Dynamic Graph Execution.Compilers will continue to improve by tracing dynamic execution patterns and automatically reorganizing computation graphs. This is vital for both training ultra‑large models and delivering low‑latency inference services as it will reduce overhead by minimizing redundant data transfers and synchronizations.

Silicon Photonics and Optical Networking.To overcome bandwidth and latency issues in scaling out GPU clusters, traditional copper interconnects will be supplanted by optical fibers and co‑packaged optics. These technologies promise terabit‑per‑port speeds with dramatically lower power consumption, enabling global, unified data centers with minimal communication delays.

3D Memory Integration.In the near future, there will be developments in stacking High‑Bandwidth Memory (HBM) directly onto GPUs. By dramatically reducing the physical distance between compute cores and memory, these 3D designs promise massive bandwidth improvements and reduced energy losses, effectively eliminating traditional memory bottlenecks.

Performance‑Per‑Watt is a Critical Metric.As compute scales up, energy consumption becomes a first‑class concern. Innovations in low‑precision arithmetic, structured sparsity, and dynamic power management will become essential to ensure that performance gains do not come at unsustainable energy costs. It will become critical to intelligently adjust voltage/frequency based on workload phases to improve energy efficiency without sacrificing performance.

Advanced Cooling and Energy Reuse.Cooling techniques will continue to evolve including immersion cooling and micro‑fluidic systems. These will enable higher compute density and allow for the reuse or recycling of waste heat. These technologies are vital for maintaining high performance in data centers while keeping energy use and carbon footprints in check.

Integration of Quantum Computing.While practical quantum computing is still on the horizon, initiatives like NVIDIA’s CUDA Quantum highlight early efforts to integrate quantum accelerators alongside classical GPUs. This hybrid approach is poised to boost performance for specific tasks such as combinatorial optimization and certain simulation tasks - without overhauling the existing software stack.

From Data Center to Global AI Factories.Soon, individual data centers will be interconnected worldwide and not isolated. These “AI factories” will operate across continents, with ultra‑high‑bandwidth optical networks seamlessly linking millions and billions of GPUs into a single, coherent global resource. The performance engineering challenges here include local optimization and cross‑data‑center synchronization and efficient routing of aggregated results.

Holistic Full‑Stack Innovation.The drive to scale AI models to 100 trillion parameters combines everything from hardware breakthroughs to software optimizations and novel algorithmic approaches. This scale demands integrating low‑precision training, sparse computation, advanced optimizer designs, and robust infrastructure capable of handling enormous amounts of data with minimal latency.

Collaborative and Reproducible Research.As the models grow, so does the complexity of their training and deployment. Successful scaling will depend on standardized formats, collaborative frameworks, and global data sharing and reproducibility efforts that ensure research can be effectively pooled and scaled beyond what any single team or institution can handle alone.

# Conclusion

AI Systems Performance Engineering is entering its golden era. This is both challenging and exciting. The frontier of what’s possible keeps expanding. Models are hundred times larger. The demand for real-time inference to power advanced reasoning and AGI models is reaching an unprecedented scale. New hardware paradigms continue to emerge, and AI optimizing AI is a reality. The case studies we examined earlier each highlighted how creative engineering turns ambitious ideas into reality. Now, the emerging trends we’ve discussed will carry those lessons forward into the future.

One overarching theme is the shift from brute-force scaling to intelligent scaling. In the past, to go faster, you might simply throw more GPUs at the problem. In the future, that won’t be enough. It simply won’t be cost or power efficient. We have to scale wisely. This means embracing algorithmic efficiencies so we’re performing more useful work, or goodput. Using the latest hardware features - like Grace-Blackwell’s fast interconnects and huge memory - to their fullest. It means letting automation – sometimes AI itself – assist in optimization because systems have grown beyond what any single human can manage. Essentially, performance engineering is moving up a level of abstraction. We need to set up the right environment and incentives for the system to auto tune itself and continue to run optimally - rather than manually tuning every knob ourselves.

The tone of this future is not one of obsolescence for the performance engineer, but of augmentation with the engineer. Yes, many manual tasks will be taken over by smart compilers and AI agents, but our expertise is still crucial to guide these tools. We’ll spend more time on high-level architecture decisions, experiment design, interpreting results, and steering the automation in the right direction. The “engineer’s intuition” will remain extremely valuable. For example, we intuitively know when a communication pattern is inefficient or a certain metric indicates a bottleneck. We’ll just apply it through new interfaces by telling the AI assistant, “Focus on optimizing the all-reduce in this section, I suspect that’s our slow point,” rather than re-writing the all-reduce code ourselves.

Importantly, the performance engineer’s purview is expanding. It’s not just GPU kernels or single-node optimizations. It now includes system-wide, facility-wide, and global efficiency. Not to mention reliability, sustainability, and collaboration considerations. We’re becoming the architects of AI compute ecosystems that are immensely complex. It’s a bit like going from being a car mechanic incrementally fixing parts of the car to an automotive engineer designing and building an entire vehicle from scratch. The scope is much broader. This will require continuous learning and adaptation. The pace of new developments for hardware, software, and algorithms will not slow down. So neither should we slow down our understanding of these innovations.

We are entering a period where the full stack is being reinvented - sometimes by AI itself - to feed the insatiable appetite of these ultra-scale models and to push the frontier of what AI can do. It’s a future where a GPU is more than a chip - it’s an integrated computing world where optics, quantum, and silicon all intertwine to create intelligent machines at previously unthinkable scales. And as these trends mature, the boundary of what’s possible in AI will keep expanding, driven forward by the engineers and researchers dedicated to bending hardware and software to their will.

The coming years will likely bring breakthroughs that today feel barely imaginable. Just as a few years ago, training a multi-trillion parameter model felt out of reach, yet we achieved it. When we hit roadblocks, whether it’s a power constraint or a memory wall or a convergence issue in a giant model, the collective ingenuity of this field finds a way around it. And performance engineering sits at the heart of that innovation, turning lofty AI ambitions into practical reality.

In closing, the best practice I suggest is to stay curious, stay adaptable, and lean into the future. Don’t be afraid to experiment with that new compiler, or trust an AI recommendation, or try out a quantum kernel if it might give you an edge. Build a solid foundation in fundamentals as those are never wasted. And be ready to pivot as the landscape changes. By doing so, you’ll keep up with the future of ultra-scale AI systems - and help create it!

After all, every great leap in AI, from the earliest multi-layer perceptrons (MLPs) to today’s massive Transformer-based GPT and MoE models, has many unsung heroes making it run efficiently and cost-effectively behind the scenes. In the era of 100-trillion-parameter models and beyond, you could be one of those heroes and ensure that the next generation of AI is powerful, efficient, sustainable, and brilliantly engineered. The adventure is just beginning, and I, for one, can’t wait to see what we accomplish next.
