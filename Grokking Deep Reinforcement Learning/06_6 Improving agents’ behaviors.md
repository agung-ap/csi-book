# 6 Improving agents’ behaviors

### In this chapter

- You will learn about improving policies when learning from feedback that is simultaneously sequential and evaluative.
- You will develop algorithms for finding optimal policies in reinforcement learning environments when the transition and reward functions are unknown.
- You will write code for agents that can go from random to optimal behavior using only their experiences and decision making, and train the agents in a variety of environments.

*When it is obvious that the goals cannot be reached, don’t adjust the goals, adjust the action steps.*

— Confucius
Chinese teacher, editor, politician, and philosopher of the Spring and Autumn period of Chinese history

Up until this chapter, you’ve studied in isolation and interplay learning from two of the three different types of feedback a reinforcement learning agent must deal with: sequential, evaluative, and sampled. In chapter 2, you learned to represent sequential decision-making problems using a mathematical framework known as the Markov decision processes. In chapter 3, you learned how to solve these problems with algorithms that extract policies from MDPs. In chapter 4, you learned to solve simple control problems that are multi-option, single-choice, decision-making problems, called Multi-Armed Bandits, when the MDP representation isn’t available to the agent. Finally, in chapter 5, we mixed these two types of control problems, that is, we dealt with control problems that are sequential and uncertain, but we only learned to estimate value functions. We solved what’s called the prediction problem, which is learning to evaluate policies, learning to predict returns.

In this chapter, we’ll introduce agents that solve the control problem[](/book/grokking-deep-reinforcement-learning/chapter-6/), which we get simply by changing two things. First, instead of estimating state-value functions, *V*(*s*), we estimate action-value functions, *Q*(*s, a*). The main reason for this is that Q-functions, unlike V-functions, let us see the value of actions without having to use an MDP. Second, after we obtain these Q-value estimates, we use them to improve the policies. This is similar to what we did in the policy-iteration algorithm: we evaluate, we improve, then evaluate the improved policy, then improve on this improved policy, and so on. As I mentioned in chapter 3, this pattern is called *generalized policy iteration (GPI)*, and it can help us create an architecture that virtually any reinforcement learning algorithm fits under, including state-of-the-art deep reinforcement learning agents.

The outline for this chapter is as follows: First, I’ll expand on the generalized policy-iteration architecture, and then you’ll learn about many different types of agents that solve the control problem. You’ll learn about the control version of the Monte Carlo prediction and temporal-difference learning agents. You’ll also learn about slightly different kinds of agents that decouple learning from behavior. What this all means in practical terms is that in this chapter, you develop agents that learn to solve tasks by trial-and-error learning. These agents learn optimal policies solely through their interaction with the environment.

## The anatomy of reinforcement learning agents

In this section, I’d like to give you a mental model that most, if not all, reinforcement learning agents fit under. First, every reinforcement learning agent gathers experience samples, either from interacting with the environment or from querying a learned model of an environment. Still, data is generated as the agents learn. Second, every reinforcement learning agent learns to estimate something, perhaps a model of the environment, or possibly a policy, a value function, or just the returns. Third, every reinforcement learning agent attempts to improve a policy; that’s the whole point of RL, after all.

|   | Refresh My Memory Rewards, returns, and value functions |
| --- | --- |
|  | Now is a good time to refresh your memory. You need to remember the difference between rewards, returns, and value functions, so that this chapter makes sense to you and you can develop agents that learn optimal policies through trial-and-error learning. Allow me to repeat myself. A *reward* is a numeric signal indicating the goodness of a transition. Your agent observes state *S**t*, takes action *A**t*; then the environment changes and gives a reward *R**t*+1, and emits a new state *S**t*+1. Rewards are that single numeric signal indicating the goodness of the transition occurring on every time step of an episode.  A *return* is the summation of all the rewards received during an episode. Your agent receives reward *R**t*+1, then *R**t*+2, and so on until it gets the final reward RT right before landing in the terminal state *S**T*. Returns are the sum of all those rewards during an episode. Returns are often defined as the discounted sum, instead of just a sum. A discounted sum puts a priority on rewards found early in an episode (depending on the discount factor, of course.) Technically speaking, a discounted sum is a more general definition of the return, since a discount factor of one makes it a plain sum.  A *value* function is the expected return. Expectations are calculated as the sum of all possible values, each multiplied by the probability of its occurrence. Think of expectations as the average of an infinite number of samples; the expectation of returns is like sampling an infinite number of returns and averaging them. When you calculate a return starting after selecting an action, the expectation is the action-value function of that state-action pair, *Q*(*s, a*). If you disregard the action taken and count from the state s, that becomes the state-value function *V*(*s*). |

### Most agents gather experience samples

One of the unique[](/book/grokking-deep-reinforcement-learning/chapter-6/) characteristics of RL is that agents learn by trial and error. The agent interacts with an environment, and as it does so, it gathers data. The unusual aspect here is that gathering data is a separate challenge from learning from data. And as you’ll see shortly, learning from data is also a different thing from improving from data. In RL, there is gathering, learning, and improving. For instance, an agent that’s pretty good at collecting data may not be as good at learning from data; or, conversely, an agent that isn’t good at collecting data may be good at learning from data, and so on. We all have that friend who didn’t take good notes in school, yet they did well on tests, while others had everything written down, but didn’t do as well.

In chapter 3, when we learned about dynamic programming methods, I mentioned value and policy iteration shouldn’t be referred to as RL, but planning methods instead, the reason being they do not gather data. There’s no need for DP methods to interact with the environment because a model of the environment, the MDP, is provided beforehand.

| ŘŁ | With An RL Accent Planning vs. learning problems |
| --- | --- |
|  | **Planning problems**: Refers to problems in which a model of the environment is available and thus, there’s no learning required. These types of problems can be solved with planning methods such as value iteration and policy iteration. The goal in these types of problems is to find, as opposed to learn, optimal policies. Suppose I give you a map and ask you to find the best route from point A to point B; there’s no learning required there, just planning. **Learning problems**: Refers to problems in which learning from samples is required, usually because there isn’t a model of the environment available or perhaps because it’s impossible to create one. The main challenge of learning problems is that we estimate using samples, and samples can have high variance, which means they’ll be of poor quality and difficult to learn from. Samples can also be biased, either because of being from a different distribution than the one estimating or because of using estimates to estimate, which can make our estimates incorrect altogether. Suppose I don’t give you a map of the area this time. How would you find “the best route”? By trial-and-error learning, likely. |

For an algorithm to be considered a standard RL method, the aspect of interacting with the environment, with the problem we’re trying to solve, should be present. Most RL agents gather experience samples by themselves, unlike supervised learning methods, for instance, which are given a dataset. RL agents have the additional challenge of selecting their datasets. Most RL agents gather experience samples because RL is often about solving interactive learning problems.

| ŘŁ | With An RL Accent Non-interactive vs. interactive learning problems |
| --- | --- |
|  | **Non-interactive learning problems**: Refers to a type of learning problem in which there’s no need for or possibility of interacting with an environment. In these types of problems, there’s no interaction with an environment while learning, but there is learning from data previously generated. The objective is to find something given the samples, usually a policy, but not necessarily. For instance, in inverse RL, the objective is to recover the reward function given expert-behavior samples. In apprenticeship learning, the objective is to go from this recovered reward function to a policy. In behavioral cloning, which is a form of imitation learning, the goal is to go from expert-behavior samples directly to policies using supervised learning. **Interactive learning problems**: Refers to a type of learning problem in which learning and interaction are interleaved. The interesting aspect of these problems is that the learner also controls the data-gathering process. Optimal learning from samples is one challenge, and finding samples for optimal learning is another. |

### Most agents estimate something

After gathering[](/book/grokking-deep-reinforcement-learning/chapter-6/) data, there are multiple things an agent can do with this data. Certain agents, for instance, learn to predict expected re*turns* or value functions. In the previous chapter, you learned about many different ways of doing so, from using Monte Carlo to TD targets, from every-visit to first-visit MC targets, from *n*-step to *λ*-return targets. There are many different ways of calculating targets that can be used for estimating value functions.

But value functions aren’t the only thing agents can learn with experience samples. Agents may be designed to learn models of the environment, too. As you’ll see in the next chapter, model-based RL agents use the data collected for learning transition and reward functions. By learning a model of the environment, agents can predict the next state and reward. Further, with these, agents can either plan a sequence of actions similar to the way DP methods work or maybe use synthetic data generated from interacting with these learned models to learn something else. The point is that agents may be designed to learn models of the environment.

Moreover, agents can be designed to improve on policies directly using estimated returns. In later chapters, we’ll see how policy gradient methods consist of approximating functions that take in a state and output a probability distribution over actions. To improve these policy functions, we can use actual returns, in the simplest case, but also estimated value functions. Finally, agents can be designed to estimate multiple things at once, and this is the typical case. The important thing is that most agents estimate something.

|   | Refresh My Memory Monte Carlo vs. temporal-difference targets |
| --- | --- |
|  | Other important concepts worth repeating are the different ways value functions can be estimated. In general, all methods that learn value functions progressively move estimates a fraction of the error towards the targets. The general equation that most learning methods follow is *estimate = estimate + step * error*. The error is simply the difference between a sampled target and the current estimate: (*target – estimate*). The two main and opposite ways for calculating these targets are Monte Carlo and temporal-difference learning.  The Monte Carlo target consists of the actual return: really, nothing else. Monte Carlo estimation consists of adjusting the estimates of the value functions using the empirical (observed) mean return in place of the expected (as if you could average infinite samples) return.  The temporal-difference target consists of an estimated return. Remember “bootstrapping”? It basically means using the estimated expected return from later states, for estimating the expected return from the current state. TD does that: learning a guess from a guess. The TD target is formed by using a single reward and the estimated expected return from the next state using the running value function estimates. |

### Most agents improve a policy

Lastly, most agents[](/book/grokking-deep-reinforcement-learning/chapter-6/) improve a policy. This final step heavily depends on the type of agent being trained and what the agent estimates. For instance, if the agent is estimating value functions, a common thing to improve is the target policy implicitly encoded in the value function, which is the policy being learned about. The benefit of improving the target policy is that the behavior policy, which is the data-generating policy, will consequently improve, therefore improving the quality of data the agent will subsequently gather. If the target and behavior policies are the same, then the improvement of the underlying value function explicitly increases the quality of the data generated afterward.

Now, if a policy is being represented explicitly instead of through value functions, such as in policy gradient and actor-critic methods, agents can use actual returns to improve these policies. Agents can also use value functions to estimate returns for improving policies. Finally, in model-based RL, there are multiple options for improving policies. One can use a learned model of the environment to plan a sequence of actions. In this case, there’s an implicit policy being improved in the planning phase. One can use the model to learn a value function, instead, which implicitly encodes a policy. One can use the model to improve the policy directly, too. The bottom line is that all agents attempt to improve a policy.

| ŘŁ | With An RL Accent Greedy vs. epsilon-greedy vs. optimal policy |
| --- | --- |
|  | **Greedy policy**: Refers to a policy that always selects the actions believed to yield the highest expected return from each and every state. It’s essential to know that a “greedy policy” is greedy with respect to a value function. The “believed” part comes from the value function. The insight here is that when someone says, “the greedy policy,” you must ask, greedy with respect to what? A greedy policy with respect to a random value function is a pretty bad policy. **Epsilon-greedy policy**: Refers to a policy that often selects the actions believed to yield the highest expected return from each and every state. Same as before applies; an epsilon-greedy policy is epsilon-greedy with respect to a specific value function. Always make sure you understand which value function is being referenced. **Optimal policy**: Refers to a policy that always selects the actions actually yielding the highest expected return from each and every state. While a greedy policy may or may not be an optimal policy, an optimal policy must undoubtedly be a greedy policy. You ask, “greedy with respect to what?” Well done! An optimal policy is a greedy policy with respect to a unique value function, the optimal value function. |

### Generalized policy iteration

Another simple pattern that’s more commonly used to understand the architecture of reinforcement learning algorithms is called *generalized policy iteration* (GPI[](/book/grokking-deep-reinforcement-learning/chapter-6/)[](/book/grokking-deep-reinforcement-learning/chapter-6/)). GPI is a general idea that the continuous interaction of policy evaluation and policy improvement drives policies towards optimality.

As you probably remember, in the policy iteration algorithm, we had two processes: policy evaluation and policy improvement. The policy-evaluation phase takes in any policy, and it evaluates it; it estimates the policy’s value function. In policy improvement, these estimates, the value function, are used to obtain a better policy. Once policy evaluation and improvement stabilize, that is, once their interaction no longer produces any changes, then the policy and the value function are optimal.

Now, if you remember, after studying policy iteration, we learned about another algorithm, called value iteration. This one was similar to policy iteration; it had a policy-evaluation and a policy-improvement phase. The main difference, however, was that the policy-evaluation phase consisted of a single iteration. In other words, the evaluation of the policy didn’t produce the actual value function. In the policy-evaluation phase of value iteration, the value function estimates move towards the actual value function, but not all the way there. Yet, even with this policy-evaluation phase, the generalized policy-iteration pattern for value iteration also produces the optimal value function and policy.

The critical insight here is that policy evaluation, in general, consists of gathering and estimating value functions, similar to the algorithms you learned about in the previous chapter. And as you know, there are multiple ways of evaluating a policy, numerous methods of estimating the value function of a policy, various approaches to choose from for checking off the policy-evaluation requirement of the generalized policy-iteration pattern.

Furthermore, policy improvement consists of changing a policy to make it greedier with respect to a value function. In the policy improvement method of the policy-iteration algorithm, we make the policy entirely greedy with respect to the value function of the evaluated policy. But, we can completely greedify the policy only because we had the MDP of the environment. However, the policy-evaluation methods that we learned about in the previous chapter don’t require an MDP of the environment, and this comes at a cost. We can no longer completely greedify policies; we need to have our agents explore. Going forward, instead of completely greedifying the policy, we make the policy greedier, leaving room for exploration. This kind of partial policy improvement was used in chapter 4 when we used different explorations strategies for working with estimates.

There you have it. Most RL algorithms follow this GPI pattern: they have distinct policy-evaluation and improvement phases, and all we must do is pick and choose the methods.

|   | Miguel's Analogy Generalized policy iteration and why you should listen to criticism |
| --- | --- |
|  | Generalized policy iteration (GPI) is similar to the eternal dance of critics and performers. Policy evaluation gives the much-needed feedback that policy improvement uses to make policies better. In the same way, critics provide the much-needed feedback performers can use to do better. As Benjamin Franklin said, “Critics are our friends, they show us our faults.” He was a smart guy; he allowed GPI to help him improve. You let critics tell you what they think, you use that feedback to get better. It’s simple! Some of the best companies out there follow this process, too. What do you think the saying “data-driven decisions” means? It’s saying they make sure to use an excellent policy-evaluation process so that their policy-improvement process yields solid results; that’s the same pattern as GPI! Norman Vincent Peale said, “The trouble with most of us is that we’d rather be ruined by praise than saved by criticism.” Go, let critics help you. Just beware! That they can indeed help you doesn’t mean critics are always right or that you should take their advice blindly, especially if it’s feedback that you hear for the first time. Critics are usually biased, and so is policy evaluation! It’s your job as a great performer to listen to this feedback carefully, to get smart about gathering the best possible feedback, and to act upon it only when sure. But, in the end, the world belongs to those who do the work. Theodore Roosevelt said it best: *“It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.”* In later chapters, we’ll study actor-critic methods, and you’ll see how this whole analogy extends, believe it or not! Actors and critics help each other. Stay tuned for more. It’s awe-inspiring that patterns in optimal decision making are valid across the board. What you learn studying DRL can help you become a better decision maker, and what you learn in your own life can help you create better agents. Cool, right? |

## Learning to improve policies of behavior

In the previous chapter, you learned how to solve the prediction problem: how to make agents most accurately estimate the value function of a given policy. However, while this is a useful ability for our agents to have, it doesn’t directly make them better at any task. In this section, you’ll learn how to solve the control problem: how to make agents optimize policies. This new ability allows agents to learn optimal behavior by trial-and-error learning, starting from arbitrary policies and ending in optimal ones. After this chapter you can develop agents that can solve any task represented by an MDP. The task has to be a discrete state- and action-space MDP, but other than that, it’s plug-and-play.

To show you a few agents, we’re going to leverage the GPI pattern you learned. That is, we’re going to select algorithms for the policy-evaluation phase from the ones you learned about in the last chapter, and strategies for the policy-improvement phase from the ones you learned about in the chapter before. Hopefully, this sets your imagination free on the possibilities. Just pick and choose algorithms for policy evaluation and improvement, and things will work, that’s because of the interaction of these two processes.

| ŘŁ | With An RL Accent Prediction vs. control problem vs. policy evaluation vs. improvement |
| --- | --- |
|  | **Prediction problem**: Refers to the problem of evaluating policies, of estimating value functions given a policy. Estimating value functions is nothing but learning to predict returns. State-value functions estimate expected returns from states, and action-value functions estimate expected returns from state-action pairs. **Control problem**: Refers to the problem of finding optimal policies. The control problem is usually solved by following the pattern of generalized policy iteration (GPI), where the competing processes of policy evaluation and policy improvement progressively move policies towards optimality. RL methods often pair an action-value prediction method with policy improvement and action-selection strategies. **Policy evaluation**: Refers to algorithms that solve the prediction problem. Note that there’s a dynamic programming method called policy evaluation, but this term is also used to refer to *all algorithms* that solve the prediction problem. **Policy improvement**: Refers to algorithms that make new policies that improve on an original policy by making it greedier than the original with respect to the value function of that original policy. Note that policy improvement by itself doesn’t solve the control problem. Often a policy evaluation must be paired with a policy improvement to solve the control problem. Policy improvement only refers to the computation for improving a policy given its evaluation results. |

|   | A Concrete Example The slippery walk seven environment |
| --- | --- |
|  | For this chapter, we use an environment called *slippery walk seven* (SWS). This environment is a walk, a single-row grid-world environment, with seven non-terminal states. The particular thing of this environment is that it’s a slippery walk; action effects are stochastic. If the agent chooses to go left, there is a chance it does, but there is also a chance that it goes right, or that it stays in place. Let me show you the MDP for this environment. Remember, though, that the agent doesn’t have any access to the transition probabilities. The dynamics of this environment are unknown to the agent. I’m only giving you this information for didactic reasons Also, have in mind that to the agent, there are no relationships between the states in advance. The agent doesn’t know that state 3 is in the middle of the entire walk, or that it’s in between states 2 and 4; it doesn’t even know what a “walk” is! The agent doesn’t know that action zero goes left, or one goes right. Honestly, I encourage you to go to the Notebook and play with the environment yourself to gain a deeper understanding. The fact is that the agent only sees the state ids, say, 0, 1, 2, and so on, and chooses either action 0 or 1.  Slippery walk seven environment MDP The SWS environment is similar to the random walk (RW) environment that we learned about in the previous chapter, but with the ability to do control. Remember that the random walk is an environment in which the probability of going left, when taking the Left action, is equal to the probability of going right. And the probability of going right, when taking the Right action, is equal to the probability of going left, so there’s no control. This environment is noisy, but the actions the agent selects make a difference in its performance. And also, this environment has seven non-terminal states, as opposed to the five of the RW. |

### Monte Carlo control: Improving policies after each episode

Let’s try to create a control[](/book/grokking-deep-reinforcement-learning/chapter-6/)[](/book/grokking-deep-reinforcement-learning/chapter-6/) method using Monte Carlo prediction for our policy-evaluation needs. Let’s initially assume we’re using the same policy-improvement step we use for the policy-iteration algorithm. That is, the policy-improvement step gets the greedy policy with respect to the value function of the policy evaluated. Would this make an algorithm that helps us find optimal policies solely through interaction? Actually, no. There are two changes we need to make before we can make this approach work.

First, we need to make sure our agent estimates the action-value function *Q*(*s, a*), instead of the *V(s, a)* that we estimated in the previous chapter. The problem with the V-function is that, without the MDP, it isn’t possible to know what the best action is to take from a state. In other words, the policy-improvement step wouldn’t work.

![We need to estimate action-value functions](https://drek4537l1klr.cloudfront.net/morales/Figures/06_07.png)

Second, we need to make sure our agent explores. The problem is that we’re no longer using the MDP for our policy-evaluation needs. When we estimate from samples, we get values for all of the state-action pairs we visited, but what if part of the best states weren’t visited?

![We need to explore](https://drek4537l1klr.cloudfront.net/morales/Figures/06_08.png)

Therefore, let’s use first-visit Monte Carlo prediction for the policy-evaluation phase and a decaying epsilon-greedy action-selection strategy for the policy-improvement phase. And that’s it—you have a complete, model-free RL algorithm in which we evaluate policies with Monte Carlo prediction and improve them with decaying epsilon-greedy action-selection strategy.

As with value iteration, which has a truncated policy-evaluation step, we can truncate the Monte Carlo prediction method. Instead of rolling out several episodes for estimating the value function of a single policy using Monte Carlo prediction, as we did in the previous chapter, we truncate the prediction step after a single full rollout and trajectory sample estimation, and improve the policy right after that single estimation step. We alternate a single MC-prediction step and a single decaying epsilon-greedy action-selection improvement step.

Let’s look at our first RL method MC control. You’ll see three functions:

- decay_schedule: Compute decaying values as specified in the function arguments.
- generate_trajectory: Roll out the policy in the environment for a full episode.
- mc_control: Complete implementation of the MC control method.

|   | I Speak Python Exponentially decaying schedule |
| --- | --- |
|  |  |

|   | I Speak Python Generate exploratory policy trajectories |
| --- | --- |
|  |  |

|   | I Speak Python Monte Carlo control 1/2 |
| --- | --- |
|  |  |

|   | I Speak Python Monte Carlo control 2/2 |
| --- | --- |
|  |  |

### SARSA: Improving policies after each step

As we discussed[](/book/grokking-deep-reinforcement-learning/chapter-6/)[](/book/grokking-deep-reinforcement-learning/chapter-6/) in the previous chapter, one of the disadvantages of Monte Carlo methods is that they’re offline methods in an episode-to-episode sense. What that means is that we must wait until we reach a terminal state before we can make any improvements to our value function estimates. However, it’s straightforward to use temporal-difference prediction for the policy-evaluation phase, instead of Monte Carlo prediction. By replacing MC with TD prediction, we now have a different algorithm, the well-known SARSA agent.

![Comparison between planning and control methods](https://drek4537l1klr.cloudfront.net/morales/Figures/06_09.png)

|   | I Speak Python The SARSA agent 1/2 |
| --- | --- |
|  |  |

|   | I Speak Python The SARSA agent 1/2 |
| --- | --- |
|  |  |

| ŘŁ | With An RL Accent Batch vs. offline vs. online learning problems and methods |
| --- | --- |
|  | **Batch learning problems and methods**: When you hear the term “batch learning,” people are referring to one of two things: they mean a type of learning problem in which experience samples are fixed and given in advance, or they mean a type of learning method which is optimized for learning synchronously from a batch of experiences, also called fitting methods. Batch learning methods are typically studied with non-interactive learning problems, more specifically, batch learning problems. But batch learning methods can also be applied to interactive learning problems. For instance, growing batch methods are batch learning methods that also collect data: they “grow” the batch. Also, batch learning problems don’t have to be solved with batch learning methods, the same way that batch learning methods aren’t designed exclusively to solve batch learning problems. **Offline learning problems and methods**: When you hear the term “offline learning,” people are usually referring to one of two things: they’re either talking about a problem setting in which there’s a simulation available for collecting data (as opposed to a real-world, online environment), or they could also be talking about learning methods that learn offline, meaning between episodes, for instance. Note that, in offline learning methods, learning and interaction can still be interleaved, but performance is only optimized after samples have been collected, similar to the growing batch described previouisly, but with the difference that, unlike growing batch methods, offline methods commonly discard old samples; they don’t *grow* a batch. MC methods, for instance, are often considered offline because learning and interaction are interleaved on an episode-to-episode basis. There are two distinct phases, interacting and learning; MC is interactive, but also an offline learning method. **Online learning problems and methods**: When you hear the term “online learning,” people are referring to one of two things: either to learning while interacting with a live system, such a robot, or to methods that learn from an experience as soon as it’s collected, on each and every time step. Note that offline and online learning are often used in different contexts. I’ve seen offline versus online to mean non-interactive versus interactive, but I’ve also seen them, as I mentioned, for distinguishing between learning from a simulator versus a live system. My definitions here are consistent with common uses of many RL researchers: Richard Sutton (2018 book), David Silver (2015 lectures), Hado van Hasselt (2018 lectures), Michael Littman (2015 paper), and Csaba Szepesvari (2009 book). Be aware of the lingo, though. That’s what’s important. |

## Decoupling behavior from learning

I want you to think about the TD update equation for state-value functions for a second; remember, it uses *R**t*+1 *+* *γ**V(S**t*+1) as the TD target. However, if you stare at the TD update equation for action-value functions instead, which is *R**t*+1 *+* *γ**Q(S**t*+1*, A**t*+1), you may notice there are a few more possibilities there. Look at the action being used and what that means. Think about what else you can put in there. One of the most critical advancements in reinforcement learning was the development of the *Q-learning* algorithm, a model-free off-policy bootstrapping method that directly approximates the optimal policy despite the policy generating experiences. Yes, this means the agent, in theory, can act randomly and still find the optimal value function and policies. How is this possible?

### Q-learning: Learning to act optimally, even if we choose not to

The SARSA[](/book/grokking-deep-reinforcement-learning/chapter-6/) algorithm[](/book/grokking-deep-reinforcement-learning/chapter-6/) is a sort of “learning on the job.” The agent learns about the same policy it uses for generating experience. This type of learning is called on-policy[](/book/grokking-deep-reinforcement-learning/chapter-6/). On-policy learning is excellent—we learn from our own mistakes. But, let me make it clear, in on-policy learning, we learn from our own current mistakes only. What if we want to learn from our own previous mistakes? What if we want to learn from the mistakes of others? In on-policy learning, you can’t. Off-policy learning[](/book/grokking-deep-reinforcement-learning/chapter-6/), on the other hand, is sort of “learning from others.” The agent learns about a policy that’s different from the policy-generating experiences. In off-policy learning, there are two policies: a behavior policy[](/book/grokking-deep-reinforcement-learning/chapter-6/), used to generate experiences, to interact with the environment, and a target policy[](/book/grokking-deep-reinforcement-learning/chapter-6/), which is the policy we’re learning about. SARSA is an on-policy method; Q-learning is an off-policy one.

|   | Show Me The Math SARSA vs. Q-learning update equations |
| --- | --- |
|  |  |

|   | I Speak Python The Q-learning agent 1/2 |
| --- | --- |
|  |  |

|   | I Speak Python The Q-learning agent 2/2 |
| --- | --- |
|  |  |

|   | Miguel's Analogy Humans also learn on-policy and off-policy |
| --- | --- |
|  | On-policy is learning about a policy that’s being used to make decisions; you can think about it as “learning on the job.” Off-policy learning is learning about a policy different from the policy used for making decisions. You can think about it as “learning from others’ experiences,” or “learning to be great, without trying to be great.” Both are important ways of learning and perhaps vital for a solid decision maker. Interestingly, you can see whether a person prefers to learn on-policy or off-policy pretty quickly. My son, for instance, tends to prefer on-policy learning. Sometimes I see him struggle playing with a toy, so I come over and try to show him how to use it, but then he complains until I leave him alone. He keeps trying and trying, and he eventually learns, but he prefers his own experience instead of others’. On-policy learning is a straightforward and stable way of learning. My daughter, on the other hand, seems to be OK with learning off-policy. She can learn from my demonstrations before she even attempts a task. I show her how to draw a house, then she tries. Now, beware; this is a stretch analogy. Imitation learning and off-policy learning are not the same. Off-policy learning is more about the learner using their experience at, say, running, to get better at something else, say, playing soccer. In other words, you do something while learning about something else. I’m sure you think of instances when you’ve done that, when you have learned about painting, while cooking. It doesn’t matter where the experiences come from for doing off-policy learning; as long as the target policy and the behavior policy are different, then you can refer to that as off-policy learning. Also, before you make conclusions about which one is “best,” know that in RL, both have pros and cons. On one hand, on-policy learning is intuitive and stable. If you want to get good at playing the piano, why not practice the piano? On the other hand, it seems useful to learn from sources other than your own hands-on experience; after all, there’s only so much time in a day. Maybe meditation can teach you something about playing the piano, and help you get better at it. But, while off-policy learning helps you learn from multiple sources (and/or multiple skills), methods using off-policy learning are often of higher variance and, therefore, slower to converge. Additionally, know that off-policy learning is one of the elements that, when combined, have been proven to lead to divergence: off-policy learning, bootstrapping, and function approximation. These don’t play nice together. You’ve learned about the first two, and the third one is soon to come. |

| ŘŁ | With An RL Accent Greedy in the limit with infinite exploration and stochastic approx. theory |
| --- | --- |
|  | Greedy in the limit with infinite exploration (GLIE) is a set of requirements that on-policy RL algorithms, such as Monte Carlo control and SARSA, must meet to guarantee convergence to the optimal policy. The requirements are as follows: <br>      <br>      All state-action pairs must be explored infinitely often. <br>      The policy must converge on a greedy policy. <br>      What this means in practice is that an epsilon-greedy exploration strategy, for instance, must slowly decay epsilon towards zero. If it goes down too quickly, the first condition may not be met; if it decays too slowly, well, it takes longer to converge. Notice that for off-policy RL algorithms, such as Q-learning, the only requirement of these two that holds is the first one. The second one is no longer a requirement because in off-policy learning, the policy learned about is different than the policy we’re sampling actions from. Q-learning, for instance, only requires all state-action pairs to be updated sufficiently, and that’s covered by the first condition in this section. Now, whether you can check off with certainty that requirement using simple exploration strategies such as epsilon-greedy, that’s another question. In simple grid worlds and discrete action and state spaces, epsilon-greedy most likely works. But, it’s easy to imagine intricate environments that would require more than random behavior. There is another set of requirements for general convergence based on stochastic approximation theory[](/book/grokking-deep-reinforcement-learning/chapter-6/) that applies to all of these methods. Because we’re learning from samples, and samples have some variance, the estimates won’t converge unless we also push the learning rate, alpha, towards zero: <br>      <br>      The sum of learning rates must be infinite. <br>      The sum of squares of learning rates must be finite. <br>      That means you must pick a learning rate that decays but never reaches zero. For instance, if you use 1/*t* or 1/*e*, the learning rate is initially large enough to ensure the algorithm doesn’t follow only a single sample too tightly, but becomes small enough to ensure it finds the signal behind the noise. Also, even though these convergence properties are useful to know for developing the theory of RL algorithms, in practice, learning rates are commonly set to a small-enough constant, depending on the problem. Also, know that a small constant is better for non-stationary environments, which are common in the real world. |

| ŘŁ | With An RL Accent On-policy vs. off-policy learning |
| --- | --- |
|  | **On-policy learning**: Refers to methods that attempt to evaluate or improve the policy used to make decisions. It is straightforward; think about a single policy. This policy generates behavior. Your agent evaluates that behavior and select areas of improvement based on those estimates. Your agent learns to assess and improve the same policy it uses for generating the data. **Off-policy learning**: Refers to methods that attempt to evaluate or improve a policy different from the one used to generate the data. This one is more complex. Think about two policies. One produces the data, the experiences, the behavior; but your agent uses that data to evaluate, improve, and overall learn about a different policy, a different behavior. Your agent learns to assess and improve a policy different than the one used for generating the data. |

### Double Q-learning: A max of estimates for an estimate of a max

Q-learning often overestimates the value function. Think about this. On every step, we take the maximum over the estimates of the action-value function of the next state. But what we need is the actual value of the maximum action-value function of the next state. In other words, we’re using the maximum over merely estimates as an estimate of the maximum.

Doing this isn’t only an inaccurate way of estimating the maximum value but also a more significant problem, given that these bootstrapping estimates, which are used to form TD targets, are often biased. The use of a maximum of biased estimates as the estimate of the maximum value is a problem known as *maximization bias*[](/book/grokking-deep-reinforcement-learning/chapter-6/).

It’s simple. Imagine an action-value function whose actual values are all zeros, but the estimates have bias: some positive, some negative: for example, 0.11, 0.65, –0.44, –0.26, and so on. We know the actual maximum of the values is zero, but the maximum over the estimates is 0.65. Now, if we sometimes pick a value with a positive bias and sometimes one with a negative bias, then perhaps the issue wouldn’t be as pronounced. But because we’re always taking a max, we always tend to take high values even if they have the largest bias, the biggest error. Doing this over and over again compounds the errors in a negative way.

We all know someone with a positive-bias personality who has let something go wrong in their lives: someone who’s blinded by shiny things that aren’t as shiny. To me, this is one of the reasons why many people advise against feeding the AI hype; because overestimation is often your enemy, and certainly something to mitigate for an improved performance.

|   | I Speak Python The double Q-learning agent 1/3 |
| --- | --- |
|  |  |

|   | I Speak Python The double Q-learning agent 2/3 |
| --- | --- |
|  |  |

|   | I Speak Python The double Q-learning agent 3/3 |
| --- | --- |
|  |  |

One way of dealing with maximization bias is to track estimates in two Q-functions. At each time step, we choose one of them to determine the action, to determine which estimate is the highest according to that Q-function. But, then we use the other Q-function to obtain that action’s estimate. By doing this, there’s a lower chance of always having a positive bias error. Then, to select an action for interacting with the environment, we use the average, or the sum, across the two Q-functions for that state. That is, the maximum over *Q*1(*S**t+1**)+Q*2*(S**t+1*), for instance. The technique of using these two Q-functions is called *double learning*, and the algorithm that implements this technique is called *double Q-learning*[](/book/grokking-deep-reinforcement-learning/chapter-6/)[](/book/grokking-deep-reinforcement-learning/chapter-6/). In a few chapters, you’ll learn about a deep reinforcement learning algorithm called *double deep Q-networks* (DDQN[](/book/grokking-deep-reinforcement-learning/chapter-6/)), which uses a variant of this double learning technique.

|   | It's In The Details FVMC, SARSA, Q-learning, and double Q-learning on the SWS environment |
| --- | --- |
|  | Let’s put it all together and test all the algorithms we just learned about in the Slippery Walk Seven environment. So you’re aware, I used the same hyperparameters in all algorithms, the same gamma, alpha, epsilon, and respective decaying schedules. Remember, if you don’t decay alpha toward 0, the algorithm doesn’t fully converge. I’m decaying it to 0.01, which is good enough for this simple environment. Epsilon should also be decayed to zero for full convergence, but in practice this is rarely done. In fact, often state-of-the-art implementations don’t even decay epsilon and use a constant value instead. Here, we’re decaying to 0.1.   Another thing: note that in these runs, I set the same number of episodes for all algorithms; they all run 3,000 episodes in the SWS environment. You’ll notice some algorithms don’t converge in this many steps, but that doesn’t mean they wouldn’t converge at all. Also, some of the other environments in the chapter’s Notebook, such as Frozen Lake, terminate on a set number of steps, that is, your agent has 100 steps to complete each episode, else it’s given a done flag. This is somewhat of an issue that we’ll address in later chapters. But, please, go to the Notebook and have fun! I think you’ll enjoy playing around in there. |

|   | Tally it Up Similar trends among bootstrapping and on-policy methods |
| --- | --- |
|  |  |

|   | Tally it Up Examining the policies learned in the SWS environment |
| --- | --- |
|  |  |

|   | Tally it Up Examining the value functions learned in the SWS environment |
| --- | --- |
|  |  |

## Summary

In this chapter, you put everything you’ve learned so far into practice. We learned about algorithms that optimize policies through trial-and-error learning. These algorithms learn from feedback that’s simultaneously sequential and evaluative; that is, these agents learn to simultaneously balance immediate and long-term goals and the gathering and utilization of information. But unlike in the previous chapter, in which we restricted our agents to solving the prediction problem, in this chapter, our agents learned to solve the control problem.

There are many essential concepts you learned about in this chapter. You learned that the prediction problem consists of evaluation policies, while the control problem consists of optimizing policies. You learned that the solutions to the prediction problem are in policy evaluation methods, such as those learned about in the previous chapter. But unexpectedly, the control problem isn’t solved alone by policy-improvement methods you’ve learned in the past. Instead, to solve the control problem, we need to use policy-evaluation methods that can learn to estimate action-value functions merely from samples and policy-improvement methods that take into account the need for exploration.

The key takeaway from this chapter is the generalized policy-iteration pattern (GPI) which consists of the interaction between policy-evaluation and policy-improvement methods. While policy evaluation makes the value function consistent with the policy evaluated, policy improvement reverses this consistency but produces a better policy. GPI tells us that by having these two processes interact, we iteratively produce better and better policies until convergence to optimal policies and value functions. The theory of reinforcement learning supports this pattern and tells us that, indeed, we can find optimal policies and value functions in the discrete state and action spaces with only a few requirements. You learned that GLIE and stochastic approximation theories apply at different levels to RL algorithms.

You learned about many other things, from on-policy to off-policy methods, from online to offline, and more. Double Q-learning and double learning, in general, are essential techniques that we build on later. In the next chapter, we examine advanced methods for solving the control problem. As environments get challenging, we use other techniques to learn optimal policies. Next, we look at methods that are more effective in solving environments, and they do so more efficiently, too. That is, they solve these environments, and do so using fewer experience samples than methods we learned about in this chapter.

By now, you

- Know that most RL agents follow a pattern known as generalized policy iteration
- Know that GPI solves the control problem with policy evaluation and improvement
- Learned about several agents that follow the GPI pattern to solve the control problem

|   | Tweetable Feat Work on your own and share your findings |
| --- | --- |
|  | Here are several ideas on how to take what you have learned to the next level. If you’d like, share your results with the rest of the world and make sure to check out what others have done, too. It’s a win-win situation, and hopefully, you’ll take advantage of it. <br>      <br>      **#gdrl_ch06_tf01:** All the algorithms presented in this chapter use two crucial variables: the learning rate, alpha, and the discount factor, gamma. I want you to do an analysis on these two variables. For instance, how do these variables interact? How do they affect the total reward obtained by the agent, and the policy success rate? <br>      **#gdrl_ch06_tf02:** Another thing to think about after this chapter is that we used the same exploration strategy for all of the methods: it was an exponentially decaying epsilon-greedy strategy. But, is this the best strategy? How would you use other strategies from chapter 4? How about creating an exploration strategy of your own and testing that out? How about changing the hyperparameters related to the exploration strategy and see how results change? That shouldn’t be hard at all to try. Head to the book’s Notebooks and start by changing severalof the hyperparameters: then change the exploration strategy altogether, and tell us what you find. <br>      **#gdrl_ch06_tf03:** You’re probably guessing this right. The algorithms in this chapter are also not using the time step limit correctly. Make sure to investigate what I’m alluding to, and once you find out, change the algorithms to do this right. Do the results change at all? Do agents now do better than before? Are they better in terms of estimating the optimal value function, the optimal policy, or both? How much better? Make sure to investigate, or come back after chapter 8. Share your findings. <br>      **#gdrl_ch06_tf04:** In every chapter, I’m using the final hashtag as a catchall hashtag. Feel free to use this one to discuss anything else that you worked on relevant to this chapter. There’s no more exciting homework than that which you create for yourself. Make sure to share what you set yourself to investigate and your results. <br>      Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use the particular hashtag from the list to help interested folks find your results. There are no right or wrong results; you share your findings and check others’ findings. Take advantage of this to socialize, contribute, and get yourself out there! We’re waiting for you! Here’s a tweet example: “Hey, @mimoralea. I created a blog post with a list of resources to study deep reinforcement learning. Check it out at <link>. #gdrl_ch01_tf01” I’ll make sure to retweet and help others find your work. |
