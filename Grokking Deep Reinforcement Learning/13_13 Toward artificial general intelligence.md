# 13 Toward artificial general intelligence

### In this chapter

- You will look back at the algorithms you learned in this book, as well as learn about deep reinforcement learning methods that weren’t covered in depth.
- You will learn about advanced deep reinforcement learning techniques that, when combined, allow agents to display more general intelligence.
- You will get my parting advice on how to follow your dreams and contribute to these fabulous fields of artificial intelligence and deep reinforcement learning.

*Our ultimate objective is to make programs that learn from their experience as effectively as humans do.*

— John McCarthy
Founder of the field of Artificial Intelligence Inventor of the Lisp programming language

In this book, we have surveyed a wide range of decision-making algorithms and reinforcement learning agents; from the planning methods that you learned about in chapter 3 to the state-of-the-art deep reinforcement learning agents that we covered in the previous chapter. The focus of this book is to teach the ins and outs of the algorithms; however, there’s more to DRL than what we covered in this book, and I want you to have some direction going forward.

I designed this chapter to hit a couple of points. In the first section, we recap the entire book. I’d like you to zoom out and look at the big picture again. I want you to see what you’ve learned so that you can choose for yourself where to go next. I also mention several of the notable types of agents that I couldn’t cover before I ran out of pages. But, know that while there are more types of algorithms, what you learned in this book covers the foundational methods and concepts.

After going over what was covered and what wasn’t, I introduce several of the more advanced research areas in DRL that may lead to the eventual creation of artificial general intelligence (AGI[](/book/grokking-deep-reinforcement-learning/chapter-13/)[](/book/grokking-deep-reinforcement-learning/chapter-13/)). I know AGI is a hot topic, and lots of people use it in a deceiving way. Being such an exciting and controversial topic, people use it to get attention. Don’t give your energy to those folks; don’t be misled; don’t get distracted. Instead, focus on what matters, what’s right in front of you. And make progress toward your goals, whatever they may be.

I do believe humans can create AGI because we’ll keep trying forever. Understanding intelligence and automating tasks is a quest we’ve been longing for and working on for centuries, and that’s never going to change. We try to understand intelligence through philosophy and the understanding of the self. We look for answers about intelligence through introspection. I would argue that most AI researchers are part-time philosophers themselves. They use what they’ve learned in reinforcement learning to better themselves and the other way around, too.

Also, humans love automation; that’s what intelligence has allowed us to do. We’re going to continue trying to automate life, and we’ll get there. Now, while we can argue whether AGI is the beginning of human-like robots that overtake the world, today, we still cannot train a single agent to play all Atari games at a super-human level. That is, a single trained agent cannot play all games, though a single general-purpose algorithm can be trained independently. But, we should be cautious when considering AGI.

To close the chapter and the book, I provide ideas for you going forward. I receive many questions regarding applying DRL to custom problems, environments of your own. I do this for a living on my full-time job, so I can share my two cents as to how to go about it. I also give career advice for those interested, and a parting message. It’s one more chapter: let’s do this.

## What was covered and what notably wasn’t?

This book covers most of the foundations of deep reinforcement learning, from MDPs and their inner workings to state-of-the-art actor-critic algorithms and how to train them in complex environments. Deep reinforcement learning is an active research field in which new algorithms are published every month. The field is advancing at a rapid pace, and it’s unfortunately not possible to provide high-quality explanations for everything there is in a single book.

Thankfully, most of the things left out are advanced concepts that aren’t required in most applications. That doesn’t mean that they aren’t relevant; I highly recommend you continue the journey of learning DRL. You can count on me to help along the way; I’m easy to find. For now, though, of the things left out of this book, I only consider two essential; they’re model-based deep reinforcement learning methods and derivative-free optimization methods.

In this section, we quickly review the algorithms and methods that you learned about in this book and touch on these two essential methods that were notably missing.

![Comparison of different algorithmic approaches to deep reinforcement learning](https://drek4537l1klr.cloudfront.net/morales/Figures/13_01.png)

### Markov decision processes

The first two chapters were an introduction to the field of reinforcement learning and to the way we describe the problems we’re trying to solve. MDPs are an essential concept to have in mind, and even if they look simple and limited, they’re powerful. There’s much more we could have explored in this area. The thing I want you to take from these concepts is the ability to think of problems as MDPs[](/book/grokking-deep-reinforcement-learning/chapter-13/). Practice this yourself. Think about a problem, and break it down into states, observations, actions, and all the components that would make that problem an MDP.

![The transition function of the frozen lake environment](https://drek4537l1klr.cloudfront.net/morales/Figures/13_02.png)

You’ll notice that even though it seems the world is non-stationary and non-Markovian, we can transform some of the things that make it seem that way, and then see the world as an MDP. Do the probability distributions of the real world change, or is it that we don’t have enough data to determine the actual distributions? Does the future depend on past states, or is the state space so high-dimensional that we can’t conceive all history of the world being part of a single state? Again, as an exercise, think of problems and try to fit them into this MDP framework. It may turn out to be useful if you want to apply DRL to your problems.

### Planning methods

In the third chapter, we discussed methods that help you find optimal policies of problems that have MDPs available. These methods, such as value iteration[](/book/grokking-deep-reinforcement-learning/chapter-13/)[](/book/grokking-deep-reinforcement-learning/chapter-13/) and policy iteration[](/book/grokking-deep-reinforcement-learning/chapter-13/), iteratively compute the optimal value functions, which in turn allow extracting optimal policies quickly. Policies are nothing but universal plans—a plan for every situation.

![Policy evaluation on the always-left policy on the SWF environment](https://drek4537l1klr.cloudfront.net/morales/Figures/13_03.png)

The two most important takeaways from this section are first. These algorithms isolate sequential decision-making problems. There’s no uncertainty because they require the MDP, and there’s no complexity because they only work for discrete state and action spaces. Second, there’s a general pattern to see here; we’re interested in evaluating behaviors, perhaps as much as we’re interested in improving them. This realization was something I didn’t get for some time. To me, improving, optimizing sounded more interesting, so policy evaluation methods didn’t get my attention. But you later understand that if you get evaluation right, improving is a piece of cake. The main challenge is usually evaluating policies accurately and precisely. But, if you have the MDP, you calculate these values correctly and straightforwardly.

### Bandit methods

The fourth chapter was about[](/book/grokking-deep-reinforcement-learning/chapter-13/) learning from evaluative feedback. In this case, we learned about the uncertainty aspect of reinforcement learning by taking the MDP away. We hide the MDP, but make the MDP super simple; a single-state single-step horizon MDP, in which the challenge is to find the optimal action or action distribution in the fewest number of episodes; that is, minimizing total regret.

![In chapter 4, you learned more effective ways for dealing with the exploration-exploitation trade-off](https://drek4537l1klr.cloudfront.net/morales/Figures/13_04.png)

We studied several different exploration strategies and tested them in a couple of bandit environments. But at the end of the day, my goal for that chapter was to show you that uncertainty on its own creates a challenge worth studying separately. There are a few great books about this topic, and if you’re interested in it, you should pursue that path; it’s a reasonable path that needs much attention.

![10-armed Gaussian bandits](https://drek4537l1klr.cloudfront.net/morales/Figures/13_05.png)

The right nugget to get out of this chapter and into reinforcement learning is that reinforcement learning is challenging because we don’t have access to the MDP, as in the planning methods in chapter 3. Not having the MDP creates uncertainty, and we can only solve uncertainty with exploration. Exploration strategies are the reason why our agents can learn on their own, by trial-and-error learning, and it’s what makes this field exciting.

### Tabular reinforcement learning

Chapters 5, 6, and 7 are all about mixing the sequential and uncertain aspects of reinforcement learning. Sequential decision-making problems under uncertainty are at the core of reinforcement learning when presented in a way that can be more easily studied; that is, without the complexity of large and high-dimensional state or action spaces.

Chapter 5 was about evaluating policies, chapter 6 was about optimizing policies, and chapter 7 was about advanced techniques for evaluating and optimizing policies. To me, this is the core of reinforcement learning, and learning these concepts well helps you understand deep reinforcement learning more quickly. Don’t think of DRL as something separate from tabular reinforcement learning; that’s the wrong thinking. Complexity is only one dimension of the problem, but it’s the same exact problem. You often see top, deep reinforcement learning–research labs releasing papers solving problems in discrete state and action spaces. There’s no shame in that. That’s often the smart approach, and you should have that in mind when you experiment. Don’t start with the highest-complexity problem; instead, isolate, then tackle, and finally, increase complexity.

In these three chapters, we covered a wide variety of algorithms. We covered evaluation methods such as first-visit and every-visit Monte Carlo prediction, temporal-difference prediction, *n*-step TD, and TD(*λ*). We also covered control methods such as first-visit and every-visit Monte Carlo control, SARSA, Q-learning, double Q-learning, and more advanced methods, such as SARSA(*λ*) and Q(*λ*) both with replacing and also with accumulating traces. We also covered model-based approaches, such as Dyna-Q and trajectory sampling.

![Deep reinforcement learning is part of the larger field of reinforcement learning](https://drek4537l1klr.cloudfront.net/morales/Figures/13_06.png)

### Value-based deep reinforcement learning

Chapters 8, 9, and 10 are all about the nuances of value-based deep reinforcement learning methods. We touched on neural fitted Q-iteration (NFQ), deep Q-networks (DQN), double deep Q-networks (DDQN), dueling architecture in DDQN (dueling DDQN), and prioritized experience replay (PER). We started with DQN and added improvements to this baseline method one at a time. We tested all algorithms in the cart-pole environment.

There are many more improvements that one can implement to this baseline algorithm, and I recommend you try that. Check out an algorithm called Rainbow, and implement some of the improvements to DQN not in this book. Create a blog post about it and share it with the world. Techniques you learn when implementing value-based deep reinforcement learning methods are essential to other deep reinforcement learning approaches, including learning critics in actor-critic methods. There are a many improvements and techniques to discover. Keep playing around with these methods.

### Policy-based and actor-critic deep reinforcement learning

Chapter 11 was an introduction to policy-based[](/book/grokking-deep-reinforcement-learning/chapter-13/) and actor-critic methods. Policy-based was a new approach to reinforcement learning at that point in the book, so we introduced the concepts in a straightforward algorithm known as REINFORCE[](/book/grokking-deep-reinforcement-learning/chapter-13/), which only parameterizes the policy. For this, we approximated the policy directly and didn’t use any value function at all. The signal that we use to optimize the policy in REINFORCE is the Monte Carlo return[](/book/grokking-deep-reinforcement-learning/chapter-13/), the actual returns experienced by the agent during an episode.

We then explored an algorithm that learns a value function to reduce the variance of the MC return. We called this algorithm vanilla policy gradient (VPG[](/book/grokking-deep-reinforcement-learning/chapter-13/)). The name is somewhat arbitrary, and perhaps a better name would have been *rEINFORCE with Baseline*. Nevertheless, it’s important to note that this algorithm, even though it learns a value function, is not an actor-critic method because it uses the value function as a baseline and not as a critic. The crucial insight here is that we don’t use the value function for bootstrapping purposes, and because we also train the value-function model using MC returns, there’s minimal bias in it. The only bias in the algorithm is the bias introduced by the neural network, nothing else.

Then, we covered more advanced actor-critic methods that do use bootstrapping. A3C, which uses *n*-step returns; GAE, which is a form of lambda return for policy updates; and A2C, which uses synchronous updates to the policy. Overall, these are state-of-the-art methods, and you should know that they’re reliable methods that are still widely used. One of the main advantages and unique characteristics of A3C, for instance, is that it only needs CPUs, and can train faster than other methods, if you lack a GPU.

### Advanced actor-critic techniques

Even though A3C, GAE, and A2C, are actor-critic methods, they don’t use the critic in unique ways. In chapter 12, we explored methods that do. For instance, many people consider DDPG and TD3 actor-critic methods, but they fit better as value-based methods for continuous action spaces. If you look at the way A3C uses the actor and the critic, for instance, you find substantial differences in DDPG. Regardless, DDPG and TD3 are state-of-the-art methods, and whether actor-critic or not, it doesn’t make much of a difference when solving a problem. The main caveat is that these two methods can only solve continuous action-space environments. They could be high-dimensional action spaces, but the actions must be continuous. Other methods, such as A3C, can solve both continuous and discrete action spaces.

SAC is an animal of its own. The only reason why it follows after DDPG and TD3 is because SAC uses many of the same techniques as DDPG and TD3. But the unique characteristic of SAC is that it’s an entropy-maximization method. The value function maximizes not only the return but also the entropy of the policy. These kinds of methods are promising, and I wouldn’t be surprised to see new state-of-the-art methods that derive from SAC.

Finally, we looked at another exciting kind of actor-critic method with PPO. PPO is an actor-critic method, and you probably notice that because we reused much of the code from A3C. The critical insight with PPO is the policy update step. In short, PPO improves the policy a bit at a time; we make sure the policy doesn’t change too much with an update. You can think of it as a conservative policy-optimization method. PPO can be easily applied to both continuous and discrete action spaces, and PPO is behind some of the most exciting results in DRL, such as OpenAI Five, for instance.

We covered many great methods throughout these chapters, but more importantly, we covered the foundational methods that allow you to understand the field going forward. Many of the algorithms out there derive from the algorithms we covered in this book, with a few exceptions, namely, model-based deep reinforcement learning methods, and derivative-free optimization methods. In the next two sections, I give insights into what these methods are so that you can continue your journey exploring deep reinforcement learning.

![DRL algorithms in this book](https://drek4537l1klr.cloudfront.net/morales/Figures/13_07.png)

### Model-based deep reinforcement learning

In chapter 7, you learned about model-based reinforcement learning methods such as Dyna-Q and trajectory sampling. Model-based deep reinforcement learning is, at its core, what you’d expect; the use of deep learning techniques for learning the transition, the reward function, or both, and then using that for decision making. As with the methods you learned about in chapter 7, one of the significant advantages of model-based deep reinforcement learning is sample efficiency; model-based methods are the most sample efficient in reinforcement learning.

![Model-based reinforcement learning algorithms to have in mind](https://drek4537l1klr.cloudfront.net/morales/Figures/13_08.png)

In addition to sample efficiency, another inherent advantage of using model-based methods is transferability. Learning a model of the dynamics of the world can help you achieve different related tasks. For instance, if you train an agent to control a robotic arm to reach an object, a model-based agent that learns how the environment reacts to the agent’s attempts to move toward the object might more easily learn to pick up that object in a later task. Notice that, in this case, learning a model of the reward function isn’t useful for transfer. However, learning how the environment reacts to its motion commands is transferable knowledge that can allow the accomplishment of other tasks. Last time I checked, the laws of physics hadn’t been updated for hundreds of years—talk about a slow-moving field!

A couple of other pluses worth mentioning follow. First, learning a model is often a supervised-learning task, which is much more stable and well-behaved than reinforcement learning. Second, if we have an accurate model of the environment, we can use theoretically grounded algorithms for planning, such as trajectory optimization, model-predictive control, or even heuristic search algorithms, such as Monte Carlo tree search. Last, by learning a model, we make better use of experiences overall because we extract the most information from the environment, which means more possibilities for better decisions.

But it isn’t all roses; model-based learning is also challenging. There are a few disadvantages to have in mind when using model-based methods. First, learning a model of the dynamics of an environment, in addition to a policy, a value function, or both, is more computationally expensive. And if you were to learn only a model of the dynamics, then the compounding of model error from the model would make your algorithm impractical.

Not all aspects of the dynamics are directly beneficial to the policy. We covered this issue when arguing for learning a policy directly instead of learning a value function. Imagine a pouring task; if you need first to learn fluid dynamics, the viscosity of fluids, and fluid flow when you only want to pick up a cup and pour, then we’re overcomplicating the task. Trying to learn a model of the environment is more complicated than learning the policy directly.

It’s essential to recall that deep learning models are data hungry. As you know, to get the best out of a deep neural network, you need lots of data, and this is a challenge for model-based deep reinforcement learning methods. The problem compounds with the fact that it’s also hard to estimate model uncertainty in neural networks. And so, given that a neural network tries to generalize, regardless of model uncertainty, you can end up with long-term predictions that are total garbage.

This issue makes the argument that model-based methods are the most sample efficient questionable because you may end up needing more data to learn a useful model than the data you need for learning a good policy under model-free methods. However, if you have that model, or acquire the model independently of the task, then you can reuse that model for other tasks. Additionally, if you were to use “shallow” models, such as Gaussian processes, or Gaussian mixture models, then we’re back at square one, having model-based methods as the most sample efficient.

I’d like you to move from this section, knowing that it isn’t about model-based versus model-free. And even though you can combine model-based and model-free methods and get attractive solutions, at the end of the day, engineering isn’t about that either, the same way that it isn’t a matter of value-based versus policy-based, and it also isn't actor-critic. You don’t want to use a hammer when you need a screwdriver. My job is to describe what each type of algorithm is suitable for, but it’s up to you to use that knowledge the right way. Of course, explore, have fun, that matters, but, when it’s time to solve a problem, pick wisely.

### Derivative-free optimization methods

Deep learning is the use of multi-layered function approximators to learn a function. A traditional deep learning use case goes as follows. First, we create a parametric model that mirrors a function of interest. Then, we define an objective function to know how wrong the model is at any given time. Next, we iteratively optimize the model by calculating where to move the parameters, using backpropagation. And finally, we update the parameters, using gradient descent.

Backpropagation and gradient descent are practical algorithms for optimizing neural networks. These methods are valuable for finding the lowest or highest point of a function within a given range; for instance, a local optimum of the loss or objective function. But, interestingly, they aren’t the only way of optimizing parametric models, such as deep neural networks, and, more importantly, they aren’t always the most effective.

Derivative-free optimization[](/book/grokking-deep-reinforcement-learning/chapter-13/), such as genetic algorithms or evolution strategies, is a different model-optimization technique that has gotten attention from the deep reinforcement learning community in recent years. Derivative-free methods, which are also known as gradient-free, black-box, and zeroth-order methods, don’t require derivatives and can be useful in situations in which gradient-based optimization methods suffer. Gradient-based optimization methods suffer when optimizing discrete, discontinuous, or multi-model functions, for instance.

Derivative-free methods can be useful and straightforward in many cases. Even randomly perturbing the weights of a neural network, if given enough compute, can get the job done. The main advantage of derivative-free methods is that they can optimize an arbitrary function. They don’t need gradients to work. Another advantage is that these methods are straightforward to parallelize. It’s not uncommon to hear hundreds or thousands of CPUs used with derivative-free methods. On the flip side, it’s good that they’re easy to parallelize because they’re sample inefficient. Being black-box optimization methods, they don’t exploit the structure of the reinforcement learning problem. They ignore the sequential nature of reinforcement learning problems, which can otherwise give valuable information to optimization methods.

![Derivative-free methods are an extreme case](https://drek4537l1klr.cloudfront.net/morales/Figures/13_09.png)

|   | Miguel's Analogy How derivative-free methods work[](/book/grokking-deep-reinforcement-learning/chapter-13/) |
| --- | --- |
|  | To get an intuitive sense of how gradient-based and gradient-free methods compare, imagine for a second the game of Hot and Cold. Yeah, that game kids play in which one kid, the hunter, is supposed to find a hidden object of interest, while the rest of the kids, who know where the object is, yell “cold” if the hunter is far from the object, and “hot” if the hunter is close to it. In this analogy, the location of the hunter is the parameters of the neural network. The hidden object of interest is the global optimum, which is either the lowest value of the loss function to minimize or the highest value of the objective function to maximize. The intent is to optimize the distance between the hunter and the object. For this, in the game, you use the kids yelling “cold” or “hot” to optimize the hunter’s position. Here’s when this analogy gets interesting. Imagine you have kids who, in addition to yelling “cold” or “hot,” get louder as the hunter gets closer to the object. You know, kids get excited too quickly and can’t keep a secret. As you hear them go from saying “cold” softly to getting louder every second, you know, as the hunter, you’re walking in the right direction. The distance can be minimized using that “gradient” information. The use of this information for getting to the object of interest is what gradient-based methods do. If the information comes in a continuous form, meaning the kids yell a couple of times per second, and get louder or softer and go from yelling “cold” to yelling “hot” with distance, then you can use the increase or decrease in the magnitude of the information, which is the gradient, to get to the object. Great! On the other hand, imagine the kids yelling the information are mean, or maybe just not perfect. Imagine they give discontinuous information. For instance, they may not be allowed to say anything while the hunter is in certain areas. They go from “softly cold” to nothing for a while to “softly cold” again. Or perhaps, imagine the object is right behind the middle of a long wall. Even if the hunter gets close to the object, the hunter won’t be able to reach the object with gradient information. The hunter would be close to the object, so the right thing to yell is “hot,” but in reality, the object is out of reach behind a wall. In all these cases, perhaps a gradient-based optimization approach isn’t the best strategy, and gradient-free methods, even if moving randomly, may be better for finding the object. A gradient-free method approach could be as simple as that. The hunter would pick a random place to go and ignore “gradient” information while getting there, then check with the yelling kids, and then try another random position. After getting an idea of a few random positions, say, 10, the hunter would take the top 3 and try random variations from those 3 that are apparently better locations. In this case, gradient information isn’t useful. Trust me, this analogy can keep going, but I’m going to stop it there. The bottom line is gradient-based and gradient-free methods are only strategies for reaching a point of interest. The effectiveness of these strategies depends on the problem at hand. |

## More advanced concepts toward AGI

In the previous section, we reviewed the foundational concepts of deep reinforcement learning that are covered in this book and touched on the two essential types of methods that we didn’t cover in depth. But, as I mentioned before, there are still many advanced concepts that, even though not required for an introduction to deep reinforcement learning, are crucial for devising artificial general intelligence (AGI), which is the ultimate goal for most AI researchers.

In this section, we start by going one step deeper into AGI and argue for some of the traits AI agents need to tackle tasks requiring more-general intelligence. I explain at a high level what these traits are and their intent so that you can continue your journey studying AI and perhaps one day contribute to one of these cutting-edge research fields.

### What is AGI, again?

In this book, you’ve seen many examples of AI agents that seem impressive at first sight. The fact that the same computer program can learn to solve a wide variety of tasks is remarkable. Moreover, after you move on to more complex environments, it’s easy to get carried away by these results: AlphaZero learns to play chess, Go, and Shogi. OpenAI Five defeats human teams at the game of Dota2. AlphaStar beats a top professional player at the game of StarCraft II. These are compelling general-purpose algorithms. But do these general-purpose algorithms show any sign of general intelligence? First of all, what is general intelligence?

General intelligence is the ability to combine various cognitive abilities to solve new problems. For artificial general intelligence (AGI[](/book/grokking-deep-reinforcement-learning/chapter-13/)[](/book/grokking-deep-reinforcement-learning/chapter-13/)), we then expect a computer program to show general intelligence. Okay. Now, let’s ask the following question: are any of the algorithms presented in this book, or even state-of-the-art methods such as AlphaZero, OpenAI Five, and AlphaStar, examples of artificial general intelligence? Well, it’s not clear, but I’d say no.

You see, on the one hand, many of these algorithms can use “multiple cognitive abilities,” including perception and learning for solving a new task, say, playing Pong. If we stick to our definition, the fact that the algorithm uses multiple cognitive abilities to solve new problems is a plus. However, one of the most dissatisfying parts of these algorithms is that none of these trained agents are good at solving new problems unless you train them, which most of the time requires millions of samples before you get any impressive results. In other words, if you train a DQN agent to play Pong from pixels, that trained agent, which can be at superhuman level at Pong, has no clue about how to play a decent game of Breakout and has to train for millions of frames before it shows any skill.

Humans don’t have this problem. If you learn to play Pong, I’m pretty sure you can pick up Breakout in like two seconds. Both games have the same task of hitting a ball with a paddle. On the other hand, even the AlphaZero agent, a computer program with the most impressive skills of all time at multiple fundamentally different board games, and that can beat professional players who dedicate their lives at these games, but will never do your laundry.

Certain AI researchers say their goal is to create AI systems that perceive, learn, think, and even feel emotions like humans do. Machines that learn, think, feel, and perhaps even look like people, are most definitely an exciting thought. Other researchers have a more practical approach; they don’t necessarily want an AI that thinks like humans unless thinking like a human is a requirement for making a good lunch. And perhaps emotions are what make a great cook, who knows. The point is that while some folks want AGI to delegate, to stop doing mundane tasks, other folks have a more philosophical goal. Creating AGI could be a path to understanding intelligence itself, to understanding the self, and that on its own would be a remarkable accomplishment for humanity.

Either way, every AI researcher would agree that, regardless of the end goal, we still need AI algorithms that display more general and transferable skills. There are many traits that AI systems likely require before they can do more human-like tasks, such as doing the laundry, cooking lunch, or washing the dishes. Interestingly, it’s those mundane tasks that are the most difficult to solve for AI. Let’s review several of the research areas that are currently pushing the frontier on making deep reinforcement learning and artificial intelligence show signs of general intelligence.

The following sections introduce several of the concepts that you may want to explore further as you keep learning advanced deep reinforcement learning techniques that get the field of AI closer to human-level intelligence. I spend only a few sentences so that you’re aware of as many as possible. I intend to show you the door, not what’s inside it. It’s for you to decide which door to open.

![Workforce revolutions](https://drek4537l1klr.cloudfront.net/morales/Figures/13_10.png)

### Advanced exploration strategies

One area of research[](/book/grokking-deep-reinforcement-learning/chapter-13/) that’s showing exciting results has to do with the reward function. Throughout this book, you’ve seen agents that learn from the reward signal[](/book/grokking-deep-reinforcement-learning/chapter-13/), but interestingly, there’s recent research that shows agents can learn without any reward at all. Learning from things other than rewards is an exciting thought, and it may be essential for developing human-like intelligence. If you observe a baby learn, there’s much unsupervised and self-supervised learning going on. Sure, at one point in their lives, we reward our children. You know you get an A, you get B; your salary is x, yours is y. But agents aren’t always after the rewards we put along their way. What is the reward function of life? Is it career success? Is it to have children? It’s not clear.

Now, removing the reward function from the reinforcement learning problem can be a bit scary. If we’re not defining the reward function for the agent to maximize, how do we make sure their goals align with ours? How do we make artificial general intelligence that’s suitable for the goals of humankind? Maybe it’s the case that, to create human-like intelligence, we need to give agents the freedom to choose their destiny. Either way, to me, this is one of the critical research areas to pursue.

### Inverse reinforcement learning

There are other ways to learn behavior[](/book/grokking-deep-reinforcement-learning/chapter-13/) without a reward function, and even though we often prefer a reward function, learning to imitate a human first can help learn policies with fewer samples. There are a few related fields to look for here. *Behavioral cloning*[](/book/grokking-deep-reinforcement-learning/chapter-13/) is the application of supervised learning techniques to learn a policy from demonstrations, often from a human. As the name suggests, there’s no reasoning going on here, merely generalization. A related field, called *inverse reinforcement learning*, consists of inferring the reward function from demonstrations. In this case, we’re not merely copying the behavior, but we’re learning the intentions of another agent. *Inferring intentions* can be a powerful tool for multiple goals. For instance, in multi-agent reinforcement learning, for both adversarial and cooperative settings, knowing what other agents are after can be useful information. If we know what an agent wants to do, and what it wants to do goes against our goals, we can devise strategies for stopping it before it’s too late.

But, inverse reinforcement learning allows agents to learn new policies. Learning the reward function from another agent, such as a human, and learning a policy from this learned reward function is a technique often referred to as *apprenticeship learning*. One interesting point to consider when learning about inverse reinforcement learning is that the reward function is often more succinct than the optimal policy. Attempting to learn the reward function can make sense in multiple cases. Techniques that learn policies from demonstrations are also called *imitation learning*, often whether a reward function is inferred before the policy or straight behavioral cloning. A frequent use case for imitation learning is the initialization of agents to a good enough policy. For instance, if an agent has to learn from random behavior, it could take a long time before it learns a good policy. The idea is that imitating a human, even if suboptimal, may lead to optimal policies with fewer interactions with the environment. However, this isn’t always the case, and policies pretrained with demonstrations by humans may introduce unwanted bias and prevent agents from finding optimal policies.

### Transfer learning

You probably notice that the agent[](/book/grokking-deep-reinforcement-learning/chapter-13/) trained on an environment, in general, cannot be transferred to new environments. Reinforcement learning algorithms are general purpose in the sense that the same agent can be trained in different environments, but they don’t have general intelligence, and what they learn cannot be straightforwardly transferred to new environments.

Transfer learning is an area of research that looks at ways of transferring knowledge from a set of environments to a new environment. One approach, for instance, that may be intuitive to you if you have a deep learning background, is what’s called *fine-tuning*. Similar to reusing the weights of a pretrained network in supervised learning, agents trained in related environments can reuse the features learned by the convolution layers on a different task. If the environments are related, such as Atari games, for instance, several of the features may be transferable. In certain environments, even policies can be transferred.

![Sim-to-real transfer learning task is a common need in the real world](https://drek4537l1klr.cloudfront.net/morales/Figures/13_11.png)

The general area of research on making agents learn more general skills is called *transfer learning*. Another frequent use of transfer learning is to transfer policies learned in simulation to the real world. Sim-to-real transfer learning is a common need in robotics, in which training agents controlling robots can be tricky, costly, and dangerous. It’s also not as scalable as training in simulation. A common need is to train an agent in simulation and then transfer the policy to the real world. A common misconception is that simulations need to be high-fidelity and realistic for transferring agents from simulation to the real world. There’s research suggesting that it’s the opposite. It’s the variety, the diversity of observations, that makes agents more transferable. Techniques such as domain randomization are at the forefront of this research area and show much promise.

### Multi-task learning

A related area of research, called *multi-task learning*[](/book/grokking-deep-reinforcement-learning/chapter-13/), looks at transfer learning from a different perspective. In multi-task learning, the goal is to train on multiple tasks, instead of one, and then transfer to a new task. In this case, model-based reinforcement learning approaches come to mind. In robotics, for instance, learning a variety of tasks with the same robot can help the agent learn a robust model of the dynamics of the environment. The agent learns about gravity, how to move toward the right, or left, and so on. Regardless of the tasks, the model of the dynamics learned can be transferred to a new task.

![Multi-task learning consists of training on multiple related tasks and testing on a new one](https://drek4537l1klr.cloudfront.net/morales/Figures/13_12.png)

### Curriculum learning

A common use-case scenario for multi-task learning is decomposing a task into multiple tasks sorted by difficulty level. In this case, the agent is put through a curriculum, learning more complicated tasks progressively. *Curriculum learning*[](/book/grokking-deep-reinforcement-learning/chapter-13/) makes sense and can be useful when developing scenarios. If you need to create an environment for an agent to solve, it often makes sense for you to create the most straightforward scenario with a dense reward function. By doing this, your agent can quickly show progress toward learning the goal, and this validates that your environment is working well. Then, you can increase the complexity and make the reward function more sparse. After you do this for a handful of scenarios, you naturally create a curriculum that can be used by your agent. Then, you can train your agent in progressively more complex environments, and hopefully, have an agent reach the desired behaviors more quickly.

### Meta learning

Another super exciting research area is called *meta learning[](/book/grokking-deep-reinforcement-learning/chapter-13/)*. If you think about it, we’re hand-coding agents to learn many different tasks. At one point, we become the bottleneck. If we could develop an agent that, instead of learning to solve a challenging task, learns to learn itself, we could remove humans from the equation; well, not quite, but take a step in that direction. Learning to learn is an exciting approach to using experiences from learning multiple tasks to get good at learning itself. It makes intuitive sense. Other exciting research paths coming out of meta learning are automatically discovering neural network architectures and optimization methods. Keep an eye out for these.

### Hierarchical reinforcement learning

Often, we find ourselves developing environments that have problems with multiple horizons. For instance, if we want an agent to find the best high-level strategy, but give it only low-level control commands for actions, then the agent needs to learn to go from low-level to high-level action space. Intuitively, there’s a hierarchy in the policies for most agents. When I plan, I do so on a higher-level action space. I think about going to the store, not moving my arms to get to the store. *Hierarchical reinforcement learning*[](/book/grokking-deep-reinforcement-learning/chapter-13/) enables agents to create a hierarchy of actions internally to tackle long-horizon problems. Agents no longer reason about left-right commands, but more about go here or there.

### Multi-agent reinforcement learning

The world wouldn’t be as exciting without other agents. In multi-agent reinforcement learning, we look at techniques for having agents learn when there are multiple agents around. One of the main issues that arise, when learning in multi-agent settings, is that as your agent learns, other agents learn too, and therefore change their behavior. The problem is that this change makes the observations non-stationary, because what your agent learns is outdated right after the other agents learn, and so learning becomes challenging.

One exciting approach to cooperative *multi-agent reinforcement learning*[](/book/grokking-deep-reinforcement-learning/chapter-13/) is to use actor-critic methods[](/book/grokking-deep-reinforcement-learning/chapter-13/) in which the critic uses the full state information of all agents during training. The advantage here is that your agents learn to cooperate through the critic, and we can then use the policy during testing using a more realistic observation space. Sharing the full state may seem unrealistic, but you can think about it as similar to how teams practice. During practice, everything is allowed. Say that you’re a soccer player, and you can tell other agents you intend to run on the wing when you make this move, and so on. You get to practice moves with full information during training; then, you can only use your policy with limited information during testing.

Another appealing thought when looking into multi-agent reinforcement learning is that hierarchical reinforcement learning can be thought of as another case of multi-agent reinforcement learning. How so? Think about multiple agents deciding on different horizons. The multiple-horizon structure is similar to the way most companies do business. Folks at the top plan higher-level goals for the next few years and other folks decide on how to get there on a month-to-month and a day-to-day basis. The ones at the top set the goals for those on the bottom. The whole system gets rewarded for the performance of all agents.

Of course, multi-agent reinforcement learning isn’t only for the cooperative case but also for the adversarial case, which is perhaps the most exciting. Humans often see competition and adversaries as something inherently unfortunate, but multi-agent reinforcement learning suggests that our adversaries often are the best way to make ourselves better. Underlying many recent reinforcement learning success stories are training techniques that include adversaries: either a previous version of the same agent, such as in self-play, to a whole tournament-like distribution of other agents that form after all the matches—only the best agents survive. Adversaries often make us better, and for better or worse, they might be needed for optimal behavior.

### Explainable AI, safety, fairness, and ethical standards

There are a few other critical areas of research that, even though not directly a push for human-level intelligence, are fundamental for the successful development, deployment, and adoption of artificial intelligence solutions.

*Explainable artificial intelligence*[](/book/grokking-deep-reinforcement-learning/chapter-13/)[](/book/grokking-deep-reinforcement-learning/chapter-13/) is an area of research that tries to create agents that are more easily understandable by humans. The motives are apparent. A court of law can interrogate any person that breaks the law; however, machine learning models aren’t designed to be explainable. To ensure the fast adoption of AI solutions by society, researchers must investigate ways to ease the problem of explainability. To be clear, I don’t think this is a requirement. I prefer having an AI give me an accurate prediction in the stock market, whether it can explain to me why or not. However, neither decision is straightforward. In life-or-death decisions involving humans, things become hairy quickly.

Safety is another area of research that should get more attention. It’s often the case that AIs fail catastrophically in ways that are too obvious to humans. Also, AIs are vulnerable to attacks that humans aren’t. We need to make sure that when AIs are deployed, we know how the systems react to a variety of situations. AIs currently don’t have a way to go through classical validation and verification (V&V) of software approaches, and this poses a significant challenge for the adoption of AI.

Fairness is another crucial issue. We need to start thinking about who controls AIs. If a company creates an AI to maximize profits at the expense of society, then what’s the point of AI technologies? We already have something similar going on with advertising. Top companies use AI to maximize gains through a form of manipulation. Should these companies be allowed to do this for profit? How about when AIs get better and better? What’s the purpose of this, destroy a human through manipulation? These are things that need to be seriously considered.

Finally, AI ethical standards are another issue that has gotten recent attention with the Montreal Declaration for Responsible Development of Artificial Intelligence. These are 10 ethical principles for AI that serve the interests of society, and not merely for-profit companies. These are several of the top fields to have in mind when you’re ready to contribute.

## What happens next?

While this section marks the end of this book, it should only mark the beginning or continuation of your contributions to the field of AI and DRL. My intention with this book was, not only to get you understanding the basics of DRL, but also to onboard you into this fantastic community. You don’t need much other than a commitment to continue the journey. There are many things you could do next, and in this section, I’d like to give you ideas to get you started. Have in mind that the world is a choir needing a wide variety of voice types and talents; your job is to accept the talents given to you, develop them to the best of your abilities, and play your part with all you've got. While I can give you ideas, it’s up to you what happens next; the world needs and awaits your voice.

### How to use DRL to solve custom problems

There’s something[](/book/grokking-deep-reinforcement-learning/chapter-13/) super cool about RL algorithms that I want you to have in mind as you learn about other types of agents. The fact is that most RL agents can solve any problem that you choose, as long as you can represent the problem as a correct MDP, the way we discussed in chapter 2. When you ask yourself, “What can X or Y algorithm solve?” the answer is the same problems other algorithms can solve. While in this book, we concentrate on a handful of algorithms, all the agents presented can solve many other environments with some hyperparameter tuning. The need for solving custom environments is something many people want but could take a whole other book to get right. My recommendation is to look at some of the examples available online. For instance, Atari environments use an emulator called Stella in the backend. The environments pass images for observations and actions back and forth between the environment and the emulator. Likewise, MuJoCo and the Bullet Physics simulation engine are the backends that drive continuous-control environments. Take a look at the way these environments work.

Pay attention to how observations are passed from the simulation to the environment and then to the agent. Then, the actions selected by the agent are passed to the environment and then to the simulation engine. This pattern is widespread, so if you want to create a custom environment, investigate how others have done it, and then do it yourself. Do you want to create an environment for an agent to learn to invest in the stock market? Think about which platforms have an API that allows you to do that. Then, you can create different environments using the same API. One environment, for instance, can buy stocks, another buys options, and so on. There are so many potential applications for state-of-the-art deep reinforcement learning methods that it’s a shame we have a limited number of quality environments at our disposal. Contributions in this area are undoubtedly welcome. If you want to create an environment and don’t find it out there, consider investing the time to create your own and share it with the world.

### Going forward

You have learned a lot; there’s no doubt about that. But, if you look at the big picture, there’s so much more to learn. Now, if you zoom out even further, you realize that there’s even more to discover; things nobody has learned before. You see that what the AI community is after is no easy feat; we’re trying to understand how the mind works.

The fact is, even other fields, such as Psychology, Philosophy, Economics, Linguistic, Operations Research, Control Theory, and many more, are after the same goal, each from their perspective, and using their language. But the bottom line is that all of these fields would benefit from understanding how the mind works, how humans make decisions, and how to help them make optimal decisions. Here are some ideas for us moving forward.

First, find your motivation, your ambition, and focus. Certain people find the sole desire to explore; to discover facts about the mind is exciting. Others want to leave a better world behind. Whatever your motivation, find it. Find your drive. If you aren’t used to reading research papers, you won’t enjoy them unless you know your motives. As you find your motivation and drive, you must remain calm, humble, and transparent; you need your drive to focus and work hard toward your goals. Don’t let your excitement get in your way. You must learn to keep your motivation in your heart, yet move forward. Our ability to focus is in constant jeopardy from a myriad of readily available distractions. I’m guaranteed to find new notifications on my cell phone every 15 minutes. And we’re trained to think that this is a good thing. It isn’t. We must get back in control of our lives and be able to concentrate long and hard on something that interests us, that we love. Practice focus.

Second, balance learning and contributions and give yourself time to rest. What do you think would happen if, for the next 30 days, I eat 5,000 calories a day and burn 1,000? What if I, instead, eat 1,000 and burn 5,000? How about I’m an athlete and eat and burn 5,000, but train every day of the week? Right, all of those spell trouble to the body. The same happens with the mind; certain people think they need to learn for years before they can do anything, so they read, watch videos, but don’t do anything with it. Others think they no longer need to read any papers; after all, they’ve already implemented a DQN agent and written a blog post about it. They quickly become obsolete and lack the fuel to think. Some people get those two right, but never include the time to relax, and enjoy their family and reflect. That’s the wrong approach. Find a way to balance what you take and what you give, and set time to rest. We’re athletes of the mind, too much “fat” in your mind, too much information that you put in without purpose, and you become sluggish and too slow. Too many blog posts without doing any research, and you become outdated, repetitive, and dry. Not enough rest, and you won’t plan well enough for the long term.

Also, know that you can’t learn everything. Again, we’re learning about the mind, and there’s much information about it out there. Be wise, and be picky about what you read. Who’s the author? What’s her background? Still, you can read it but have a higher sense of what you’re doing. Try to give often. You should be able to explain things that you learn another way. The quote, “Don’t reinvent the wheel,” is misleading at best. It would be best if you tried things on your own; that’s vital. It’s inevitable that if you explore, early on, you may find yourself having a great idea, that you later realize has been worked on before. There’s no shame in that; it’s more critical for you to keep moving forward than to keep yourself waiting for a eureka moment that solves world hunger. I heard Rich Sutton say something along the lines of “the obvious to you is your biggest contribution.” But if you don’t allow yourself to “reinvent the wheel,” you run the risk of not sharing the “obvious to you,” thinking it’s probably not worthy. I’m not saying you need to publish a paper about this new algorithm you thought about, Q-learning. I’m asking, please, don’t let the fear of doing “worthless” work stop you from experimenting. The bottom line is to keep reading, keep exploring, keep contributing, and let it all sink in. It’s a cycle; it’s a flow, so keep it going.

Third and last, embrace the process, lose yourself on the path. Your dreams are only a way to get you going, but it’s in the going that you live your dreams. Get deep into it; don’t just follow what others are doing, and follow your interests. Think critically of your ideas, experiment, gather data, try to understand the results, and detach yourself from the result. Don’t bias your experiments, discover facts. If you lose yourself for long enough, you start to specialize, which is good. This field is so vast that being great at everything isn’t possible. However, if you follow your interests and intuition for long enough, you automatically spend more time on certain things versus other things. Keep going. Some of us feel a need to stay in the know, and once we start asking questions that have no answers, we feel the need to get back to shore. Don’t be afraid to ask hard questions, and work on getting answers. There are no dumb questions; each question is a clue for solving the mystery. Keep asking questions. Keep playing the game, and enjoy it.

### Get yourself out there! Now!

Assuming you just finished this book, make sure to get out there right away, think about how to put things together, and contribute something to this amazing community. How about writing a blog post about several of the algorithms not covered in this book that you find interesting? How about investigating some of the advanced concepts discussed in this chapter and sharing what you find? Write a blog post, create a video, and let the world know about it. Become part of the movement; let’s find out what intelligence is, let’s build intelligent systems, together. It’s never the right time unless it’s now.

## Summary

That’s it! You did it! That’s a wrap for this book. The ball is in your court.

In the first chapter, I defined deep reinforcement learning as follows: “Deep reinforcement learning is a machine learning approach to artificial intelligence concerned with creating computer programs that can solve problems requiring intelligence. The distinct property of DRL programs is learning through trial and error from feedback that’s simultaneously sequential, evaluative, and sampled by leveraging powerful non-linear function approximation.”

I mentioned that success to me was that after you complete this book, you should be able to come back to this definition and understand it precisely. I said that you should be able to tell why I used the words that I used, and what each of these words means in the context of deep reinforcement learning.

Did I succeed? Do you intuitively now understand this definition? Now it’s your turn to send your reward to the agent behind this book. Was this project a –1, a 0, or a +1? Whatever your comments, I learn, just like DRL agents, from feedback, and I look forward to reading your review and what you have to say. For now, my part is complete.

In this final chapter, we reviewed everything the book teaches, and we also discussed the core methods that we skipped, and some of the advanced concepts that could play a part in the eventual creation of artificial general intelligence agents.

As a parting message, I’d like to first thank you for the opportunity you gave me to share with you my take on the field of deep reinforcement learning. I also want to encourage you to keep going, to concentrate on the day-to-day, to think more about what you can do next, with your current abilities, with your unique talents.

By now, you

- Intuitively understand what deep reinforcement learning is; you know the details of the most critical deep reinforcement learning methods, from the most basic and foundational to the state-of-the-art.
- Have a sense of where to go next because you understand how what we’ve learned fits into the big picture of the fields of deep reinforcement learning and artificial intelligence.
- Are ready to show us what you’ve got, your unique talents, and interests. Go, make the RL community proud. Now, it’s your turn!

|   | Tweetable Feat Work on your own and share your findings |
| --- | --- |
|  | Here are several ideas on how to take what you’ve learned to the next level. If you’d like, share your results with the rest of the world and make sure to check out what others have done, too. It’s a win-win situation, and hopefully, you’ll take advantage of it. <br>      <br>      **#gdrl_ch13_tf01:** Implement model-based deep reinforcement learning methods. <br>      **#gdrl_ch13_tf02:** Implement gradient-free deep reinforcement learning methods. <br>      **#gdrl_ch13_tf03:** Implement a multi-agent environment or agent, and share it. <br>      **#gdrl_ch13_tf04:** Use advance deep learning techniques not discussed in this book to get better results from deep reinforcement learning agents. To give you an idea, variational autoencoders (VAE) could be a promising way to compress the observation space. This way the agent can learn much quicker. Any other DL technique? <br>      **#gdrl_ch13_tf05:** Create a list of resources for learning several of the more advance techniques to develop artificial general intelligence, whether mentioned or not. <br>      **#gdrl_ch13_tf06:** Grab your favorite algorithm from one of the approaches to AGI in this chapter, create a Notebook, and make a blog post explaining the details. <br>      **#gdrl_ch13_tf07:** Create a list of interesting environments already out there. <br>      **#gdrl****_ch13_tf08:** Create a custom environment that you are passionate about, something unique, maybe a wrapper for AI to play a game, or the stock market, and so on. <br>      **#gdrl_ch13_tf09:** Update your resume and send it my way, and I’ll retweet it. Make sure to include several of the projects that you worked on, DRL related, of course. <br>      **#gdrl_ch13_tf10:** In every chapter, I’m using the final hashtag as a catchall hashtag. Feel free to use this one to discuss anything else that you worked on relevant to this chapter. There’s no more exciting homework than that which you create for yourself. Make sure to share what you set yourself to investigate and your results. <br>      Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use the particular hashtag from the list to help interested folks find your results. There are no right or wrong results; you share your findings and check others’ findings. Take advantage of this to socialize, contribute, and get yourself out there! We’re waiting for you! Here’s a tweet example: “Hey, @mimoralea. I created a blog post with a list of resources to study deep reinforcement learning. Check it out at <link>. #gdrl_ch01_tf01” I’ll make sure to retweet and help others find your work. |
