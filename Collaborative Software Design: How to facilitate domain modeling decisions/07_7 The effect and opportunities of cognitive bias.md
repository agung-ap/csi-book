# 7 [](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)The effect and opportunities of cognitive bias

### This chapter covers

- Defining, recognizing, and embracing cognitive bias
- Understanding how cognitive bias affects collaboration and software design
- Altering behavior through nudges
- Becoming a choice architect

The previous chapter explained ranking and its effect on collaborative modeling sessions. Similar to ranking, *cognitive bias* affects group dynamics and outcomes. Cognitive bias is a systematic pattern of deviation from a norm that affects decision-making and judgment, that is, mental shortcuts that help us make sense of the world. Isn’t it strange that when you’re thinking about buying a car from a specific brand, you seem to see that car way more often than before? It’s like everyone is driving that car now. Spoiler alert: this has to do with cognitive bias. [](/book/collaborative-software-design/chapter-7/)

Sticking with the car example, have you ever driven on a highway and all of a sudden realize you’ve driven quite some miles without consciously registering everything that happened around you? Did you pass a car on the right? Were there any traffic signs? Did anything noticeable happen? Realizing you were driving that car sounds a bit dangerous now, right? Driving a car requires you to make many decisions on the spot: you have to react to other drivers by tapping the gas, possibly changing gears, slowing down or speeding up, staying in your lane, avoiding obstacles, and so on. It doesn’t sound like an activity best done partly unconsciously, and yet, we end up a few miles farther than we remember. This, too, has to do with our brains using mental shortcuts to make decisions quickly and effectively.

Just like in the preceding example, this happens regularly during collaborative modeling sessions. In some situations, it’s beneficial because decisions can be made quickly and effectively. In other situations, the mental shortcuts can be a hindrance, and we need to find a way to properly manage them. In this chapter, we’ll dive into cognitive bias and how they can affect collaborative modeling sessions. We’ll discuss some of the examples we see regularly and how you can recognize them. As facilitators, observing cognitive bias can help you unblock groups and nudge them toward rethinking solutions and withholding judgment when necessary. This chapter will provide you with guidance and heuristics on how to observe, benefit, embrace, and overcome cognitive bias when desired[](/book/collaborative-software-design/chapter-7/).

## 7.1 Cognitive bias explained

[](/book/collaborative-software-design/chapter-7/)This section will dive into the meaning and effect of cognitive bias in collaborative modeling sessions. As mentioned in the introduction, cognitive bias is a systematic pattern of deviation from a norm that affects decision-making and judgment. The “systematic” part in this definition is important. Because it’s systematic, it becomes predictable. This is why we can observe, anticipate, and we see similar biases occurring in almost every collaborative modeling session. As facilitators, this gives you an advantage because you can share your observations with the group and decide where these biases help and where they hinder the group and its progress. [](/book/collaborative-software-design/chapter-7/)

As you can imagine, a lot of (unconscious) decisions are being made during collaborative modeling: Which pivotal events do we choose? How do we split the group? Where do you place your stickies on the timeline? How much detail do you add to the board? Where do you start in the process? When do you move on from a discussion? That’s just a few of the possibilities. A lot of these decisions partly rely on cognitive bias, which means cognitive bias can trigger behavior too. Understanding and recognizing biases therefore is a useful skill when doing collaborative modeling.

For the record, we’re fans of cognitive bias. It sometimes has a negative connotation, but without these biases our lives would be so much more complex and much less fun. In the coming sections, we’ll explain this further and provide lots of examples. We’ll start with explaining cognitive bias using some theory and research. Equally important are the misconceptions we’ll discuss because there are many of the[](/book/collaborative-software-design/chapter-7/)m.

### 7.1.1 What is cognitive bias?

In our daily lives, a lot of information is coming our way almost continuously. We have to make sense of all those chunks of information by processing them and giving meaning to them. It sounds easy, but it’s actually a ton of work. Imagine having to consciously, deliberately process every tiny bit of information you encounter. For example, can you describe and explain every single decision you had to make this (or yesterday) morning to get from your house to work? These decisions range from turning off your alarm clock, to taking a shower, making breakfast, checking your email, leaving the house, driving your car, parking your car (or going to the train station, taking a train), entering the office, and so on.

Our human brains are brilliant and powerful, but they are also limited in the amount of information they can structurally process simultaneously. The brain simply can’t register, identify, process, analyze, and understand every single chunk of information that’s thrown at it in a rational way—there are limitations. So, what our brilliant brain then tries to do is simplify this information processing. Cognitive biases can be a result of our brain’s attempt to simplify information processing. You could consider them mental shortcuts, or rules of thumb, that help you make sense of the world, process information quickly based on previous experience and knowledge, and help you make a decision with relative speed. For example, our brain may take only the information it needs to make a decision, which can be convenient during collaborative modeling sessions. If we had to process everything consciously, we wouldn’t have a lot of time left to do the actual modeling part[](/book/collaborative-software-design/chapter-7/).

### 7.1.2 What does cognitive bias look like?

[](/book/collaborative-software-design/chapter-7/)Now that we know a little bit more about what cognitive biases are, we need to know how they operate in our daily lives. More specifically, given the context of this book, it’s valuable to know how they operate in collaborative modeling sessions. Section 7.2 will go into this topic in more detail, but we’ll provide two brief examples here to further explain the concept of cognitive bias. [](/book/collaborative-software-design/chapter-7/)

During EventStorming, the group needs to decide on pivotal events at one point. What we often see happening is that someone, usually relatively high in rank, points at one domain event as a pivotal event: “In my opinion, this event should be a pivotal event.” The rest of the group goes along with that suggestion or sometimes suggests alternative pivotal events that are close to the first suggestion on the timeline. This is the *anchoring effect*—a cognitive bias—in full swing. The first suggestion made here serves as an anchor that the rest of the group will move to. They may adjust it a bit, but the group will move toward that anchor, which means that whoever drops this anchor, strongly influences group decisions. The anchoring effect can be reinforced by ranking, as we saw in the example situation. If the person who drops the first anchor is relatively high in rank, people will tend to follow their suggestion and might be hesitant to share their initial thoughts and perspective. This particular example illustrates how several social dynamics (in this case, anchoring effect and ranking) can influence and reinforce each other, which is exactly why it’s so important to deal with them properly. [](/book/collaborative-software-design/chapter-7/)

Another example where we see cognitive bias in collaborative modeling is when people work together in subgroups on a solution, model, or proposal. In our training, we deliberately do this to demonstrate cognitive bias. Let’s take Example Mapping as our example. Different subgroups are asked to come up with rules and examples for a specific challenge. We let the groups work on it, and after some time, we ask them to walk around and see what the other groups came up with. After that, we ask them to do the sensemaking exercise shown in figure 7.1.[](/book/collaborative-software-design/chapter-7/)

![Figure 7.1 Sensemaking exercise to demonstrate cognitive bias during collaborative modeling](https://drek4537l1klr.cloudfront.net/baas/Figures/CH07_F01_Baas.png)

[](/book/collaborative-software-design/chapter-7/)We usually see very similar outcomes and examples in the different groups. Apart from the anchoring effect just described, various cognitive biases are involved here:

-  *Availability bias*—A mental shortcut that heavily relies on examples that come to mind quickly and easily. This is based on the notion that if something comes to mind so quickly, it must be important. More or less, the same information likely comes to mind for all individuals, creating similarity in their models or solutions.[](/book/collaborative-software-design/chapter-7/)
-  *Functional fixedness*—A cognitive bias that affects creativity. This is about getting stuck in what we know, which hinders us from taking on new perspectives to solve problems. We all have the same mental image of what a cinema looks like, for example. That often limits our creativity when thinking about solutions. [](/book/collaborative-software-design/chapter-7/)
-  *Bandwagon effect*—A cognitive bias that describes the tendency of people to adopt certain behavior or decisions just because others are doing it. When the whole group agrees on an example or takes on a certain strategy to come to those examples, it’s easy to follow the majority and jump on that bandwagon. [](/book/collaborative-software-design/chapter-7/)

To take this a bit further, we have to explain a little bit about what is called *System 1 and System 2*. Described in detail in Daniel Kahneman’s book *Thinking, Fast and Slow* (Macmillan, 2013), System 1 thinking is an almost instantaneous process—our automatic pilot that’s intuitive and requires little effort. System 2 thinking is slower, more conscious, and rational, requiring more effort. Our System 1 thinking heavily benefits from cognitive[](/book/collaborative-software-design/chapter-7/) bias.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

##### Exercise 7.1

Think about your recent meetings or collaborative modeling sessions; did you observe (in hindsight) the anchoring effect, availability bias, functional fixedness, or the bandwagon effect? If so, how did it affect the outcomes of the session?[](/book/collaborative-software-design/chapter-7/)

Hint: If you can’t think of any examples, that might be your System 1 thinking being predominant. Try to talk to others about the session to see if they can indicate some examples [](/book/collaborative-software-design/chapter-7/)of bias.

### 7.1.3 System 1 and System 2: A crash course

[](/book/collaborative-software-design/chapter-7/)Psychologists and philosophers have been busy with the distinction between instinctive thinking and conscious reasoning for many centuries. As mentioned earlier, in *Thinking, Fast and Slow*, Kahneman managed to get the distinction between automatic and conscious thought processes into mainstream thinking. More specifically, he popularized the concepts of System 1 and System 2 thinking[1](/book/collaborative-software-design/chapter-7/footnote-002) to describe the two processes. [](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

System 1 operates almost automatically and quickly, with little to no effort. This system is about intuition and drawing conclusions. It’s a very valuable system that helps you function in and navigate through a fast-paced world that throws information at you continuously. System 1 helps you deal with things such as driving for a long time on an empty road, solving 1 + 1, and detecting emotions in voices and facial expressions. After enough repetition, more complex tasks can become routine decisions that can eventually be taken care of by System 1. An example is driving a car in general. When you first get your driver’s license, you have to actively and consciously think about every decision you make while driving. After a few years, this becomes more of a routine that can be handled by System 1. System 1 relies on mental shortcuts based on experience and repetition. This is also where cognitive biases come in.

System 2 thinking is slower than System 1. It’s more rational and refers to consciously making decisions. We use System 2 mainly for more complex mental activities such as solving 37 × 17, deciding between two potential new houses or jobs, or writing a book. System 2 is also very useful for challenging System 1’s conclusions when necessary.

It might seem that these are two separate systems that we can use as we please in specific situations: for routine work, we use System 1, and for complex activities, we ping System 2. When System 1 draws conclusions (too fast), we ask System 2 to challenge them. Sounds almost too good to be true right? It is. There needs to be some nuance added to this story. Some of this nuance and depth got lost when the concepts of System 1 and System 2 became more mainstream and popular. As Kahneman explains in his book, this dual-system approach combines both forms of reasoning because (almost) all information processing is based on a mix of the two systems. We may rely a bit more on System 2 in mentally complex situations, but both systems work together, which is actually the beauty of it all. Together, these systems enable us to enhance our overall decision-making and make effective decisions.

Note that this is just another model to think about and deal with—in this case—cognitive bias. Like we mentioned earlier, we consider all models to be wrong, but some models can be helpful. The model of System 1 and System 2 isn’t the holy grail. There are downsides and criticisms of course, as with every model. Keep this in mind when deciding which models are usef[](/book/collaborative-software-design/chapter-7/)ul to you.

### 7.1.4 Embracing cognitive bias

System 1 is sensitive to cognitive bias. We rely on System 1 a lot, but that doesn’t mean most of our decisions rely on biases. Luckily, the dual-system approach helps us make effective decisions. We can spend a lot of time actively questioning System 1, but this would be a waste of time. Instead, we should embrace both systems and learn to recognize situations in which potential mistakes are very costly and cognitive bias might influence the outcome of our decisions. Recognizing these situations and cognitive bias isn’t enough, of course. It’s about balance. If you recognize them, what’s your next step? Do you need to challenge bias or not? There’s no good or bad here—it’s all context dependent. We’ll give some examples on how to apply this balance in sections 7.2 and 7.3.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

An important reason to embrace cognitive bias—apart from the huge help in our decision-making—is that it influences behavior. Human behavior is extremely complex and context- and person-sensitive. The same bias could influence behavior of different people in different ways. So, during collaborative modeling, you need to be aware of this. If you feel like challenging certain biases, be aware that they affect behavior in different ways. Section 7.3 will elaborate on how to facilitate this during collaborative modeling.

Finally, cognitive biases often have a negative connotation, which we feel needs to be removed because biases help us. Sure, in some cases, they lead to poor judgment and decision-making. However, in most cases, these biases don’t make us irrational but rather effective and efficient, which is exactly what we need in collaborative modeling. As long as we can bring balance and facilitate properly, we can stop referring to biases as synonyms to irrationality and poor decision-making. Instead, we can embrace them and benefit from their effect on collaborative modeling.

Now that we’ve explained cognitive bias and its effect on group dynamics, it’s time to dive into collaborative modeling specifically. How do you recognize biases, when do they pop up, and how can you anticipate them when facilitating? Sections 7.2 and 7.3 will go into th[](/book/collaborative-software-design/chapter-7/)ese topics. [](/book/collaborative-software-design/chapter-7/)

## 7.2 Cognitive bias during collaboration and software design

You may have heard someone say, “This blog post says it’s a good choice to go for boring technologies, so we were right to use background processes with library *X*, instead of using message queues!” while not realizing some libraries that enable you to do message queueing have been around a lot longer than the library that they picked out to schedule background processes. Or maybe this sounds more familiar: “I listened to a talk by Eric Evans, and he used approach *X* in his project and said it was an excellent choice too.” These are examples of decisions made on software design and architecture driven by cognitive biases. Some of these decisions will affect the software system for years to come, and making decisions like this won’t have a positive effect on the system. In this section, we’ll dive into a few biases that you should become aware of in others and most importantly in yourself, and we’ll walk you through the effect they have during collaborative modeling and on your sof[](/book/collaborative-software-design/chapter-7/)tware design.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

### 7.2.1 Confirmation bias

[](/book/collaborative-software-design/chapter-7/)The first example in the introduction illustrated *confirmation bias*, in which people search or interpret information in a way that confirms an already held belief. We see this one happen a lot in software development teams when designing their architecture. The same bias can happen when you try to introduce collaborative modeling in your organization. During one of our sessions at BigScreen, Caelan (team lead and invited to participate) was skeptical about the technique. While we were modeling, he looked for reasons to confirm that this wasn’t going to work. At one point, one of the other participants asked a question about how to deal with the absence of the customer in these sessions. We explained that being able to invite customers directly would be more beneficial to examine the customer journeys, but that we could compensate by making some informed assumptions and then implement a monitoring system to validate those assumptions. Caelan immediately responded with this: “So, what you’re saying is that collaborative modeling isn’t optimal for BigScreen because we can’t invite our customers to this session?” This had a huge effect on the collaborative modeling sessions Caelan participated in. The other people in the session became very guarded and reluctant to speak. Even we, the consultants, started weighing our words because we knew we would have to spend time countering Caelan’s arguments on why collaborative modeling wasn’t going to work[](/book/collaborative-software-design/chapter-7/) for BigScreen.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

### 7.2.2 The law of triviality

The *law of triviali[](/book/collaborative-software-design/chapter-7/)ty*, also known as “bike-shedding,” is probably the best-known cognitive bias in the software industry. The law of triviality states that people within an organization spend more time discussing trivial problems than important ones. When we were designing the bounded contexts at BigScreen, we spent a lot of time discussing what the name should be of the bounded context that would be used by both systems instead of discussing what this bounded context would be responsible for, how we would design the public interface to serve both systems, or how to extract this from the current system. Making those design decisions required difficult discussions, and even though the problem had been settled, the team still remembered the conflict between the developers and architect whenever the architecture was discussed. Finding the perfect name, which was a “trivial” decision compared to the other decisions we had to make, got a lot of attention from the team. We were definit[](/book/collaborative-software-design/chapter-7/)ely bike-shedding![](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

##### The origin of bike-shedding

When C. Northcote Parkinson, a British naval historian best known for his law on bureaucracy, wrote about the law of triviality, he offered the example of an executives meeting in which three budgets needed to be approved. The first one was approving the budget (£10.000.000) for an atomic reactor; the second budget was the new bike shed that was going to be built at the office, which would cost £350; and the last one was the annual budget for refreshments during meetings, which was £21. He stated that the time spent on approving a budget would be disproportionate to the amount needed to be approved. Because the executives in the meeting would have limited or no knowledge of atomic reactors, this item would be discussed the least, even though it had the biggest budget. The bike shed and the coffee, however, would be discussed at length because each executive felt they could contribute to those topics.[](/book/collaborative-software-design/chapter-7/)

When retelling the story, as often happens with stories, people didn’t mention the budgets and the coffee, and spoke about time spent approving an atomic reactor and a bike shed. It resonated with a few people in the software industry because discussing details of a specific topic, such as the color or the position of a button on a page, is often given more time than the important decisions that need to be made (What does this feature have to do?). As it gained traction, the law became kn[](/book/collaborative-software-design/chapter-7/)own as bike-shedding.

### 7.2.3 False-consensus effect

[](/book/collaborative-software-design/chapter-7/)Confirmation bias and bike-shedding are cognitive biases that pop up during a collaboration session, so you have to be aware of and manage them. Other cognitive biases have a greater effect on the design and solutions. One of the most common cognitive biases that will not only harm your collaboration but also[](/book/collaborative-software-design/chapter-7/) your solutions is the *false-consensus effect*, the assumption that your personal qualities, characteristics, beliefs, and actions are widespread through the general population. Put differently, people assume everybody is just like them. For collaborative modeling, this translates into assuming that everyone using your software system is just like the people in the session. During one of the sessions at BigScreen, we were digging a bit deeper into the Seat Allocation bounded context. This bounded context is responsible for suggesting and reserving seats when people purchase a ticket to a movie. We decided to do some Example Mapping to come up with specific examples of how Seat Allocations should work. After a while, we noticed that none of the groups had an example of how this should work when somebody in a wheelchair was purchasing a ticket. This is the false-consensus effect bias at work—the assumption that the people we have to allocate seats to are just like [](/book/collaborative-software-design/chapter-7/)everybody in the room.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

### 7.2.4 Availability bias

During collaborative modeling, we want to generate different designs so we can evaluate them and pick the best aspects of each of those as a basis to create the architecture. This isn’t always easy because of the aforementioned availability bias: when we’re trying to make decisions or assess information, we add more value to information that can be recalled easily, or at least find it more important than alternative solutions not as readily recalled.[2](/book/collaborative-software-design/chapter-7/footnote-001) When it comes to collaborative modeling, this bias often makes participants favor the first design they came up with because they substitute “good” for “easy.” [](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

During a modeling session at BigScreen where we were designing the Movie Scheduling bounded context, we tried to create a good model for the concept: a movie is shown at a specific time in a specific hall of a theater at a specific date. We did a first iteration and came up with a “movie” (figure 7.2, left), which had a name, a duration, and a playtime. In this proposed solution, we would have movies that together represented the schedule. We started iterating on this and eventually came up with a “movie showing” (figure 7.2, right). When we asked which one is better, Caelan responded, ”Well, the left one.” When we asked him why that was the better model, he said, ”Because we spent less time on creating it. We struggled a lot to come up with the second one, so it has to be less fitting.” That is the availability bias at play here. Caelan confused the quality of the model (the left model is better than the right model) with the suitability of the model (the left model was easy to come up with, a[](/book/collaborative-software-design/chapter-7/)nd the right one wasn’t).

Part of designing a good architecture is designing your models in different ways to gain knowledge of what would be the best way to represent a concept in your system. You need to dig deeper than the first [](/book/collaborative-software-design/chapter-7/)thing you can come up with!

### 7.2.5 Loss aversion

Once we have our models in production for a while, it also becomes hard to think about a better solution. We become emotionally attached to our solution, and we have a hard time letting go of it. One of the biases at play here is *loss aversion*, the tendency to avoid losses over gaining wins. This is because we have a stronger emotional response to a loss than to a gain.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

Flexibility in our software design is something we want to achieve as good software developers. We want software to be able to change easily, but at the same time, loss aversion prevents us from letting go of what we’ve already designed. That is also a reason we start model fitting. *Model fitting* in the context of software design is deforming, that is, adding or leaving out elements from the problem domain to force it into an already existing model, metaphor, or abstraction. [](/book/collaborative-software-design/chapter-7/)

![Figure 7.2 On the left, we see the first model we came up with, Movie, to represent the concepts of “a movie is shown at a specific time in a specific hall of a theater at a specific date” On the right, you can see the model we came up with when digging deeper into this concept. We realized that a better model to represent this concept would be Movie Showing, where we linked a movie to specific slots.](https://drek4537l1klr.cloudfront.net/baas/Figures/CH07_F02_Baas.png)

[](/book/collaborative-software-design/chapter-7/)Let’s have a look at a model fitting example from BigScreen. The Movie Showing model has two concepts: slot and movie. A slot is a moment where a movie can be played. It has a specific theater and hall, and a start and end date. So, a Movie Showing captures which movie is being played for specific slots. This model was designed in one of our early sessions and put into production. It worked really well. BigScreen also has private movie showings in which a company can book a company event at BigScreen. They can pick a movie from a list and select the type of reception they want. BigScreen has a reception room in each theater where the reception takes place.

When this concept was explained to the software developers, they were afraid to lose two things. First, they were afraid to lose the current model, which was working very well for the purpose it was designed. Second, they were afraid to lose face in front of the business because the current model could not deal with a private movie showing and they would have to redesign the Movie Scheduling bounded context to implement this.

To prevent this, they said, “Oh, private movie showings are just movie showings with two slots: one for the reception, and one for the movie.” The software developers adapted the already existing movie showing and added a property `IsPrivate` to the Movie Showing model to indicate whether or not this is a private showing. They also added the property `IsReception` to a slot, so they could check whether or not the hall should be filled in. When the slot is a reception, it will have no hall assigned to it (see figure 7.3). Due to their loss aversion, they fell into[](/book/collaborative-software-design/chapter-7/) the trap of model fitting.

![Figure 7.3 A visualization of model fitting at BigScreen. The concept of a private movie showing was fitted into an already existing model, Movie Showing, by adding IsPrivate to the model. A slot is now being used to not just represent the playtime of a movie but also a reception that only happens when a company books a private movie showing.](https://drek4537l1klr.cloudfront.net/baas/Figures/CH07_F03_Baas.png)

### 7.2.6 Additive bias

Many biases are interconnected, each influencing the other. For example, the software developers who were afraid to lose face engaged in model fitting and the addition of properties to the model due to loss aversion. This aversion is closely tied to our default strategy of introducing new elements rather than removing existing ones, known as the additive bias. Though distinct, this behavior shares roots with loss aversion and presents challenges in developing software that is secure, maintainable, protects data privacy, and is decoupled. At BigScreen, the goal was to break down the existing big ball of mud (BBoM) architecture for a system that is maintainable and loosely coupled, enabling the team to deliver a sustainable fast flow of business value and allowing the company to swiftly adapt to changing user demands.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

Ignoring this bias and adding extra properties means that developers working within a specific bounded context will need to know more concepts that are relevant to solving the business problem at hand, increasing cognitive load and the risk of making mistakes. This situation can also le[](/book/collaborative-software-design/chapter-7/)ad to a type of coupling known as *connascence of name*, where multiple bounded contexts need to agree on the naming of an entity. If these properties are added across various bounded contexts and later need to be updated simultaneously, coordinating among different teams managing these contexts could slow down the delivery of business value and extend time to market. Overlooking the additive bias might inadvertently lead BigScreen back toward creating a BBoM. Like many biases, this tendency often goes unnoticed, with people typically unaware of how their emotions influence their decisions and actions. Now that we’ve introduced several examples of cognitive biases and their potential effects on architecture and software systems, we’ll explore in greater detail h[](/book/collaborative-software-design/chapter-7/)ow these biases can be addressed.[](/book/collaborative-software-design/chapter-7/)

##### Exercise 7.2

Prepare for possible cognitive biases in your next collaborative modeling session: Which of the mentioned biases could potentially affect your next session and in what way? Then, when you observe them in the session, make them explicit by discussing what you observe and how it can affect the outcomes of the session. Discuss[](/book/collaborative-software-design/chapter-7/) this with the rest of the group. [](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

## 7.3 Facilitating cognitive bias

Now, you might be thinking that cognitive bias sounds like a big problem for collaborative modeling and group decision-making. And you’d be right—cognitive bias can definitely get in the way of achieving a common goal. But here’s the cool part: with the right approach to facilitation, cognitive bias can actually be a useful tool to help groups work together more effectively.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

That’s exactly what we did in one of the follow-up sessions with BigScreen where we used Example Mapping. In that session, we facilitated the group to be aware of the effect of cognitive bias they had while Example Mapping. By shining a light on these biases and using some helpful heuristics, we were able to show them how to use cognitive bias to their advantage. Suddenly, the team was able to explore new possibilities and gain a deeper understanding of the problem at hand.

Of course, it’s not always easy to identify and work with cognitive bias in a group setting. In fact, we’ve all probably experienced situations where it felt like our ideas were getting lost in the shuffle. So, before we dive into our work with BigScreen, let’s take a closer look at how cognitive bias can affect group decision-ma[](/book/collaborative-software-design/chapter-7/)king when it’s not properly addressed.

### 7.3.1 Self-fulfilling prophecy

Before our engagement at BigScreen, we were tasked at another organization with determining the optimal time to lock agendas in a team planning software, considering the different factors that could affect the timeline. As the participants walked into the conference room, we introduced them to the agenda of the day and collaborative modeling. Before starting the session, we did a check-in with the participants to get a sense of what they hoped to get out of the session and what might hold them back from achieving the desired outcome. The participants answered that they hoped to get new ideas to tackle the problem of the user story we were discussing in refinement. Up until now, they experienced a lot of bugs in production because of a misunderstanding of the rules and use cases. They also said they tried a lot of different ways to make these refinements better and were worried they would get the same outcome as before. For us, it was nothing out of the ordinary—this is what we usually hear.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

Now as facilitators, we must recognize that we aren’t immune to biases. In this case, we were affected by two biases: the false-consensu[](/book/collaborative-software-design/chapter-7/)s effect as discussed previously and the *overconfidence bias*. The overconfidence bias refers to the tendency for individuals to have more confidence in their own abilities or judgments than is objectively justified. In our case, we had done these refinements before, and we heard nothing out of the ordinary. We expected the session to overcome what was said in the check-in—I mean that’s why they hired us! But it made us not really listen to what had been said. This led us to overlook the biases that the group was f[](/book/collaborative-software-design/chapter-7/)acing, such as confirmation bias and the *status quo bias*, which affected the session’s effectiveness. The status quo bias is the tendency to prefer the current state of affairs over changing to a new one, even if the new option may be better.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

As we continued with the session, we introduced the participants to the collaborative modeling tool called Example Mapping, as explained in chapter 2, and you can see how such a session might look in figure 7.4. The challenge was to determine when to lock the agenda, considering the different factors that could affect the timeline. However, we soon noticed that the group was[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/) struggling to write down new examples.

![Figure 7.4 An example of how an Example Mapping looked at BigScreen. The groups are divided and are using index cards on the table to collaboratively model and discover new scenarios and acceptance criteria.](https://drek4537l1klr.cloudfront.net/baas/Figures/CH07_F04_Baas.png)

That was because the group was facing the status quo bias, which is closely related to the availability bias and the anchoring effect. The user story we tackled with Example Mapping was rather complicated, with many things that can happen during a specific time period. Now we had already EventStormed that timeline, and Example Mapping would help them move forward by running through several use cases from that EventStorming. Because we didn’t have a predefined template with several use cases, the group stopped writing down their examples because they required the visualization of a timeline. We thought the EventStorm itself was enough to write those examples. Unfortunately, the participants went back to discussing things among themselves without visualizing, which meant that they were sticking to their traditional methods and not using the new tool. We realized that we needed to provide more structure to the process and nudge them to use the tools effectively. We’re aware of other factors that can be at play here, such as unfamiliarity with tools that might lead to uncomfortableness or simply not knowing how. While many factors can influence the preference for specific tools, cognitive bias is very often one of them. If we’re not careful of the effect of biases, collaborative modeling can become a self-fulfilling prophecy.

From this experience, we learned that collaborative modeling tools such as Example Mapping can be powerful assets in decision-making processes, but they require careful consideration of cognitive biases and appropriate facilitation. Our experience showed us that we can’t assume that we understand the needs and biases of a group, and that it’s important to be mindful of our own biases as facilitators. By providing structure, guidance, and support, we can help groups overcome their biases and make the most of these tools.

We learned from our mistakes when we started engaging with BigScreen. With the lessons we learned from our previous experience, we were able to anticipate and address potential biases from the beginning. Let’s look at how we facilitated that exactly by using nudges during an Example Mapping session we did at Bi[](/book/collaborative-software-design/chapter-7/)gScreen in one of the follow-up sessions.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

### 7.3.2 Altering behavior through nudges

[](/book/collaborative-software-design/chapter-7/)In the follow-up session, we were tasked with finding a solution for a tricky problem: How should seat allocations work? It may seem like a simple question, but when you consider the complexity of cinema layouts, seating arrangements, and customer preferences, finding a satisfactory answer can be challenging.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

Because we learned from our previous experience, we together with the software engineering team prepared specific templates to visualize what a cinema looks like. We already showed you how we did Example Mapping in chapter 2, where we used a specific template to visualize what a cinema looks like. The templates, shown in figure 7.5, helped us to establish a common understanding of what different types of cinema seating arrangements there are in BigScreen. From there, we could dive into specific scenarios and discuss what to do in each situation. We would have definitely wanted to collaboratively create those templates together with the stakeholders, but som[](/book/collaborative-software-design/chapter-7/)etimes they don’t have enough time to do so.

##### GUIDING HEURISTIC

When doing Example Mapping in complex or complicated scenarios, spend some time up front to design a template in which you can discuss these scenarios so that you don’t spend [](/book/collaborative-software-design/chapter-7/)too much time on visualizing the examples.

![Figure 7.5 An example of the template we created together with the engineering team for use in an Example Mapping session. Although this is a very basic version of a cinema seating arrangement, it can be easily adapted to, for instance, add in a corridor in the middle. Sometimes, a good first example is enough.](https://drek4537l1klr.cloudfront.net/baas/Figures/CH07_F05_Baas.png)

When we as facilitators asked the team to create templates together up front and bring them to the session, we were using a nudge. A *nudge* is a way to alter people’s behavior of making decisions in a predictable way. For example, you can nudge employees to have healthier lunches by putting healthy food on display that is easy to grab and keeping the unhealthy food more out of sight. The idea of a nudge was popularized by Richard Thaler and Cass Sunstein in their book *Nudge: Improving Decisions About Health, Wealth, and Happiness* (Penguin Books, 2009). One key aspect of a nudge is that it should always be optional, meaning a nudge always retains freedom of choice. Changes in behavior aren’t forced in any way, just triggered. The nudge should be easily avoidable by the group, and they shouldn’t be made to feel like they don’t have a choice to act on the nudge or that they are being manipulated. For example, in our sessions, we presented the templates to the group and explicitly asked for their agreement before using them as a foundation for our discussion.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

We’re using the template as a nudge here because talking about complex scenarios can overwhelm our brains and make us susceptible to cognitive biases such as the availability bias, anchoring effect, and confirmation bias. This is one of the main reasons visualization is so important in collaborative modeling—it reduces the effect of these biases and allows us to make more informed decisions. And the template makes it easy for the group to visualize their conversation.

Not all nudges require explicit agreement as we asked for during our BigScreen Example Mapping session. As a facilitator, you can also lead by example and start drawing a visualization of what the group is discussing. This can encourage others to do the same and start visualizing their ideas. Remember, the group should always be able to decide not to use the nudge if they prefer and keep having a conversation without visualizing. It’s not up to us as facilitators to decide if a cognitive bias is affecting them. Our responsibility is to make it easy for the group to do collaborative modeling. If they don’t see they are affected by a bias[](/book/collaborative-software-design/chapter-7/), they can decide not to do anything about it.

##### GUIDING HEURISTIC

When a conversation isn’t being visualized, lead by example as the facilitator and visualize the conversation.

When we use nudges, we either trigger cognitive biases or work around them to reduce their effect. In the case of Example Mapping, we wanted the group to be affected by the functional fixedness bias we talked about earlier to achieve our goal of focusing on allocating seats in a cinema room. When we think of a cinema, we picture the traditional setting—rows of red chairs, a big screen up front, and no windows. This preconceived image we hold in our minds can stifle our creativity when it comes to designing new ideas. The session’s goal was to focus on the rules that were in place for allocating seats in a cinema room. And everyone had preconceived knowledge and a model in mind of what a cinema room looked like, as well as how to allocate seats for it. Now a competing heuristic here can be to not take any templates with you initially to an Example Mapping session if you[](/book/collaborative-software-design/chapter-7/) want to avoid the functional fixedness bias.

##### GUIDING HEURISTIC

When doing Example Mapping in complex or complicated scenarios, use a little structure up front to avoid the effect that functional fixedness might have on the session.

As an example, when the COVID-19 outbreak happened after our engagement at BigScreen, the team was asked to design a way to reopen within a 1.5-meter distance between reservations. The template affected their creativity to think outside what they already knew. When you run the same session with the templates, people came up with two seats in between reservations, one row in between reservations, or maybe even thinking diagonally. But perhaps we can solve it in a different way and be more creative, which requires us to go around the functional fixedness bias. We say to go around the bias, because you can’t cancel out or fight the bias. In his book *Thinking, Fast and Slow*, Kahneman explains that even if you’re aware of the bias, it doesn’t necessarily help you overcome the possible down sides the bias gives you. Because the team used the template we created a lot with stakeholders, not using a template during Example Mapping didn’t lower the effect of functional fixedness. So, the team needed to design nudges to go around it, and one of our favorite ones to use [](/book/collaborative-software-design/chapter-7/)for functional fixedness is to model it wrong.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

##### GUIDING HEURISTIC

Model it wrong. Think about how you don’t want the model to be and see how that gives you insights to improve your current model and go around functional fixedness.

Model it wrong means think about models, rules, and scenarios that you don’t want. Let the participants get creative and experiment with different scenarios, such as seating vaccinated people on one side and unvaccinated people on the other. You could even get really wild and make people sit in plastic bubbles or remove the chairs altogether and have them stand vertically to fit more people in the room.

Now this can be perceived as all fun and games, but we can learn from it. In the session we had with BigScreen, the example that sparked creativity was the idea of returning to a system where tickets are distributed per screening, allowing people to self-organize and find their own seats upon entering the room, rather than having the able to pick seating in advance. They wondered how many people would still come per reservation and what was allowed. Currently, there was a rule that you could have eight tickets per reservation max, and the attendees needed to sit together in a row. But the COVID-19 rules only allowed for a maximum of two people to sit together from more than one household. So, the question was, what would most reservations look like, and can our cinema room be optimized for that? So, they took a different approach where they wouldn’t allow people to pick from all the seats, but instead, they could choose from predef[](/book/collaborative-software-design/chapter-7/)ined seats that conformed to the COVID-19 rules.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

### 7.3.3 The different dimensions of nudges

Now that you’ve been given a short introduction on how nudges can affect decision-making and collaborative modeling, let’s talk about what we find are the most important dimensions we can categorize nudges in, type 1 and type 2, transparency of the nudge, and the change[](/book/collaborative-software-design/chapter-7/)s or additions to the decision-making environment.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

#### Type 1 and type 2

The most crucial dimensions for us are type 1 and type 2. These dimensions are related to the System 1 and System 2 model discussed earlier. It’s essential to differentiate between these dimensions because, as Pelle Guldborg Hansen and Andreas Maaløe Jespersen point out, “while nudging always affects automatic modes of thinking, it doesn’t necessarily involve reflective thinking.”[3](/book/collaborative-software-design/chapter-7/footnote-000) Type 2 nudges are important as they influence attention and thinking actively and enable people to reflect on the nudge. These types of nudges are essential to reduce the influence of cognitive bias. Merely being aware of a bias that is affecting you doesn’t necessarily decrease its effect. Therefore, it’s necessary to let people decide for themselves how to reduce the effect of the bias.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

Let’s consider one of our sessions at BigScreen, where Caelan was skeptical about the technique. As mentioned earlier, he was influenced by confirmation bias and sought out information that supported his existing beliefs. This made the rest of the group more guarded and hesitant to speak, and the session veered toward discussing whether collaborative modeling would be beneficial for BigScreen. As facilitators, we used a type 2 nudge in this scenario and asked Caelan what evidence could change his mind. This triggered his system 2, leading him to reflect. Now two things can happen: he could start opening up the dialogue, and we can have a group discussion about the pros and cons of the session. Alternatively, he could close down and say nothing could change his mind, leading to a conflict that we’ll discuss in more detail in the next chapter.

In some cases, only one person may be affected by confirmation bias, but it could also affect the entire group. In the next chapter, we’ll explain ho[](/book/collaborative-software-design/chapter-7/)w to allow the group to deal with conflict themselves.

##### GUIDING HEURISTIC

When you feel someone is affected by confirmation bias, ask what evidence can change their mind.

While type 2 nudges are used to allow people to reflect and minimize the effect of cognitive bias, type 1 nudges take advantage of the opportunities that cognitive bias presents. For instance, during our Example Mapping session, we used functional fixedness by introducing templates. We were up front and transparent about using these templates to uncover existing rules. However, there are times when we want to be less transparent about our use of these templates because it could affect the nudge’s effectiveness. This leads us to the[](/book/collaborative-software-design/chapter-7/) second essential dimension—the level of transparency.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

#### Level of transparency

Transparency is a crucial aspect of nudging, as a nudge that lacks transparency can be manipulative and reduce awareness of choice. Consider the example of the anchoring effect discussed earlier in this chapter. As facilitators, if we have a stake in the game, we may set the anchor of a pivotal event where we want it, potentially influencing or even manipulating the decision-making process. As a facilitator, it’s essential to ensure that our nudges are as neutral as possible, meaning that the nudge itself should be neutral even though the decision to design and use the nudge is not.

Let’s explore how we can design nudges with different levels of transparency using an example from a BigScreen session, which was affected by the availability bias when designing bounded contexts together. This bias is one of *the* biases that affect designing more effective models for our problems with stakeholders. We often use familiar language and models because they come quickly to mind, but this can hold us back in finding better models to solve our problems. While designing bounded contexts with BigScreen, they were consistently using the word “reservation” because that was what the old system called it, and it kept popping up during several sessions.

To combat this, we used our favorite nudge, which is to give counterexamples to the words being used. For instance, when consistently hearing the word “reservation,” we used alternative terms such as “allocating seats,” “assigning tickets to seats,” or ”making [](/book/collaborative-software-design/chapter-7/)a placement to a seat” to broaden the group’s thinking.

##### GUIDING HEURISTIC

When a specific word is used a lot during collaborative modeling sessions, try to change the word to see if that can change the model.

The “Use different word for a domain concept” nudge is a nontransparent nudge, where we didn’t make it clear that we were using a nudge and aiming to reduce bias. This is different from the transparent nudge we applied when introducing the Example Mapping templates talked about earlier in this chapter, where we openly asked for the group’s consent before using them to guide our discussion. The main difference is that transparent nudges involve openly sharing your intention and method, unlike nontransparent nudges, where we don’t. For example, we chose to use a different term than the usual one without informing the group or explaining why. Our goal was to encourage them to reflect on the language we used, placing this strategy in the Typ[](/book/collaborative-software-design/chapter-7/)e 2, Nontransparent category, as shown in figure 7.6.

![Figure 7.6 Nudges are organized based on their type and level of transparency. The figure highlights the significance of visually categorizing nudges into quadrants before application. For transparent type II nudges, individuals are informed and provided time to consider whether they wish to respond to the nudge. On the other hand, nudges that fall into the Type I, Nontransparent category are characterized by a lower awareness among people that nudges are being applied. These can potentially manipulate individuals, and the lower they are placed within this quadrant, the more unethical their use becomes. This framework serves as a guide for ethically applying nudges by understanding their effect on awareness and choice.](https://drek4537l1klr.cloudfront.net/baas/Figures/CH07_F06_Baas.png)

Even though the group continued to use “reservation” in our discussions, they started to think more about the words we used. This change was partly due to insights from domain experts, who pointed out the clear difference between issuing tickets and reserving seats, a distinction we covered in chapter 3. As a result, they divided the concepts into “ticketing” and “reservations,” which was helpful. However, “reservation” began to be used in the context of ticketing, creating confusion about the two concepts. It turned out that what they meant by “reservation” wasn’t really about reservations at all. The nudge we initially used didn’t succeed in making them reconsider the term “reservation.”

To help them think in more explicit smaller bounded contexts for a specific problem, we restricted the use of the word “reservation” during one of the follow-up EventStorming sessions. We encouraged them to come up with counterexamples of using that word. They quickly realized that “reservation” was an ambiguous word that could have a lot of different meanings and was holding them back from making deeper insights.

As a result, they renamed the Reservation context to the Seat Allocation context. Although sometimes naming a context is trivial, in this case, it triggered cognitive biases such as availability and functional fixedness, which restricted their thinking about the model. It may take multiple sessions to fully transition to the new model, but the group is now reinf[](/book/collaborative-software-design/chapter-7/)orcing the use of these two new concepts by themselves.

##### GUIDING HEURISTIC

When a certain word can be ambiguous or overused, try to run a collaborative modeling session and restrict the use of the word to find out if that changes your models.

Now you can imagine restricting people to use a certain word changes the environment they can make decisions in,[](/book/collaborative-software-design/chapter-7/) which brings us to our last important nudge dimension.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

#### Changes or additions to the decision-making environment

[](/book/collaborative-software-design/chapter-7/)Nudges can change the decision-making environment in different ways. Some, like restricting the words we use, change the decision-making environment directly. Others can add to the decision-making environment. For example, when deciding on pivotal events, we let people discover them on their own, but sometimes they get stuck in discussion mode without making decisions.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

To address this problem, we added more vertical boundaries without putting a pivotal event on them. This was a nudge that didn’t change the decision-making environment directly. We found that it’s easier to add to the decision-making environment than to make changes, and too many changes can lead to resistance to the nudge. In the next chapter, we’ll explain why this is the case in more detai[](/book/collaborative-software-design/chapter-7/)l. But, for now, it’s important to remember this heuristic.

##### GUIDING HEURISTIC

Favor additions of nudges to your decision-making environment to change the decision-making environment.

Whether a nudge is considered an addition or a change to the decision-making environment depends on the context. For example, the nudge we discussed earlier, where we lead by example and visualize conversations when they’re not already being visualized, can change the decision-making environment if people aren’t used to visualization. However, if people are already used to visualizing but stopped doing so for a specific reason, then using the nudge is an addition.

This distinction is important because people in the first context may be more reluctant to follow your lead, while in the second context, they may gladly start visualizing again. In the first context, using the nudge may make people dependent on you to visualize, which can be a problem if you’re organizing several sessions. You want people to own the flow and outcome, and nudges that change the environment can make people dependent on you, which isn’t ideal. While using the nudge for a single session to achieve the desired outcome might not be a problem, it’s important to be mindful of this in the long run.

To combat the problem of people sitting back and not actively participating in collaborative modeling, we can make use of the bandwagon effect. We can demonstrate how to do it and then ask others to continue while we step back. When at least one person joins in, others will eventually follow. We also try to get someone in[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/) the group to support us and let others join the bandwagon.

##### GUIDING HEURISTIC

If you want to let people take ownership of the collaborative modeling session, show them an example of what is to be done and then step back.

##### GUIDING HEURISTIC

If you expect a lot of people to remain idle during a collaborative modeling session, see if you can run through the session beforehand with two people who might nudge the rest of the group to come along.

This same approach can also be used to get people on board with doing collaborative modeling. We are often asked how to get people on board, and the easiest way to be successful is to start using Guerilla modeling during a meeting that should have been visualized and intentionally make mistakes. Someone will likely correct you, and, from there, you might get others to join in. Of course, the most successful approach is to make collaborative modeling the default. This means facilitating sessions in meeting rooms and providing whiteboards and sticky notes as tools. We’ve supported making collaborative modeling the default by ensuring that ea[](/book/collaborative-software-design/chapter-7/)ch meeting room has a whiteboard and a box of sticky notes.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

##### GUIDING HEURISTIC

If you want people to do more collaborative modeling, make it easy to start doing these sessions during meetings. They sho[](/book/collaborative-software-design/chapter-7/)uld have all the tools available to easily start visualizing.

### 7.3.4 Becoming a choice architect

When designing nudges, it’s essential to remember that they’re culturally dependent. Research on cognitive biases can be biased itself because humans are complex, and different cultures can be affected differently by their environments. Research often involves people from the same cultural group, age, and other factors, which means that copying and pasting nudges might not work in your situation. Our heuristics can be a helpful starting point to become a better facilitator and a *choice architect*, as referred to in Richard Thaler and Cass Sunstein’s book, *Nudge: Improving Decisions About Health, Wealth, and Happiness* (Penguin, 2009). A choice architect is someone who designs and structures decision-making environments in a way that encourages people to make better choices. However, these nudges are never silver bullets, so we encourage you to start designing and using your own and then share them with the world.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

There are already many books explaining heuristics and nudges, and our training approach follows the work of Sharon L Bowman’s book *Training from the Back of the Room! 65 Ways to Step Aside and Let Them Learn* (Pfeiffer, 2008). Some of her training techniques can also help you become better at facilitating collaborative modeling. She talks about how “different trumps the same” when people are learning during training. We try to change the way we do collaborative modeling sessions within a company by avoiding using the same structure and session for retrospectives since they can become bor[](/book/collaborative-software-design/chapter-7/)ing. Instead, we aim to make each session different and engaging.[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)

## 7.4 Collaborative software design catalysts

-  Write down behavioral patterns from people during a meeting or a collaborative modeling session. See if you can link the patterns to the biases described in this chapter. What are the effects of this bias for the process, outcome, and group dynamics? If you’re comfortable, share your insights with the group during or after the meeting/session.[](/book/collaborative-software-design/chapter-7/)
-  In your next meeting where knowledge is shared, make sure there is room for individual contribution before the group conversation to avoid the anchoring effect. You can do this, for example, by having everyone write down their individual ideas on a sticky note before sharing it with the group.
-  Be the first one to start visualizing a flow when only conversations happen between people without visualizing the flow. Observe the effects.
-  Prepare nudges you could use in your next session or meeting, such as a template you designed up front, you starting to visualize options or conversations, or using specific words[](/book/collaborative-software-design/chapter-7/) and concepts. Define the nudge and what you want to achieve with it.

## 7.5 Chapter heuristics

*Guiding heuristics**[](/book/collaborative-software-design/chapter-7/)*

-  When doing Example Mapping in complex or complicated scenarios, spend some time up front to design a template in which you can discuss these scenarios so that you don’t spend too much time on visualizing the examples.
-  When a conversation isn’t being visualized, lead by example as the facilitator and visualize the conversation.
-  When doing Example Mapping in complex or complicated scenarios, use as little structure up front to avoid the effect that functional fixedness might have on the session.
-  Model it wrong. Think about how you don’t want the model to be and see how that gives you insights to improve your current model and go around functional fixedness.
-  When you feel someone is affected by confirmation bias, ask what evidence can change their mind.
-  When a specific word is used a lot during collaborative modeling sessions, try to change the word to see if that can change the model.
-  When a certain word can be ambiguous or overused, try to run a collaborative modeling session and restrict the use of the word to find out if that changes your models.
-  Favor additions of nudges to your decision-making environment to change the decision-making environment.
-  If you want to let people take ownership of the collaborative modeling session, show them an example of what is to be done and then step back.
-  If you expect a lot of people to remain idle during a collaborative modeling session, see if you can run through the session beforehand with two people who might nudge the rest of the group to come along.
-  If you want people to do more collaborative modeling, make it easy to start doing these sessions during meetings. They should have all the tools available to easily start visualizing.

## 7.6 Further reading

-  Choiceology, an original podcast from Charles Schwab by Katy Milkman [](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)
-  Cognitive Bias Codex ([https://mng.bz/x2z7](https://mng.bz/x2z7). Licensed by John Manoogian III under a Creative Commons Attribution-ShareAlike 4.0 license)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)
-  “How the Bias toward Additive Can Lead Us to a Suboptimal and Costly Software Design” by Kenny Baas-Schwegler ([https://mng.bz/rVEj](https://mng.bz/rVEj))[](/book/collaborative-software-design/chapter-7/)
-  *Noise: A Flaw in Judgment* by Daniel Kahneman (Little, Brown Spark, 2021)[](/book/collaborative-software-design/chapter-7/)
-  *Nudge: Improving Decisions About Health, Wealth, and Happiness* by Richard H. Thaler and Cass R. Sunstein (Penguin, 2009)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)
-  “Nudge and the Manipulation of Choice: A Framework for the Responsible Use of the Nudge Approach to Behaviour Change in Public Policy” by Pelle Guldborg Hansen and Andreas Maaløe Jespersen (*European Journal of Risk Regulation*, 2013)[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)
-  *Predictably Irrational: The Hidden Forces That Shape Our Decisions* by Dan Ariely (Harper Perennial, 2010)[](/book/collaborative-software-design/chapter-7/)
-  *Thinking, Fast and Slow* by Daniel Kahneman (Farrar, Straus and Giroux, 2013)[](/book/collaborative-software-design/chapter-7/)
-  *Training From the BACK of the R[](/book/collaborative-software-design/chapter-7/)[](/book/collaborative-software-design/chapter-7/)oom! 65 Ways to Step Aside and Let Them Learn* by Sharon L. Bowman (Pfeiffer, 2008)[](/book/collaborative-software-design/chapter-7/)

## Summary[](/book/collaborative-software-design/chapter-7/)

-  Cognitive biases can be a result of our brain’s attempt to simplify information processing. You can consider them mental shortcuts, or rules of thumb, that help you make sense of the world.
-  Cognitive bias is a systematic pattern of deviation from a norm that affects decision-making and judgment.
-  System 1 and system 2 distinguish between automatic and conscious thought processes. The automatic process is affected by cognitive biases.
-  Cognitive biases help us but can also affect our decision-making and behavior.
-  Confirmation bias, bike-shedding, false-consensus effect, availability bias, loss aversion, and additive bias are all examples of cognitive biases that can affect our collaboration.
-  A nudge is a way to change people’s behavior of making decisions in a predictable way, and they should be easily avoidable by the group.
-  A nudge can come in different dimensions: type 1 and type 2, transparency of the nudge, and the changes or additions to the decision-making environment. It’s important to know where to categorize the nudge.
-  One important aspect in designing nudges is that they are always culturally dependent. You should design the nudges and create your own set of heuristics.[](/book/collaborative-software-design/chapter-7/)

---

[](/book/collaborative-software-design/chapter-7/)[1](/book/collaborative-software-design/chapter-7/footnote-002-backlink)   Stanovich, K. E., & West, R. F. “Individual Difference in Reasoning: Implications for the Rationality Debate?,” 2000. *Behavioral and Brain Sciences*, 23(5): 645–726.

[](/book/collaborative-software-design/chapter-7/)[2](/book/collaborative-software-design/chapter-7/footnote-001-backlink)   Schwarz, N., Bless, H., Strack, F., Klumpp, G., Rittenauer-Schatka, H., & Simons, A. “Ease of Retrieval as Information: Another Look at the Availability Heuristic,” 1991. *Journal of Personality and Social Psychology*, 61(2): 195–202.

[](/book/collaborative-software-design/chapter-7/)[3](/book/collaborative-software-design/chapter-7/footnote-000-backlink)   Hansen, P. G., & Jespersen, A. M., “Nudge and the Manipulation of Choice: A Framework for the Responsible Use of the Nudge Approach to Behaviour Change in Public Policy,” 2013. *European Journal of Risk Regulation* (1): 3–28.
